{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from Code.envs.statemachine import SuccessiveLookups\n",
    "from Code.train import train, OptWrapper\n",
    "from Code.everything5 import build_standard_loop\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "MAIN_DECAY = np.exp(-1/(20)*0.5)\n",
    "ADAP_DECAY = np.exp(-1/(20*2))\n",
    "\n",
    "spec = {\n",
    "    'control_config': {\n",
    "        'neuron_type': 'LIF',\n",
    "        'n_neurons': 100,\n",
    "        'BETA': 0.8,\n",
    "        '1-beta': 'improved',\n",
    "        'SPIKE_FN': 'bellec'\n",
    "    },\n",
    "    'mem_config': {\n",
    "        'neuron_type': 'FlipFlop',\n",
    "        'n_neurons': 20,\n",
    "        'BETA': 0.8,\n",
    "        '1-beta': 'improved',\n",
    "        'SPIKE_FN': 'bellec',\n",
    "        'ADAPSCALE': 30,\n",
    "        'ADAPDECAY': None, #TODO: set this\n",
    "        'OFFSET': None,\n",
    "        'DECAY': MAIN_DECAY\n",
    "    },\n",
    "    'exp_config': {\n",
    "        'n_sequence': 30,\n",
    "        'val_sequence': 100,\n",
    "        'round_length': 20\n",
    "    },\n",
    "    'lr': 0.001,\n",
    "    'lr_decay': 1,\n",
    "    'iterations': 5000,\n",
    "    'batch_size': 64,\n",
    "    'architecture': '1L'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "\n",
    "train_problem = SuccessiveLookups(spec['iterations'], spec['batch_size'], spec['exp_config']['n_sequence'],\n",
    "                               spec['exp_config']['round_length'], DEVICE)\n",
    "val_problem = SuccessiveLookups(1, spec['batch_size'], spec['exp_config']['val_sequence'],\n",
    "                               spec['exp_config']['round_length'], DEVICE)\n",
    "\n",
    "\n",
    "n_in, n_out, input_rate = train_problem.get_infos()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'control': [[('input', 1), ('control', 1), ('mem', 1)], LIFNeuron(), <class 'torch.nn.modules.linear.Linear'>], 'mem': [[('input', 1), ('control', 1), ('mem', 1)], NewFlipFlop(\n",
      "  (lif_on): LIFNeuron()\n",
      "  (lif_off): LIFNeuron()\n",
      "), <class 'torch.nn.modules.linear.Linear'>], 'output': [[('control', 1), ('mem', 1)], BaseNeuron(), None]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "OuterWrapper(\n  (model): DynNetwork(\n    (layers): ModuleDict(\n      (loop): SequenceWrapper(\n        (model): ParallelNetwork2(\n          (layers): ModuleDict(\n            (control): LIFNeuron()\n            (control_synapse): Linear(in_features=148, out_features=100, bias=True)\n            (mem): NewFlipFlop(\n              (lif_on): LIFNeuron()\n              (lif_off): LIFNeuron()\n            )\n            (mem_synapse): Linear(in_features=148, out_features=20, bias=True)\n            (output): BaseNeuron()\n          )\n        )\n      )\n      (output_synapse): Linear(in_features=120, out_features=8, bias=True)\n      (output): BaseNeuron()\n    )\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Code.everything5 import OuterWrapper, DynNetwork, ParallelNetwork2, SequenceWrapper, BaseNeuron\n",
    "\n",
    "loop = build_standard_loop(spec, n_in, input_rate)\n",
    "out_neuron_size = spec['control_config']['n_neurons'] + spec['mem_config']['n_neurons']\n",
    "\n",
    "outer = {\n",
    "    'input': n_in,\n",
    "    'loop': [['input'], SequenceWrapper(ParallelNetwork2(loop)), None],\n",
    "    'output': [['loop'], BaseNeuron(n_out, None), nn.Linear]\n",
    "}\n",
    "\n",
    "model = OuterWrapper(DynNetwork(outer))\n",
    "model.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "optimizer = OptWrapper(model.parameters(), spec['lr'], spec['lr_decay'], 2500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 13.02% | Val Time: 2.4s | Time per it: 2.3s\n",
      "It:   20 | Loss: 2.084 | Acc: 12.37%\n",
      "It:   40 | Loss: 2.077 | Acc: 14.73%\n",
      "It:   60 | Loss: 2.064 | Acc: 15.23%\n",
      "It:   80 | Loss: 2.060 | Acc: 14.52%\n",
      "It:  100 | Loss: 2.068 | Acc: 14.57%\n",
      "Val Acc: 13.34% | Val Time: 2.6s | Time per it: 2.2s\n",
      "It:  120 | Loss: 2.040 | Acc: 16.54%\n",
      "It:  140 | Loss: 2.054 | Acc: 15.11%\n",
      "It:  160 | Loss: 2.024 | Acc: 19.24%\n",
      "It:  180 | Loss: 1.992 | Acc: 22.69%\n",
      "It:  200 | Loss: 1.939 | Acc: 25.63%\n",
      "Val Acc: 24.95% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It:  220 | Loss: 2.011 | Acc: 22.43%\n",
      "It:  240 | Loss: 2.023 | Acc: 21.25%\n",
      "It:  260 | Loss: 1.958 | Acc: 22.34%\n",
      "It:  280 | Loss: 1.900 | Acc: 27.21%\n",
      "It:  300 | Loss: 1.876 | Acc: 26.82%\n",
      "Val Acc: 27.73% | Val Time: 2.5s | Time per it: 2.2s\n",
      "It:  320 | Loss: 1.954 | Acc: 25.81%\n",
      "It:  340 | Loss: 1.934 | Acc: 25.06%\n",
      "It:  360 | Loss: 1.910 | Acc: 24.95%\n",
      "It:  380 | Loss: 1.879 | Acc: 26.96%\n",
      "It:  400 | Loss: 1.814 | Acc: 33.00%\n",
      "Val Acc: 33.65% | Val Time: 2.5s | Time per it: 2.2s\n",
      "It:  420 | Loss: 1.803 | Acc: 31.64%\n",
      "It:  440 | Loss: 1.768 | Acc: 32.69%\n",
      "It:  460 | Loss: 1.715 | Acc: 32.96%\n",
      "It:  480 | Loss: 1.647 | Acc: 35.68%\n",
      "It:  500 | Loss: 1.622 | Acc: 36.47%\n",
      "Val Acc: 37.31% | Val Time: 2.6s | Time per it: 2.2s\n",
      "It:  520 | Loss: 1.612 | Acc: 36.79%\n",
      "It:  540 | Loss: 1.645 | Acc: 36.00%\n",
      "It:  560 | Loss: 1.636 | Acc: 36.33%\n",
      "It:  580 | Loss: 1.592 | Acc: 38.65%\n",
      "It:  600 | Loss: 1.571 | Acc: 40.02%\n",
      "Val Acc: 41.04% | Val Time: 2.5s | Time per it: 2.2s\n",
      "It:  620 | Loss: 1.527 | Acc: 42.04%\n",
      "It:  640 | Loss: 1.491 | Acc: 42.95%\n",
      "It:  660 | Loss: 1.456 | Acc: 44.20%\n",
      "It:  680 | Loss: 1.437 | Acc: 44.73%\n",
      "It:  700 | Loss: 1.424 | Acc: 45.47%\n",
      "Val Acc: 44.98% | Val Time: 2.5s | Time per it: 2.2s\n",
      "It:  720 | Loss: 1.408 | Acc: 46.02%\n",
      "It:  740 | Loss: 1.391 | Acc: 46.39%\n",
      "It:  760 | Loss: 1.381 | Acc: 46.81%\n",
      "It:  780 | Loss: 1.387 | Acc: 46.39%\n",
      "It:  800 | Loss: 1.380 | Acc: 46.88%\n",
      "Val Acc: 46.70% | Val Time: 2.5s | Time per it: 2.1s\n",
      "It:  820 | Loss: 1.360 | Acc: 46.63%\n",
      "It:  840 | Loss: 1.372 | Acc: 46.44%\n",
      "It:  860 | Loss: 1.368 | Acc: 46.70%\n",
      "It:  880 | Loss: 1.407 | Acc: 44.68%\n",
      "It:  900 | Loss: 1.439 | Acc: 43.76%\n",
      "Val Acc: 39.25% | Val Time: 2.3s | Time per it: 2.1s\n",
      "It:  920 | Loss: 1.546 | Acc: 39.25%\n",
      "It:  940 | Loss: 1.548 | Acc: 39.73%\n",
      "It:  960 | Loss: 1.525 | Acc: 40.60%\n",
      "It:  980 | Loss: 1.483 | Acc: 42.75%\n",
      "It: 1000 | Loss: 1.410 | Acc: 45.47%\n",
      "Val Acc: 46.39% | Val Time: 2.5s | Time per it: 2.1s\n",
      "It: 1020 | Loss: 1.375 | Acc: 46.57%\n",
      "It: 1040 | Loss: 1.339 | Acc: 48.46%\n",
      "It: 1060 | Loss: 1.311 | Acc: 49.45%\n",
      "It: 1080 | Loss: 1.296 | Acc: 50.41%\n",
      "It: 1100 | Loss: 1.280 | Acc: 50.36%\n",
      "Val Acc: 52.00% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 1120 | Loss: 1.264 | Acc: 51.14%\n",
      "It: 1140 | Loss: 1.255 | Acc: 51.58%\n",
      "It: 1160 | Loss: 1.250 | Acc: 51.14%\n",
      "It: 1180 | Loss: 1.226 | Acc: 53.23%\n",
      "It: 1200 | Loss: 1.223 | Acc: 52.65%\n",
      "Val Acc: 52.43% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 1220 | Loss: 1.208 | Acc: 53.35%\n",
      "It: 1240 | Loss: 1.200 | Acc: 53.85%\n",
      "It: 1260 | Loss: 1.200 | Acc: 53.98%\n",
      "It: 1280 | Loss: 1.207 | Acc: 53.32%\n",
      "It: 1300 | Loss: 1.192 | Acc: 53.78%\n",
      "Val Acc: 54.58% | Val Time: 2.5s | Time per it: 2.1s\n",
      "It: 1320 | Loss: 1.196 | Acc: 54.01%\n",
      "It: 1340 | Loss: 1.172 | Acc: 55.31%\n",
      "It: 1360 | Loss: 1.159 | Acc: 55.23%\n",
      "It: 1380 | Loss: 1.154 | Acc: 55.88%\n",
      "It: 1400 | Loss: 1.143 | Acc: 56.15%\n",
      "Val Acc: 55.13% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 1420 | Loss: 1.144 | Acc: 56.43%\n",
      "It: 1440 | Loss: 1.133 | Acc: 57.21%\n",
      "It: 1460 | Loss: 1.116 | Acc: 58.08%\n",
      "It: 1480 | Loss: 1.133 | Acc: 57.09%\n",
      "It: 1500 | Loss: 1.141 | Acc: 57.28%\n",
      "Val Acc: 56.98% | Val Time: 2.3s | Time per it: 2.0s\n",
      "It: 1520 | Loss: 1.144 | Acc: 56.97%\n",
      "It: 1540 | Loss: 1.137 | Acc: 57.56%\n",
      "It: 1560 | Loss: 1.164 | Acc: 56.26%\n",
      "It: 1580 | Loss: 1.140 | Acc: 57.14%\n",
      "It: 1600 | Loss: 1.104 | Acc: 58.49%\n",
      "Val Acc: 60.09% | Val Time: 2.4s | Time per it: 2.0s\n",
      "It: 1620 | Loss: 1.112 | Acc: 57.63%\n",
      "It: 1640 | Loss: 1.095 | Acc: 58.40%\n",
      "It: 1660 | Loss: 1.106 | Acc: 58.44%\n",
      "It: 1680 | Loss: 1.102 | Acc: 58.80%\n",
      "It: 1700 | Loss: 1.089 | Acc: 59.37%\n",
      "Val Acc: 60.05% | Val Time: 2.4s | Time per it: 2.0s\n",
      "It: 1720 | Loss: 1.068 | Acc: 59.87%\n",
      "It: 1740 | Loss: 1.085 | Acc: 58.80%\n",
      "It: 1760 | Loss: 1.071 | Acc: 59.01%\n",
      "It: 1780 | Loss: 1.051 | Acc: 60.11%\n",
      "It: 1800 | Loss: 1.028 | Acc: 60.90%\n",
      "Val Acc: 59.97% | Val Time: 2.5s | Time per it: 2.0s\n",
      "It: 1820 | Loss: 1.036 | Acc: 60.62%\n",
      "It: 1840 | Loss: 1.033 | Acc: 60.99%\n",
      "It: 1860 | Loss: 1.011 | Acc: 61.65%\n",
      "It: 1880 | Loss: 0.997 | Acc: 62.39%\n",
      "It: 1900 | Loss: 1.002 | Acc: 62.74%\n",
      "Val Acc: 62.01% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 1920 | Loss: 0.992 | Acc: 63.34%\n",
      "It: 1940 | Loss: 0.979 | Acc: 63.84%\n",
      "It: 1960 | Loss: 0.956 | Acc: 64.83%\n",
      "It: 1980 | Loss: 0.949 | Acc: 65.61%\n",
      "It: 2000 | Loss: 0.932 | Acc: 65.99%\n",
      "Val Acc: 65.37% | Val Time: 2.5s | Time per it: 2.1s\n",
      "It: 2020 | Loss: 0.928 | Acc: 65.89%\n",
      "It: 2040 | Loss: 0.912 | Acc: 66.47%\n",
      "It: 2060 | Loss: 0.914 | Acc: 66.52%\n",
      "It: 2080 | Loss: 0.904 | Acc: 67.07%\n",
      "It: 2100 | Loss: 0.890 | Acc: 68.03%\n",
      "Val Acc: 67.33% | Val Time: 2.3s | Time per it: 2.1s\n",
      "It: 2120 | Loss: 0.857 | Acc: 69.61%\n",
      "It: 2140 | Loss: 0.867 | Acc: 69.06%\n",
      "It: 2160 | Loss: 0.830 | Acc: 70.44%\n",
      "It: 2180 | Loss: 0.850 | Acc: 69.86%\n",
      "It: 2200 | Loss: 0.811 | Acc: 71.54%\n",
      "Val Acc: 70.57% | Val Time: 2.3s | Time per it: 2.0s\n",
      "It: 2220 | Loss: 0.799 | Acc: 71.73%\n",
      "It: 2240 | Loss: 0.773 | Acc: 73.14%\n",
      "It: 2260 | Loss: 0.801 | Acc: 71.83%\n",
      "It: 2280 | Loss: 0.761 | Acc: 73.46%\n",
      "It: 2300 | Loss: 0.743 | Acc: 74.45%\n",
      "Val Acc: 73.14% | Val Time: 2.4s | Time per it: 2.0s\n",
      "It: 2320 | Loss: 0.745 | Acc: 74.42%\n",
      "It: 2340 | Loss: 0.733 | Acc: 74.85%\n",
      "It: 2360 | Loss: 0.728 | Acc: 75.08%\n",
      "It: 2380 | Loss: 0.730 | Acc: 74.91%\n",
      "It: 2400 | Loss: 0.700 | Acc: 75.47%\n",
      "Val Acc: 73.64% | Val Time: 2.3s | Time per it: 2.0s\n",
      "It: 2420 | Loss: 0.724 | Acc: 74.94%\n",
      "It: 2440 | Loss: 0.720 | Acc: 75.02%\n",
      "It: 2460 | Loss: 0.672 | Acc: 77.06%\n",
      "It: 2480 | Loss: 0.622 | Acc: 79.02%\n",
      "It: 2500 | Loss: 0.616 | Acc: 79.41%\n",
      "Val Acc: 78.54% | Val Time: 2.4s | Time per it: 2.0s\n",
      "It: 2520 | Loss: 0.590 | Acc: 80.15%\n",
      "It: 2540 | Loss: 0.589 | Acc: 79.91%\n",
      "It: 2560 | Loss: 0.591 | Acc: 79.84%\n",
      "It: 2580 | Loss: 0.579 | Acc: 80.63%\n",
      "It: 2600 | Loss: 0.549 | Acc: 81.64%\n",
      "Val Acc: 81.90% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 2620 | Loss: 0.542 | Acc: 81.96%\n",
      "It: 2640 | Loss: 0.554 | Acc: 81.36%\n",
      "It: 2660 | Loss: 0.545 | Acc: 81.79%\n",
      "It: 2680 | Loss: 0.550 | Acc: 81.25%\n",
      "It: 2700 | Loss: 0.493 | Acc: 83.59%\n",
      "Val Acc: 84.26% | Val Time: 2.5s | Time per it: 2.1s\n",
      "It: 2720 | Loss: 0.452 | Acc: 85.01%\n",
      "It: 2740 | Loss: 0.458 | Acc: 84.96%\n",
      "It: 2760 | Loss: 0.447 | Acc: 85.59%\n",
      "It: 2780 | Loss: 0.436 | Acc: 85.84%\n",
      "It: 2800 | Loss: 0.418 | Acc: 86.52%\n",
      "Val Acc: 86.00% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 2820 | Loss: 0.415 | Acc: 86.52%\n",
      "It: 2840 | Loss: 0.413 | Acc: 86.62%\n",
      "It: 2860 | Loss: 0.400 | Acc: 87.01%\n",
      "It: 2880 | Loss: 0.388 | Acc: 87.44%\n",
      "It: 2900 | Loss: 0.396 | Acc: 87.05%\n",
      "Val Acc: 86.68% | Val Time: 2.5s | Time per it: 2.1s\n",
      "It: 2920 | Loss: 0.385 | Acc: 87.59%\n",
      "It: 2940 | Loss: 0.391 | Acc: 87.38%\n",
      "It: 2960 | Loss: 0.382 | Acc: 87.63%\n",
      "It: 2980 | Loss: 0.377 | Acc: 87.91%\n",
      "It: 3000 | Loss: 0.351 | Acc: 88.77%\n",
      "Val Acc: 87.78% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 3020 | Loss: 0.326 | Acc: 89.40%\n",
      "It: 3040 | Loss: 0.323 | Acc: 89.64%\n",
      "It: 3060 | Loss: 0.310 | Acc: 90.24%\n",
      "It: 3080 | Loss: 0.325 | Acc: 89.77%\n",
      "It: 3100 | Loss: 0.334 | Acc: 89.37%\n",
      "Val Acc: 88.98% | Val Time: 2.4s | Time per it: 2.1s\n",
      "It: 3120 | Loss: 0.305 | Acc: 90.59%\n",
      "It: 3140 | Loss: 0.291 | Acc: 90.92%\n",
      "It: 3160 | Loss: 0.284 | Acc: 91.09%\n",
      "It: 3180 | Loss: 0.270 | Acc: 91.62%\n",
      "It: 3200 | Loss: 0.268 | Acc: 91.76%\n",
      "Val Acc: 91.35% | Val Time: 2.4s | Time per it: 2.1s\n"
     ]
    }
   ],
   "source": [
    "train(train_problem, val_problem, optimizer, model, None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}