{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.distributions.geometric import Geometric\n",
    "from Code.envs.storerecall import make_batch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "USE_JIT = False\n",
    "\n",
    "#TODO: test device\n",
    "device = torch.device('cpu')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "spec2 = {'beta': 0.9,\n",
    "   'lr': 0.001,\n",
    "   'lr_decay': 0.8,\n",
    "   '1-beta': False,\n",
    "   'ported_weights': True,\n",
    "   'NoBias': True,\n",
    "   'iterations': 5000,\n",
    "   'batch_size': 128,\n",
    "   'mem_beta': 1,\n",
    "   'spkfn': 'ss',\n",
    "   'decay_out': False,\n",
    "   'architecture': '1L',\n",
    "   'control_neuron': 'LIF',\n",
    "   'mem_neuron': 'Cooldown'}\n",
    "\n",
    "spec = {'beta': 0.9,\n",
    "   'lr': 0.01,\n",
    "   'lr_decay': 0.8,\n",
    "   '1-beta': False,\n",
    "   'ported_weights': True,\n",
    "   'NoBias': True,\n",
    "   'iterations': 5000,\n",
    "   'batch_size': 128,\n",
    "   'mem_beta': 0.95,\n",
    "   'spkfn': 'bellec',\n",
    "   'decay_out': False,\n",
    "   'architecture': '2L',\n",
    "   'control_neuron': 'LIF',\n",
    "   'mem_neuron': 'Adaptive'}\n",
    "\n",
    "spec['iterations'] = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "OuterWrapper(\n  (pretrace): DynNetwork(\n    (layers): ModuleDict(\n      (loop): SequenceWrapper(\n        (model): ParallelNetwork(\n          (layers): ModuleDict(\n            (control_synapse): Linear(in_features=13, out_features=10, bias=False)\n            (control): LIFNeuron()\n            (mem_synapse): Linear(in_features=10, out_features=10, bias=False)\n            (mem): AdaptiveNeuron()\n            (output): DummyNeuron()\n          )\n        )\n      )\n      (output_synapse): Linear(in_features=20, out_features=1, bias=True)\n      (output): DummyNeuron()\n    )\n  )\n  (model): DynNetwork(\n    (layers): ModuleDict(\n      (loop): SequenceWrapper(\n        (model): ParallelNetwork(\n          (layers): ModuleDict(\n            (control_synapse): Linear(in_features=13, out_features=10, bias=False)\n            (control): LIFNeuron()\n            (mem_synapse): Linear(in_features=10, out_features=10, bias=False)\n            (mem): AdaptiveNeuron()\n            (output): DummyNeuron()\n          )\n        )\n      )\n      (output_synapse): Linear(in_features=20, out_features=1, bias=True)\n      (output): DummyNeuron()\n    )\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Code.Networks import Selector, DynNetwork, OuterWrapper, LSTMWrapper, ReLuWrapper, DummyNeuron, make_SequenceWrapper, ParallelNetwork, MeanModule\n",
    "from Code.NewNeurons2 import SeqOnlySpike, CooldownNeuron, OutputNeuron, LIFNeuron, NoResetNeuron, AdaptiveNeuron\n",
    "\n",
    "\n",
    "built_config = {\n",
    "    'BETA': spec['beta'],\n",
    "    'OFFSET': 7, # TODO: was 3 for config24\n",
    "    'SPIKE_FN': spec['spkfn'],\n",
    "    '1-beta': spec['1-beta'],\n",
    "    'ADAPDECAY': 0.9985,\n",
    "    'ADAPSCALE': 180\n",
    "}\n",
    "\n",
    "mem_config = {\n",
    "    **built_config,\n",
    "    'BETA': spec['mem_beta']\n",
    "}\n",
    "\n",
    "n_input = 3\n",
    "n_control = 10\n",
    "n_mem = 10\n",
    "\n",
    "control_lookup = {\n",
    "    'LIF': LIFNeuron,\n",
    "    'Disc': SeqOnlySpike,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "mem_lookup = {\n",
    "    'Adaptive': AdaptiveNeuron,\n",
    "    'Cooldown': CooldownNeuron,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "control_neuron = control_lookup[spec['control_neuron']](n_control, built_config)\n",
    "mem_neuron = mem_lookup[spec['mem_neuron']](n_mem, mem_config)\n",
    "out_neuron = OutputNeuron(n_control+n_mem, built_config) if spec['decay_out'] else DummyNeuron(n_control+n_mem, built_config)\n",
    "\n",
    "\n",
    "loop_2L = OrderedDict([\n",
    "    ('input', n_input),\n",
    "    ('control', [['input', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['control'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop_1L = OrderedDict([\n",
    "    ('input', n_input),\n",
    "    ('control', [['input', 'control', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['input', 'control', 'mem'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop = loop_1L if spec['architecture'] == '1L' else loop_2L\n",
    "\n",
    "outer = OrderedDict([\n",
    "    ('input', n_input),\n",
    "    ('loop', [['input'], make_SequenceWrapper(ParallelNetwork(loop, bias=(not spec['NoBias'])), USE_JIT), None]),\n",
    "    ('output', [['loop'], DummyNeuron(1, None), nn.Linear]),\n",
    "])\n",
    "\n",
    "model = OuterWrapper(DynNetwork(outer), device, USE_JIT)\n",
    "\n",
    "\n",
    "#loop_model = OuterWrapper(make_SequenceWrapper(ParallelNetwork(loop), USE_JIT), device, USE_JIT)\n",
    "\n",
    "#final_linear = nn.Linear(n_control+n_mem, 10).to(device)\n",
    "'''\n",
    "if spec['ported_weights']:\n",
    "    o_weights = pickle.load(open('weight_transplant_enc', 'rb'))\n",
    "\n",
    "    o1 = torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).t()\n",
    "    o2 = torch.tensor(o_weights['InputWeights/InputWeight:0']).t()\n",
    "    o3 = torch.cat((o2, o1), dim=1)\n",
    "    with torch.no_grad():\n",
    "        model.pretrace.layers.loop.model.layers.control_synapse.weight.data[:,:300] = o3[:120] if spec['architecture'] == '1L' else o3[:120, :181]\n",
    "        model.pretrace.layers.loop.model.layers.mem_synapse.weight.data[:,:300] = o3[120:] if spec['architecture'] == '1L' else o3[120:, 180:]\n",
    "        model.pretrace.layers.output_synapse.weight.data = torch.tensor(o_weights['out_weight:0']).t()\n",
    "'''\n",
    "params = list(model.parameters())\n",
    "\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "lr = spec['lr']\n",
    "optimizer = optim.Adam(params, lr=lr)\n",
    "bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "ITERATIONS = spec['iterations']#36000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "stats = {\n",
    "    'grad_norm': [],\n",
    "    'loss': [],\n",
    "    'acc': [],\n",
    "    'batch_var': [],\n",
    "    'val': []\n",
    "}\n",
    "\n",
    "grad_norm_history = []\n",
    "def record_norm():\n",
    "    norms = []\n",
    "    for p in params:\n",
    "        norms.append(p.grad.norm().item())\n",
    "    stats['grad_norm'].append(torch.tensor(norms).norm().item())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "store_dist = (lambda : Geometric(torch.tensor([0.2], device=device)).sample().int().item()+1)\n",
    "recall_dist = (lambda : Geometric(torch.tensor([0.2], device=device)).sample().int().item()+1)\n",
    "SEQ_LEN = 13\n",
    "CHAR_DUR = 200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07343670725822449 0.0518689900636673\n",
      "0.07853973656892776 0.0625\n",
      "0.0686025395989418 0.04627403989434242\n",
      "0.06591444462537766 0.04447115212678909\n",
      "0.06657587736845016 0.043870192021131516\n",
      "0.06644821912050247 0.04747596010565758\n",
      "0.07201509922742844 0.05288461595773697\n",
      "0.06714820116758347 0.04747596010565758\n",
      "0.0709623396396637 0.05528846010565758\n",
      "0.06872430443763733 0.048076923936605453\n",
      "0.06852830201387405 0.043870192021131516\n",
      "0.06576674431562424 0.045673076063394547\n",
      "0.07093815505504608 0.043870192021131516\n",
      "0.07038183510303497 0.04747596010565758\n",
      "0.07215932011604309 0.053485576063394547\n",
      "0.07257229834794998 0.0625\n",
      "0.06927400082349777 0.05228365212678909\n",
      "0.06896336376667023 0.053485576063394547\n",
      "0.0663054883480072 0.048076923936605453\n",
      "0.0711168646812439 0.051682692021131516\n",
      "0.0711168646812439 0.06971869207918643 0.05012950673699379 17.891602277755737 3\n",
      "0.07085995376110077 0.05288461595773697\n",
      "0.06787104904651642 0.05048076808452606\n",
      "0.07125521451234818 0.05288461595773697\n",
      "0.07160301506519318 0.053485576063394547\n",
      "0.06917668133974075 0.05109074339270592\n",
      "0.0721130445599556 0.06069711595773697\n",
      "0.06656944006681442 0.049879807978868484\n",
      "0.07410752773284912 0.051682692021131516\n",
      "0.06735659390687943 0.042067307978868484\n",
      "0.07197099924087524 0.048076923936605453\n",
      "0.06645121425390244 0.04927884787321091\n",
      "0.06748103350400925 0.051682692021131516\n",
      "0.06832978874444962 0.04928185045719147\n",
      "0.06770502775907516 0.05709134787321091\n",
      "0.07284102588891983 0.05288461595773697\n",
      "0.06640733778476715 0.051682692021131516\n",
      "0.06869394332170486 0.05108173191547394\n",
      "0.0657031461596489 0.04627403989434242\n",
      "0.07122739404439926 0.051682692021131516\n",
      "0.07035104930400848 0.04928185045719147\n",
      "0.07035104930400848 0.0694037239998579 0.0511726263910532 17.79981827735901 3\n",
      "0.07061196863651276 0.04627403989434242\n",
      "0.0733802393078804 0.04927884787321091\n",
      "0.06562865525484085 0.049879807978868484\n",
      "0.07237602770328522 0.05228365212678909\n",
      "0.07016735523939133 0.049879807978868484\n",
      "0.07023816555738449 0.0510847344994545\n",
      "0.06896818429231644 0.057692307978868484\n",
      "0.06928668916225433 0.051694709807634354\n",
      "0.0701495110988617 0.04988281428813934\n",
      "0.06773056834936142 0.04687199369072914\n",
      "0.072970911860466 0.051682692021131516\n",
      "0.07430935651063919 0.053494591265916824\n",
      "0.07347553223371506 0.0510847344994545\n",
      "0.07018709927797318 0.05229266732931137\n",
      "0.06922531127929688 0.05707932636141777\n",
      "0.06897090375423431 0.05467848479747772\n",
      "0.07201539725065231 0.055294472724199295\n",
      "0.07035928219556808 0.05408954247832298\n",
      "0.07182856649160385 0.05828725919127464\n",
      "0.07268570363521576 0.05648437514901161\n",
      "0.07268570363521576 0.07072827145457268 0.052464543096721175 18.258554935455322 3\n",
      "0.07075898349285126 0.05228365212678909\n",
      "0.06397738307714462 0.049879807978868484\n",
      "0.07036951184272766 0.049284856766462326\n",
      "0.07189740240573883 0.0546875\n",
      "0.06302414834499359 0.048076923936605453\n",
      "0.07171397656202316 0.04747596010565758\n",
      "0.07022643089294434 0.053485576063394547\n",
      "0.0700351819396019 0.051685698330402374\n",
      "0.06575991213321686 0.053488582372665405\n",
      "0.07296375185251236 0.0546875\n",
      "0.07259022444486618 0.05288461595773697\n",
      "0.0696234330534935 0.051688700914382935\n",
      "0.06653013825416565 0.051676683127880096\n",
      "0.07126404345035553 0.05769531428813934\n",
      "0.06779539585113525 0.048076923936605453\n",
      "0.06786607205867767 0.04990684986114502\n",
      "0.07054739445447922 0.05166466161608696\n",
      "0.06832537800073624 0.049290865659713745\n",
      "0.07447744905948639 0.06128004938364029\n",
      "0.07364785671234131 0.05526141822338104\n",
      "0.07364785671234131 0.06966970339417458 0.05222310703247786 17.138765335083008 3\n",
      "0.06783333420753479 0.05707632377743721\n",
      "0.06991058588027954 0.055880408734083176\n",
      "0.06964205205440521 0.05167968571186066\n",
      "0.06740709394216537 0.049876801669597626\n",
      "0.06699980050325394 0.051688700914382935\n",
      "0.06522154808044434 0.05288461595773697\n",
      "0.07007663697004318 0.049879807978868484\n",
      "0.06845653057098389 0.051682692021131516\n",
      "0.07038065046072006 0.054083533585071564\n",
      "0.07351633161306381 0.05228365212678909\n",
      "0.06899292021989822 0.051682692021131516\n",
      "0.0705704316496849 0.048076923936605453\n",
      "0.07056435942649841 0.04507211595773697\n",
      "0.06880142539739609 0.05048377439379692\n",
      "0.06968404352664948 0.055300481617450714\n",
      "0.07417208701372147 0.05888821929693222\n",
      "0.0660141259431839 0.03668870031833649\n",
      "0.07132688164710999 0.04985877498984337\n",
      "0.06835843622684479 0.04575120285153389\n",
      "Total time:  1733.9063243865967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 1\n",
    "sumloss = 0\n",
    "sumacc = 0\n",
    "\n",
    "while i < ITERATIONS:\n",
    "    batchstart = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    data = make_batch(BATCH_SIZE, SEQ_LEN, store_dist, recall_dist, device)\n",
    "    data = data.repeat_interleave(CHAR_DUR, 0)\n",
    "    input = data[:, :, :3]\n",
    "    target = data[:, :, 3]\n",
    "    recall = data[:, :, 0]\n",
    "    #TODO: repeat data over\n",
    "\n",
    "    output, _ = model(input)\n",
    "    output = output.squeeze()\n",
    "    assert output.shape == target.shape, f'shapes on loss {output.shape}, {target.shape}'\n",
    "    #TODO: mask with recall\n",
    "    loss = (bce(output, target)*recall).mean() #correct shape?\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        record_norm()\n",
    "        stats['loss'].append(loss.item())\n",
    "        acc = (((output > 0.5).float() == target).float()*recall).mean().item()\n",
    "        #stats['acc'].append(acc)\n",
    "        batch_var = 3 #out_final.var(0).mean().item()\n",
    "        #stats['batch_var'].append(batch_var)\n",
    "\n",
    "        print(loss.item(), acc)\n",
    "\n",
    "\n",
    "    sumloss += loss.item()\n",
    "    sumacc += acc\n",
    "    if i%20 == 0:\n",
    "        print(loss.item(), sumloss/20, sumacc/20, time.time()-batchstart, batch_var) #torch.argmax(outputs[-1], 1).float().var()\n",
    "        sumloss = 0\n",
    "        sumacc = 0\n",
    "    if i%2500 == 0:\n",
    "        lr = lr * spec['lr_decay']\n",
    "        optimizer = optim.Adam(params, lr=lr)\n",
    "        print('Learning Rate: ', lr)\n",
    "    i += 1\n",
    "    #config['stats'] = stats\n",
    "    #config['progress'] = i\n",
    "    #with open('configs/' + run_id + '.json', 'w') as config_file:\n",
    "    #    json.dump(config, config_file, indent=2)\n",
    "    #model.save('models/'+run_id)\n",
    "\n",
    "\n",
    "print('Total time: ', time.time()-start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "test_data = make_batch(1, 13, store_dist, recall_dist, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "a = test_data[:, 0 ,0].nonzero()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-40-a8c9fa0ea733>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "b = test_data[:a, 0, 1].nonzero()[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[True],\n        [True]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[b, 0, 2] == test_data[a, 0, 3]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}