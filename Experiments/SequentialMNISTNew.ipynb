{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from Code.envs.SequentialMNIST import SequentialMNIST\n",
    "from Code.train import train, OptWrapper\n",
    "from Code.everything5 import build_standard_loop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "spec = {\n",
    "    'control_config': {\n",
    "        'neuron_type': 'LIF',\n",
    "        'n_neurons': 120,\n",
    "        'TAU': 20,\n",
    "        '1-beta': False,\n",
    "        'SPIKE_FN': 'bellec'\n",
    "    },\n",
    "    'mem_config': {\n",
    "        'neuron_type': 'NoReset',\n",
    "        'n_neurons': 100,\n",
    "        'TAU': 700,\n",
    "        '1-beta': False,\n",
    "        'SPIKE_FN': 'bellec',\n",
    "        'GAMMA': 0.27,\n",
    "        'TAU_THR': 10000,\n",
    "    },\n",
    "    'experiment': 'SequentialMNIST',\n",
    "    'lr': 0.001,\n",
    "    'lr_decay': 0.9,\n",
    "    'iterations': 36000,\n",
    "    'batch_size': 64,\n",
    "    'architecture': '1L'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "\n",
    "train_problem = SequentialMNIST(spec['iterations'], spec['batch_size'], DEVICE, '../')\n",
    "val_problem = SequentialMNIST(-1, spec['batch_size'], DEVICE, '../', validate=True)\n",
    "\n",
    "\n",
    "n_in, n_out, input_rate = train_problem.get_infos()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'control': [[('input', 1), ('control', 1), ('mem', 1)], LIFNeuron(), <class 'torch.nn.modules.linear.Linear'>], 'mem': [[('input', 1), ('control', 1), ('mem', 1)], NoResetNeuron(), <class 'torch.nn.modules.linear.Linear'>], 'output': [[('control', 1), ('mem', 1)], BaseNeuron(), None]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "OuterWrapper(\n  (model): DynNetwork(\n    (layers): ModuleDict(\n      (loop): SequenceWrapper(\n        (model): ParallelNetwork2(\n          (layers): ModuleDict(\n            (control): LIFNeuron()\n            (control_synapse): Linear(in_features=301, out_features=120, bias=True)\n            (mem): NoResetNeuron()\n            (mem_synapse): Linear(in_features=301, out_features=100, bias=False)\n            (output): BaseNeuron()\n          )\n        )\n      )\n      (mean): MeanModule()\n      (output_synapse): Linear(in_features=220, out_features=10, bias=True)\n      (output): BaseNeuron()\n    )\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Code.everything6 import OuterWrapper, DynNetwork, ParallelNetwork2, SequenceWrapper, MeanModule, BaseNeuron, ParallelNetwork\n",
    "\n",
    "from Code.everything6 import LIFNeuron, SeqOnlySpike, AdaptiveNeuron, CooldownNeuron, NoResetNeuron\n",
    "def build_standard_loop(spec, n_input, input_rate):\n",
    "\n",
    "    neuron_lookup = {\n",
    "        'LIF': LIFNeuron,\n",
    "        'Disc': SeqOnlySpike,\n",
    "        'Adaptive': AdaptiveNeuron,\n",
    "        'Cooldown': CooldownNeuron,\n",
    "        'NoReset': NoResetNeuron,\n",
    "    }\n",
    "\n",
    "    control_neuron = neuron_lookup[spec['control_config']['neuron_type']](spec['control_config']['n_neurons'], spec['control_config'])\n",
    "    mem_neuron = neuron_lookup[spec['mem_config']['neuron_type']](spec['mem_config']['n_neurons'], spec['mem_config'])\n",
    "    out_neuron_size = control_neuron.out_size + mem_neuron.out_size\n",
    "    out_neuron = BaseNeuron(out_neuron_size, None)\n",
    "\n",
    "    loop_2L = {\n",
    "        'input': (n_input, input_rate),\n",
    "        'control': [['input', 'mem'], control_neuron, nn.Linear],\n",
    "        'mem': [['control'], mem_neuron, nn.Linear],\n",
    "        'output': [['control', 'mem'], out_neuron, None],\n",
    "    }\n",
    "\n",
    "    loop_1L = {\n",
    "        'input': (n_input, input_rate),\n",
    "        'control': [['input', 'control', 'mem'], control_neuron, nn.Linear],\n",
    "        'mem': [['input', 'control', 'mem'], mem_neuron, nn.Linear],\n",
    "        'output': [['control', 'mem'], out_neuron, None],\n",
    "    }\n",
    "\n",
    "    return loop_1L if spec['architecture'] == '1L' else loop_2L\n",
    "\n",
    "loop = build_standard_loop(spec, n_in, input_rate)\n",
    "loop_model = SequenceWrapper(ParallelNetwork2(loop))\n",
    "out_neuron_size = loop_model.out_size\n",
    "\n",
    "#TODO: this has to be ordered\n",
    "outer = {\n",
    "    'input': n_in,\n",
    "    'loop': [['input'], loop_model, None],\n",
    "    'mean': [['loop'], MeanModule(out_neuron_size, -56), None],\n",
    "    'output': [['mean'], BaseNeuron(n_out, None), nn.Linear]\n",
    "}\n",
    "\n",
    "model = OuterWrapper(DynNetwork(outer))\n",
    "model.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "optimizer = OptWrapper(model.parameters(), spec['lr'], spec['lr_decay'], 2500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 10.92% | Val Time: 97.8s | Time per it: 49.8s\n",
      "It:   20 | Loss: 2.316 | Acc: 10.00%\n",
      "It:   40 | Loss: 2.310 | Acc: 10.86%\n",
      "It:   60 | Loss: 2.307 | Acc: 11.95%\n",
      "It:   80 | Loss: 2.308 | Acc: 10.78%\n",
      "It:  100 | Loss: 2.309 | Acc: 10.94%\n",
      "Val Acc: 11.36% | Val Time: 104.2s | Time per it: 2.6s\n",
      "It:  120 | Loss: 2.304 | Acc: 9.61%\n",
      "It:  140 | Loss: 2.307 | Acc: 10.62%\n",
      "It:  160 | Loss: 2.304 | Acc: 10.47%\n",
      "It:  180 | Loss: 2.303 | Acc: 13.05%\n",
      "It:  200 | Loss: 2.303 | Acc: 11.25%\n",
      "Val Acc: 9.72% | Val Time: 103.0s | Time per it: 2.7s\n",
      "It:  220 | Loss: 2.306 | Acc: 10.31%\n",
      "It:  240 | Loss: 2.302 | Acc: 12.27%\n",
      "It:  260 | Loss: 2.306 | Acc: 10.78%\n",
      "It:  280 | Loss: 2.305 | Acc: 10.08%\n",
      "It:  300 | Loss: 2.308 | Acc: 9.30%\n",
      "Val Acc: 10.33% | Val Time: 97.5s | Time per it: 2.5s\n",
      "It:  320 | Loss: 2.304 | Acc: 10.00%\n",
      "It:  340 | Loss: 2.299 | Acc: 10.47%\n",
      "It:  360 | Loss: 2.309 | Acc: 9.06%\n",
      "It:  380 | Loss: 2.304 | Acc: 11.25%\n",
      "It:  400 | Loss: 2.305 | Acc: 11.41%\n",
      "Val Acc: 9.78% | Val Time: 97.4s | Time per it: 2.5s\n",
      "It:  420 | Loss: 2.303 | Acc: 10.55%\n",
      "It:  440 | Loss: 2.305 | Acc: 9.92%\n",
      "It:  460 | Loss: 2.304 | Acc: 10.62%\n",
      "It:  480 | Loss: 2.306 | Acc: 10.55%\n",
      "It:  500 | Loss: 2.304 | Acc: 11.41%\n",
      "Val Acc: 10.26% | Val Time: 108.1s | Time per it: 2.8s\n",
      "It:  520 | Loss: 2.303 | Acc: 11.95%\n",
      "It:  540 | Loss: 2.306 | Acc: 11.25%\n",
      "It:  560 | Loss: 2.304 | Acc: 11.09%\n",
      "It:  580 | Loss: 2.302 | Acc: 11.56%\n",
      "It:  600 | Loss: 2.306 | Acc: 11.25%\n",
      "Val Acc: 11.36% | Val Time: 113.5s | Time per it: 3.0s\n",
      "It:  620 | Loss: 2.309 | Acc: 10.47%\n",
      "It:  640 | Loss: 2.306 | Acc: 11.80%\n",
      "It:  660 | Loss: 2.305 | Acc: 9.14%\n",
      "It:  680 | Loss: 2.304 | Acc: 9.77%\n",
      "It:  700 | Loss: 2.307 | Acc: 10.39%\n",
      "Val Acc: 10.33% | Val Time: 114.2s | Time per it: 3.0s\n",
      "It:  720 | Loss: 2.306 | Acc: 10.23%\n",
      "It:  740 | Loss: 2.305 | Acc: 11.48%\n",
      "It:  760 | Loss: 2.306 | Acc: 10.08%\n",
      "It:  780 | Loss: 2.305 | Acc: 11.17%\n",
      "It:  800 | Loss: 2.305 | Acc: 10.55%\n",
      "Val Acc: 11.36% | Val Time: 113.9s | Time per it: 2.9s\n",
      "It:  820 | Loss: 2.308 | Acc: 9.61%\n",
      "It:  840 | Loss: 2.306 | Acc: 10.70%\n",
      "It:  860 | Loss: 2.300 | Acc: 11.95%\n",
      "It:  880 | Loss: 2.303 | Acc: 12.34%\n",
      "It:  900 | Loss: 2.306 | Acc: 11.80%\n",
      "Val Acc: 11.36% | Val Time: 96.3s | Time per it: 2.7s\n",
      "It:  920 | Loss: 2.305 | Acc: 10.08%\n",
      "It:  940 | Loss: 2.306 | Acc: 10.39%\n",
      "It:  960 | Loss: 2.305 | Acc: 11.09%\n",
      "It:  980 | Loss: 2.304 | Acc: 9.84%\n",
      "It: 1000 | Loss: 2.303 | Acc: 11.41%\n",
      "Val Acc: 9.78% | Val Time: 95.5s | Time per it: 2.5s\n",
      "It: 1020 | Loss: 2.306 | Acc: 8.91%\n",
      "It: 1040 | Loss: 2.308 | Acc: 9.53%\n",
      "It: 1060 | Loss: 2.307 | Acc: 10.39%\n",
      "It: 1080 | Loss: 2.303 | Acc: 10.08%\n",
      "It: 1100 | Loss: 2.305 | Acc: 9.22%\n",
      "Val Acc: 9.83% | Val Time: 93.3s | Time per it: 2.4s\n",
      "It: 1120 | Loss: 2.308 | Acc: 10.00%\n",
      "It: 1140 | Loss: 2.307 | Acc: 11.48%\n",
      "It: 1160 | Loss: 2.302 | Acc: 12.34%\n",
      "It: 1180 | Loss: 2.304 | Acc: 12.27%\n",
      "It: 1200 | Loss: 2.304 | Acc: 10.55%\n",
      "Val Acc: 9.78% | Val Time: 93.1s | Time per it: 2.4s\n",
      "It: 1220 | Loss: 2.303 | Acc: 9.45%\n",
      "It: 1240 | Loss: 2.303 | Acc: 10.39%\n",
      "It: 1260 | Loss: 2.304 | Acc: 9.77%\n",
      "It: 1280 | Loss: 2.305 | Acc: 10.31%\n",
      "It: 1300 | Loss: 2.299 | Acc: 11.88%\n",
      "Val Acc: 11.36% | Val Time: 94.0s | Time per it: 2.4s\n",
      "It: 1320 | Loss: 2.300 | Acc: 12.58%\n",
      "It: 1340 | Loss: 2.308 | Acc: 10.23%\n",
      "It: 1360 | Loss: 2.308 | Acc: 9.77%\n",
      "It: 1380 | Loss: 2.305 | Acc: 9.38%\n",
      "It: 1400 | Loss: 2.304 | Acc: 9.92%\n",
      "Val Acc: 10.26% | Val Time: 93.7s | Time per it: 2.4s\n",
      "It: 1420 | Loss: 2.306 | Acc: 10.00%\n",
      "It: 1440 | Loss: 2.305 | Acc: 9.45%\n",
      "It: 1460 | Loss: 2.306 | Acc: 11.09%\n",
      "It: 1480 | Loss: 2.306 | Acc: 9.30%\n",
      "It: 1500 | Loss: 2.306 | Acc: 8.75%\n",
      "Val Acc: 11.36% | Val Time: 93.5s | Time per it: 2.4s\n",
      "It: 1520 | Loss: 2.300 | Acc: 11.09%\n",
      "It: 1540 | Loss: 2.303 | Acc: 11.80%\n",
      "It: 1560 | Loss: 2.304 | Acc: 12.27%\n",
      "It: 1580 | Loss: 2.309 | Acc: 8.52%\n",
      "It: 1600 | Loss: 2.306 | Acc: 11.02%\n",
      "Val Acc: 10.26% | Val Time: 93.5s | Time per it: 2.4s\n",
      "It: 1620 | Loss: 2.305 | Acc: 9.06%\n",
      "It: 1640 | Loss: 2.304 | Acc: 10.94%\n",
      "It: 1660 | Loss: 2.303 | Acc: 12.50%\n",
      "It: 1680 | Loss: 2.306 | Acc: 11.09%\n",
      "It: 1700 | Loss: 2.307 | Acc: 10.08%\n",
      "Val Acc: 10.26% | Val Time: 93.0s | Time per it: 2.4s\n",
      "It: 1720 | Loss: 2.309 | Acc: 10.47%\n",
      "It: 1740 | Loss: 2.300 | Acc: 12.03%\n",
      "It: 1760 | Loss: 2.310 | Acc: 10.00%\n",
      "It: 1780 | Loss: 2.306 | Acc: 8.44%\n",
      "It: 1800 | Loss: 2.304 | Acc: 9.92%\n",
      "Val Acc: 11.36% | Val Time: 94.2s | Time per it: 2.4s\n",
      "It: 1820 | Loss: 2.305 | Acc: 10.08%\n",
      "It: 1840 | Loss: 2.307 | Acc: 9.45%\n",
      "It: 1860 | Loss: 2.303 | Acc: 11.09%\n",
      "It: 1880 | Loss: 2.306 | Acc: 10.78%\n",
      "It: 1900 | Loss: 2.304 | Acc: 10.94%\n",
      "Val Acc: 10.07% | Val Time: 93.7s | Time per it: 2.4s\n",
      "It: 1920 | Loss: 2.305 | Acc: 10.08%\n",
      "It: 1940 | Loss: 2.305 | Acc: 10.39%\n",
      "It: 1960 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 1980 | Loss: 2.300 | Acc: 12.03%\n",
      "It: 2000 | Loss: 2.309 | Acc: 9.84%\n",
      "Val Acc: 10.26% | Val Time: 93.3s | Time per it: 2.4s\n",
      "It: 2020 | Loss: 2.301 | Acc: 11.64%\n",
      "It: 2040 | Loss: 2.305 | Acc: 9.77%\n",
      "It: 2060 | Loss: 2.301 | Acc: 12.11%\n",
      "It: 2080 | Loss: 2.303 | Acc: 12.42%\n",
      "It: 2100 | Loss: 2.308 | Acc: 9.45%\n",
      "Val Acc: 10.33% | Val Time: 93.4s | Time per it: 2.4s\n",
      "It: 2120 | Loss: 2.306 | Acc: 9.84%\n",
      "It: 2140 | Loss: 2.310 | Acc: 10.16%\n",
      "It: 2160 | Loss: 2.303 | Acc: 9.69%\n",
      "It: 2180 | Loss: 2.310 | Acc: 9.61%\n",
      "It: 2200 | Loss: 2.302 | Acc: 11.41%\n",
      "Val Acc: 10.26% | Val Time: 93.3s | Time per it: 2.4s\n",
      "It: 2220 | Loss: 2.305 | Acc: 8.98%\n",
      "It: 2240 | Loss: 2.303 | Acc: 12.03%\n",
      "It: 2260 | Loss: 2.299 | Acc: 13.44%\n",
      "It: 2280 | Loss: 2.310 | Acc: 10.23%\n",
      "It: 2300 | Loss: 2.308 | Acc: 9.45%\n",
      "Val Acc: 10.26% | Val Time: 94.0s | Time per it: 2.4s\n",
      "It: 2320 | Loss: 2.306 | Acc: 11.72%\n",
      "It: 2340 | Loss: 2.308 | Acc: 9.38%\n",
      "It: 2360 | Loss: 2.306 | Acc: 10.94%\n",
      "It: 2380 | Loss: 2.304 | Acc: 10.55%\n",
      "It: 2400 | Loss: 2.305 | Acc: 11.02%\n",
      "Val Acc: 11.36% | Val Time: 93.2s | Time per it: 2.4s\n",
      "It: 2420 | Loss: 2.307 | Acc: 9.69%\n",
      "It: 2440 | Loss: 2.305 | Acc: 10.31%\n",
      "It: 2460 | Loss: 2.304 | Acc: 10.55%\n",
      "It: 2480 | Loss: 2.303 | Acc: 12.27%\n",
      "It: 2500 | Loss: 2.306 | Acc: 9.84%\n",
      "Val Acc: 11.36% | Val Time: 93.6s | Time per it: 2.4s\n",
      "It: 2520 | Loss: 2.304 | Acc: 9.22%\n",
      "It: 2540 | Loss: 2.307 | Acc: 9.77%\n",
      "It: 2560 | Loss: 2.300 | Acc: 12.34%\n",
      "It: 2580 | Loss: 2.309 | Acc: 9.45%\n",
      "It: 2600 | Loss: 2.306 | Acc: 9.69%\n",
      "Val Acc: 9.72% | Val Time: 94.4s | Time per it: 2.4s\n",
      "It: 2620 | Loss: 2.306 | Acc: 10.47%\n",
      "It: 2640 | Loss: 2.306 | Acc: 10.55%\n",
      "It: 2660 | Loss: 2.305 | Acc: 11.25%\n",
      "It: 2680 | Loss: 2.301 | Acc: 12.42%\n",
      "It: 2700 | Loss: 2.304 | Acc: 12.03%\n",
      "Val Acc: 11.36% | Val Time: 93.7s | Time per it: 2.4s\n",
      "It: 2720 | Loss: 2.302 | Acc: 11.09%\n",
      "It: 2740 | Loss: 2.301 | Acc: 10.70%\n",
      "It: 2760 | Loss: 2.309 | Acc: 12.03%\n",
      "It: 2780 | Loss: 2.304 | Acc: 11.33%\n",
      "It: 2800 | Loss: 2.306 | Acc: 8.52%\n",
      "Val Acc: 9.72% | Val Time: 93.9s | Time per it: 2.4s\n",
      "It: 2820 | Loss: 2.304 | Acc: 8.75%\n",
      "It: 2840 | Loss: 2.301 | Acc: 10.00%\n",
      "It: 2860 | Loss: 2.306 | Acc: 10.86%\n",
      "It: 2880 | Loss: 2.303 | Acc: 9.53%\n",
      "It: 2900 | Loss: 2.308 | Acc: 10.39%\n",
      "Val Acc: 10.07% | Val Time: 94.1s | Time per it: 2.4s\n",
      "It: 2920 | Loss: 2.304 | Acc: 10.08%\n",
      "It: 2940 | Loss: 2.304 | Acc: 10.78%\n",
      "It: 2960 | Loss: 2.305 | Acc: 10.62%\n",
      "It: 2980 | Loss: 2.301 | Acc: 10.62%\n",
      "It: 3000 | Loss: 2.302 | Acc: 10.78%\n",
      "Val Acc: 10.26% | Val Time: 94.6s | Time per it: 2.4s\n",
      "It: 3020 | Loss: 2.305 | Acc: 10.08%\n",
      "It: 3040 | Loss: 2.308 | Acc: 9.69%\n",
      "It: 3060 | Loss: 2.303 | Acc: 11.09%\n",
      "It: 3080 | Loss: 2.302 | Acc: 10.31%\n",
      "It: 3100 | Loss: 2.306 | Acc: 10.78%\n",
      "Val Acc: 11.36% | Val Time: 95.8s | Time per it: 2.4s\n",
      "It: 3120 | Loss: 2.303 | Acc: 11.80%\n",
      "It: 3140 | Loss: 2.309 | Acc: 9.61%\n",
      "It: 3160 | Loss: 2.302 | Acc: 10.39%\n",
      "It: 3180 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 3200 | Loss: 2.304 | Acc: 11.17%\n",
      "Val Acc: 10.26% | Val Time: 111.0s | Time per it: 2.8s\n",
      "It: 3220 | Loss: 2.298 | Acc: 13.28%\n",
      "It: 3240 | Loss: 2.305 | Acc: 11.80%\n",
      "It: 3260 | Loss: 2.307 | Acc: 9.53%\n",
      "It: 3280 | Loss: 2.304 | Acc: 10.08%\n",
      "It: 3300 | Loss: 2.304 | Acc: 10.78%\n",
      "Val Acc: 11.36% | Val Time: 110.4s | Time per it: 2.9s\n",
      "It: 3320 | Loss: 2.304 | Acc: 10.31%\n",
      "It: 3340 | Loss: 2.301 | Acc: 10.55%\n",
      "It: 3360 | Loss: 2.306 | Acc: 9.06%\n",
      "It: 3380 | Loss: 2.301 | Acc: 9.53%\n",
      "It: 3400 | Loss: 2.307 | Acc: 9.30%\n",
      "Val Acc: 9.59% | Val Time: 94.3s | Time per it: 2.4s\n",
      "It: 3420 | Loss: 2.305 | Acc: 9.92%\n",
      "It: 3440 | Loss: 2.303 | Acc: 11.17%\n",
      "It: 3460 | Loss: 2.307 | Acc: 8.59%\n",
      "It: 3480 | Loss: 2.304 | Acc: 11.41%\n",
      "It: 3500 | Loss: 2.301 | Acc: 12.89%\n",
      "Val Acc: 11.36% | Val Time: 95.0s | Time per it: 2.5s\n",
      "It: 3520 | Loss: 2.304 | Acc: 11.64%\n",
      "It: 3540 | Loss: 2.304 | Acc: 11.72%\n",
      "It: 3560 | Loss: 2.301 | Acc: 11.09%\n",
      "It: 3580 | Loss: 2.307 | Acc: 11.56%\n",
      "It: 3600 | Loss: 2.303 | Acc: 11.72%\n",
      "Val Acc: 11.36% | Val Time: 104.9s | Time per it: 2.6s\n",
      "It: 3620 | Loss: 2.305 | Acc: 10.16%\n",
      "It: 3640 | Loss: 2.303 | Acc: 11.09%\n",
      "It: 3660 | Loss: 2.307 | Acc: 9.69%\n",
      "It: 3680 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 3700 | Loss: 2.306 | Acc: 9.92%\n",
      "Val Acc: 11.36% | Val Time: 95.2s | Time per it: 2.6s\n",
      "It: 3720 | Loss: 2.305 | Acc: 11.17%\n",
      "It: 3740 | Loss: 2.302 | Acc: 11.41%\n",
      "It: 3760 | Loss: 2.302 | Acc: 10.16%\n",
      "It: 3780 | Loss: 2.305 | Acc: 11.64%\n",
      "It: 3800 | Loss: 2.299 | Acc: 12.66%\n",
      "Val Acc: 11.36% | Val Time: 94.7s | Time per it: 2.4s\n",
      "It: 3820 | Loss: 2.313 | Acc: 10.08%\n",
      "It: 3840 | Loss: 2.305 | Acc: 9.38%\n",
      "It: 3860 | Loss: 2.302 | Acc: 11.17%\n",
      "It: 3880 | Loss: 2.303 | Acc: 11.33%\n",
      "It: 3900 | Loss: 2.306 | Acc: 11.02%\n",
      "Val Acc: 11.36% | Val Time: 93.9s | Time per it: 2.4s\n",
      "It: 3920 | Loss: 2.307 | Acc: 8.59%\n",
      "It: 3940 | Loss: 2.306 | Acc: 9.14%\n",
      "It: 3960 | Loss: 2.306 | Acc: 9.77%\n",
      "It: 3980 | Loss: 2.304 | Acc: 10.31%\n",
      "It: 4000 | Loss: 2.305 | Acc: 10.78%\n",
      "Val Acc: 9.59% | Val Time: 93.9s | Time per it: 2.4s\n",
      "It: 4020 | Loss: 2.306 | Acc: 11.48%\n",
      "It: 4040 | Loss: 2.300 | Acc: 11.56%\n",
      "It: 4060 | Loss: 2.305 | Acc: 10.39%\n",
      "It: 4080 | Loss: 2.308 | Acc: 10.16%\n",
      "It: 4100 | Loss: 2.306 | Acc: 9.61%\n",
      "Val Acc: 11.36% | Val Time: 94.0s | Time per it: 2.4s\n",
      "It: 4120 | Loss: 2.305 | Acc: 10.31%\n",
      "It: 4140 | Loss: 2.302 | Acc: 11.64%\n",
      "It: 4160 | Loss: 2.302 | Acc: 10.86%\n",
      "It: 4180 | Loss: 2.310 | Acc: 9.92%\n",
      "It: 4200 | Loss: 2.306 | Acc: 9.53%\n",
      "Val Acc: 10.33% | Val Time: 94.3s | Time per it: 2.4s\n",
      "It: 4220 | Loss: 2.303 | Acc: 11.64%\n",
      "It: 4240 | Loss: 2.306 | Acc: 9.61%\n",
      "It: 4260 | Loss: 2.305 | Acc: 9.53%\n",
      "It: 4280 | Loss: 2.303 | Acc: 11.25%\n",
      "It: 4300 | Loss: 2.304 | Acc: 10.00%\n",
      "Val Acc: 10.11% | Val Time: 93.3s | Time per it: 2.4s\n",
      "It: 4320 | Loss: 2.305 | Acc: 9.38%\n",
      "It: 4340 | Loss: 2.303 | Acc: 12.97%\n",
      "It: 4360 | Loss: 2.300 | Acc: 11.41%\n",
      "It: 4380 | Loss: 2.300 | Acc: 11.72%\n",
      "It: 4400 | Loss: 2.302 | Acc: 12.81%\n",
      "Val Acc: 11.36% | Val Time: 94.0s | Time per it: 2.4s\n",
      "It: 4420 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 4440 | Loss: 2.306 | Acc: 10.39%\n",
      "It: 4460 | Loss: 2.304 | Acc: 10.86%\n",
      "It: 4480 | Loss: 2.308 | Acc: 9.53%\n",
      "It: 4500 | Loss: 2.306 | Acc: 9.14%\n",
      "Val Acc: 9.83% | Val Time: 93.7s | Time per it: 2.4s\n",
      "It: 4520 | Loss: 2.307 | Acc: 9.69%\n",
      "It: 4540 | Loss: 2.300 | Acc: 10.70%\n",
      "It: 4560 | Loss: 2.308 | Acc: 12.03%\n",
      "It: 4580 | Loss: 2.306 | Acc: 10.55%\n",
      "It: 4600 | Loss: 2.307 | Acc: 10.16%\n",
      "Val Acc: 10.07% | Val Time: 94.1s | Time per it: 2.4s\n",
      "It: 4620 | Loss: 2.306 | Acc: 9.69%\n",
      "It: 4640 | Loss: 2.300 | Acc: 11.09%\n",
      "It: 4660 | Loss: 2.303 | Acc: 11.88%\n",
      "It: 4680 | Loss: 2.306 | Acc: 10.78%\n",
      "It: 4700 | Loss: 2.305 | Acc: 11.48%\n",
      "Val Acc: 11.36% | Val Time: 93.8s | Time per it: 2.4s\n",
      "It: 4720 | Loss: 2.304 | Acc: 10.70%\n",
      "It: 4740 | Loss: 2.304 | Acc: 10.62%\n",
      "It: 4760 | Loss: 2.303 | Acc: 11.33%\n",
      "It: 4780 | Loss: 2.307 | Acc: 10.70%\n",
      "It: 4800 | Loss: 2.306 | Acc: 10.70%\n",
      "Val Acc: 10.26% | Val Time: 94.0s | Time per it: 2.4s\n",
      "It: 4820 | Loss: 2.306 | Acc: 10.47%\n",
      "It: 4840 | Loss: 2.303 | Acc: 11.25%\n",
      "It: 4860 | Loss: 2.302 | Acc: 10.94%\n",
      "It: 4880 | Loss: 2.305 | Acc: 11.33%\n",
      "It: 4900 | Loss: 2.298 | Acc: 10.78%\n",
      "Val Acc: 10.33% | Val Time: 101.3s | Time per it: 2.6s\n",
      "It: 4920 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 4940 | Loss: 2.306 | Acc: 12.42%\n",
      "It: 4960 | Loss: 2.305 | Acc: 10.47%\n",
      "It: 4980 | Loss: 2.307 | Acc: 9.38%\n",
      "It: 5000 | Loss: 2.304 | Acc: 10.86%\n",
      "Val Acc: 10.11% | Val Time: 100.2s | Time per it: 2.6s\n",
      "It: 5020 | Loss: 2.306 | Acc: 11.09%\n",
      "It: 5040 | Loss: 2.305 | Acc: 11.25%\n",
      "It: 5060 | Loss: 2.301 | Acc: 11.56%\n",
      "It: 5080 | Loss: 2.305 | Acc: 12.42%\n",
      "It: 5100 | Loss: 2.306 | Acc: 11.33%\n",
      "Val Acc: 11.36% | Val Time: 100.2s | Time per it: 2.6s\n",
      "It: 5120 | Loss: 2.302 | Acc: 10.55%\n",
      "It: 5140 | Loss: 2.306 | Acc: 10.62%\n",
      "It: 5160 | Loss: 2.304 | Acc: 9.45%\n",
      "It: 5180 | Loss: 2.303 | Acc: 11.09%\n",
      "It: 5200 | Loss: 2.307 | Acc: 8.20%\n",
      "Val Acc: 11.36% | Val Time: 102.5s | Time per it: 2.6s\n",
      "It: 5220 | Loss: 2.303 | Acc: 11.95%\n",
      "It: 5240 | Loss: 2.301 | Acc: 12.11%\n",
      "It: 5260 | Loss: 2.302 | Acc: 10.00%\n",
      "It: 5280 | Loss: 2.306 | Acc: 11.56%\n",
      "It: 5300 | Loss: 2.306 | Acc: 9.53%\n",
      "Val Acc: 9.78% | Val Time: 101.2s | Time per it: 2.6s\n",
      "It: 5320 | Loss: 2.305 | Acc: 10.70%\n",
      "It: 5340 | Loss: 2.309 | Acc: 9.22%\n",
      "It: 5360 | Loss: 2.305 | Acc: 10.00%\n",
      "It: 5380 | Loss: 2.304 | Acc: 10.08%\n",
      "It: 5400 | Loss: 2.309 | Acc: 9.69%\n",
      "Val Acc: 11.36% | Val Time: 100.4s | Time per it: 2.6s\n",
      "It: 5420 | Loss: 2.303 | Acc: 12.42%\n",
      "It: 5440 | Loss: 2.304 | Acc: 10.23%\n",
      "It: 5460 | Loss: 2.302 | Acc: 11.80%\n",
      "It: 5480 | Loss: 2.306 | Acc: 10.16%\n",
      "It: 5500 | Loss: 2.305 | Acc: 10.23%\n",
      "Val Acc: 10.11% | Val Time: 94.6s | Time per it: 2.5s\n",
      "It: 5520 | Loss: 2.304 | Acc: 12.19%\n",
      "It: 5540 | Loss: 2.309 | Acc: 9.30%\n",
      "It: 5560 | Loss: 2.302 | Acc: 12.34%\n",
      "It: 5580 | Loss: 2.304 | Acc: 10.47%\n",
      "It: 5600 | Loss: 2.303 | Acc: 10.16%\n",
      "Val Acc: 11.36% | Val Time: 96.9s | Time per it: 2.5s\n",
      "It: 5620 | Loss: 2.305 | Acc: 10.86%\n",
      "It: 5640 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 5660 | Loss: 2.303 | Acc: 10.86%\n",
      "It: 5680 | Loss: 2.305 | Acc: 10.78%\n",
      "It: 5700 | Loss: 2.306 | Acc: 9.77%\n",
      "Val Acc: 9.72% | Val Time: 99.2s | Time per it: 2.5s\n",
      "It: 5720 | Loss: 2.302 | Acc: 9.77%\n",
      "It: 5740 | Loss: 2.303 | Acc: 10.70%\n",
      "It: 5760 | Loss: 2.304 | Acc: 11.95%\n",
      "It: 5780 | Loss: 2.304 | Acc: 11.41%\n",
      "It: 5800 | Loss: 2.303 | Acc: 9.69%\n",
      "Val Acc: 9.59% | Val Time: 98.9s | Time per it: 2.6s\n",
      "It: 5820 | Loss: 2.309 | Acc: 8.75%\n",
      "It: 5840 | Loss: 2.300 | Acc: 12.03%\n",
      "It: 5860 | Loss: 2.303 | Acc: 10.08%\n",
      "It: 5880 | Loss: 2.304 | Acc: 11.48%\n",
      "It: 5900 | Loss: 2.304 | Acc: 9.30%\n",
      "Val Acc: 10.33% | Val Time: 97.5s | Time per it: 2.6s\n",
      "It: 5920 | Loss: 2.305 | Acc: 10.70%\n",
      "It: 5940 | Loss: 2.304 | Acc: 9.92%\n",
      "It: 5960 | Loss: 2.306 | Acc: 10.23%\n",
      "It: 5980 | Loss: 2.305 | Acc: 10.86%\n",
      "It: 6000 | Loss: 2.304 | Acc: 10.62%\n",
      "Val Acc: 10.11% | Val Time: 101.4s | Time per it: 2.6s\n",
      "It: 6020 | Loss: 2.304 | Acc: 11.41%\n",
      "It: 6040 | Loss: 2.303 | Acc: 11.88%\n",
      "It: 6060 | Loss: 2.301 | Acc: 11.64%\n",
      "It: 6080 | Loss: 2.309 | Acc: 9.14%\n",
      "It: 6100 | Loss: 2.304 | Acc: 8.52%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-7e0f344e1f2a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_problem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_problem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/train.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(train_problem, val_problem, optimizer, model, run_id, print_every, validate_every)\u001B[0m\n\u001B[1;32m     90\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0minput\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mval_problem\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m                     \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m                     \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mval_problem\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_and_acc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m                     \u001B[0mval_acc\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0macc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/everything6.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h, logging)\u001B[0m\n\u001B[1;32m    352\u001B[0m             \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_initial_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtwo_dim\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    353\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_logging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlogging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 354\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    355\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    356\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maddr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/everything6.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    263\u001B[0m                 \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m'_synapse'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_logging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_logging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 265\u001B[0;31m             \u001B[0mtmp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midxState\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    266\u001B[0m             \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtmp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_logging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/everything6.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    332\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menter_log\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mentry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 334\u001B[0;31m                 \u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    335\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_logging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    336\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/everything6.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m                     \u001B[0minputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m                 \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m                 \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ml_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(train_problem, val_problem, optimizer, model, None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input = train_problem.make_inputs()#[:,:1]\n",
    "out, _, log = OuterWrapper(model.model.layers.loop).to(DEVICE)(input, logging=True)\n",
    "ar_length = input.shape[0]\n",
    "array_list = [input, log['control'], log['mem']] #log['loop']['output']\n",
    "array_list2 = []\n",
    "for ar in array_list:\n",
    "    array_list2.append(ar[:, 0])\n",
    "    array_list2.append(torch.ones((ar_length, 1), device=input.device) * 0.5)\n",
    "big_ar = (1-torch.cat(array_list2[:-1], dim=1).detach()).t() * 255\n",
    "img = Image.fromarray(big_ar.cpu().numpy().astype(np.uint8), 'L')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log['control'].mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}