{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Code.envs.GPEnv import MultiEnv\n",
    "from wip.Code.train_q import make_dataset, backward_one\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 512#64\n",
    "SIM_TIME = 1\n",
    "MAX_ITER = 16\n",
    "USE_JIT = False\n",
    "DIMS = 2\n",
    "NUM_ACTIONS = 32\n",
    "MEM_SIZE = 128+MAX_ITER\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "env = MultiEnv(BATCH_SIZE, MAX_ITER, device, dims=DIMS)\n",
    "\n",
    "#torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from Code.Networks import OuterWrapper\n",
    "#model = torch.load('../../models/rsnn_gppred4')\n",
    "model = torch.load('../../models/rsnn_gppred2d1')\n",
    "model_memory = model.layers.mem_loop.model.model\n",
    "\n",
    "#model = OuterWrapper(model, device, USE_JIT)\n",
    "model_memory = OuterWrapper(model_memory, device, USE_JIT, two_dim=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from Code.Networks import Selector, DynNetwork, OuterWrapper, LSTMWrapper, ReLuWrapper, DummyNeuron, make_SequenceWrapper\n",
    "from Code.NewNeurons import SeqOnlySpike, CooldownNeuron\n",
    "\n",
    "s_architecture = OrderedDict([\n",
    "    ('input', MEM_SIZE), #128\n",
    "    ('layer1', [['input'], ReLuWrapper(256), nn.Linear]),\n",
    "    ('output', [['layer1'], ReLuWrapper(64), nn.Linear]),\n",
    "])\n",
    "\n",
    "q_architecture = OrderedDict([\n",
    "    ('input', 64+DIMS),\n",
    "    ('layer1', [['input'], ReLuWrapper(128), nn.Linear]),\n",
    "    ('layer2', [['layer1'], ReLuWrapper(64), nn.Linear]),\n",
    "    ('output', [['layer2'], DummyNeuron(1), nn.Linear]),\n",
    "])\n",
    "\n",
    "p_architecture = OrderedDict([\n",
    "    ('input', MEM_SIZE), #128\n",
    "    ('layer1', [['input'], ReLuWrapper(256), nn.Linear]),\n",
    "    ('layer2', [['layer1'], ReLuWrapper(128), nn.Linear]),\n",
    "    ('output', [['layer2'], DummyNeuron(DIMS), nn.Linear]),\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#144, 150, 137, 150\n",
    "\n",
    "s_model = OuterWrapper(DynNetwork(s_architecture), device, USE_JIT)\n",
    "q_model = OuterWrapper(DynNetwork(q_architecture), device, USE_JIT)\n",
    "p_model = OuterWrapper(DynNetwork(p_architecture), device, USE_JIT)\n",
    "\n",
    "#model = (OuterWrapper(DynNetwork(architecturelstm), device, True))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#params = list(s_model.parameters())+list(p_model.parameters())+list(q_model.parameters())\n",
    "params = p_model.parameters()\n",
    "optimizer = optim.Adam(params, lr=1e-4)#1e-4\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def train_dataset(num_batches, num_epochs, gamma, rand_prob):\n",
    "    obs, mem, value, actions, targets, avg_reward, _ = make_dataset(num_batches, BATCH_SIZE, MAX_ITER, s_model, q_model, p_model, model_memory, device, env, gamma, NUM_ACTIONS, DIMS, rand_prob)\n",
    "    print('Avarage Reward: ', avg_reward)\n",
    "    print('Action Variance: ', actions.var(0).mean())\n",
    "    print('Avarage Start Value: ', value[0].mean())\n",
    "    for e in range(num_epochs):\n",
    "        start = time.time()\n",
    "        idc = torch.randperm(num_batches*BATCH_SIZE*MAX_ITER, device=device)\n",
    "        sum_v = 0\n",
    "        sum_p = 0\n",
    "        for i in range(num_batches*MAX_ITER):\n",
    "            base = i*BATCH_SIZE\n",
    "            batch_mem = mem.view(-1, MEM_SIZE)[idc[base:base + BATCH_SIZE]]\n",
    "            batch_value = value.view(-1, 1)[idc[base:base + BATCH_SIZE]]\n",
    "            batch_target = targets.view(-1, DIMS)[idc[base:base + BATCH_SIZE]]\n",
    "            batch_action = actions.view(-1, DIMS)[idc[base:base + BATCH_SIZE]]\n",
    "            s_model.zero_grad()\n",
    "            q_model.zero_grad()\n",
    "            p_model.zero_grad()\n",
    "            lossv, lossp = backward_one(batch_action, batch_target, batch_mem, batch_value, s_model, q_model, p_model, device)\n",
    "            sum_v += lossv\n",
    "            sum_p += lossp\n",
    "            optimizer.step()\n",
    "        for p in model.parameters():\n",
    "            if torch.isnan(p).any():\n",
    "                raise Exception('Corrupted Model')\n",
    "        print(sum_v / (num_batches*MAX_ITER), sum_p / (num_batches*MAX_ITER), time.time()-start)\n",
    "            #if i%10 == 0:\n",
    "                #print(loss.item(), (loss/targets.view(-1).var()).item(), i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigstep:  0\n",
      "Avarage Reward:  tensor(1.6712, device='cuda:0')\n",
      "Action Variance:  tensor(0.1102, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7157, device='cuda:0')\n",
      "0.027358374819741585 0.05745071545476094 19.047433137893677\n",
      "0.027358374771429227 0.05628209276823327 19.119175910949707\n",
      "0.027358374804607594 0.055789627300109716 18.997759342193604\n",
      "0.02735837478976464 0.05543964232201688 19.086837768554688\n",
      "0.027358374827890657 0.05515270865871571 19.161126136779785\n",
      "Bigstep:  1\n",
      "Avarage Reward:  tensor(1.6704, device='cuda:0')\n",
      "Action Variance:  tensor(0.1095, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7144, device='cuda:0')\n",
      "0.027397648101323283 0.05427356669562869 19.101285696029663\n",
      "0.027397648014011792 0.053851604112423956 18.965389013290405\n",
      "0.02739764810161432 0.053563567093806344 19.0363609790802\n",
      "0.027397648071637377 0.053317585923941806 19.009596824645996\n",
      "0.027397648075711913 0.05312126914737746 19.13011646270752\n",
      "Bigstep:  2\n",
      "Avarage Reward:  tensor(1.6648, device='cuda:0')\n",
      "Action Variance:  tensor(0.1096, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7135, device='cuda:0')\n",
      "0.027122134587552863 0.05357016133493744 19.065943956375122\n",
      "0.02712213458493352 0.05321568330051377 19.20324683189392\n",
      "0.027122134652163368 0.05294583780225366 19.097158908843994\n",
      "0.02712213462393265 0.05272359181428328 19.116939544677734\n",
      "0.02712213457620237 0.05255948789650575 19.127525091171265\n",
      "Bigstep:  3\n",
      "Avarage Reward:  tensor(1.6690, device='cuda:0')\n",
      "Action Variance:  tensor(0.1097, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7174, device='cuda:0')\n",
      "0.0270078411148279 0.053246237265411764 19.159260511398315\n",
      "0.027007841033046133 0.05290104408166371 19.08776354789734\n",
      "0.027007841044396626 0.05267242619185709 19.02749276161194\n",
      "0.027007841095619368 0.0524852603324689 19.065362691879272\n",
      "0.02700784109532833 0.05234046239987947 18.948514938354492\n",
      "Bigstep:  4\n",
      "Avarage Reward:  tensor(1.6655, device='cuda:0')\n",
      "Action Variance:  tensor(0.1098, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7123, device='cuda:0')\n",
      "0.027285137461440172 0.0526396557840053 18.49010968208313\n",
      "0.02728513747366378 0.05231423975666985 18.93098211288452\n",
      "0.027285137433209455 0.05209741955739446 18.924270391464233\n",
      "0.027285137454164214 0.05192484206985682 19.223706483840942\n",
      "0.027285137435246725 0.05177323941490613 19.054163217544556\n",
      "Bigstep:  5\n",
      "Avarage Reward:  tensor(1.6686, device='cuda:0')\n",
      "Action Variance:  tensor(0.1097, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7127, device='cuda:0')\n",
      "0.027308831211121286 0.05236007367377169 19.573877573013306\n",
      "0.027308831244590694 0.05203116941964254 19.65570616722107\n",
      "0.027308831253030803 0.051798888416960835 19.689799547195435\n",
      "0.02730883122974774 0.05163558947155252 19.89410901069641\n",
      "0.027308831246336922 0.05147454318357632 19.971518754959106\n",
      "Bigstep:  6\n",
      "Avarage Reward:  tensor(1.6618, device='cuda:0')\n",
      "Action Variance:  tensor(0.1097, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7109, device='cuda:0')\n",
      "0.027142078109027353 0.05214990678709 19.791412591934204\n",
      "0.027142078127944842 0.05182239594054408 19.62088108062744\n",
      "0.027142078105243855 0.051621239132946355 19.976426124572754\n",
      "0.027142078120959923 0.05145991370431147 19.532628774642944\n",
      "0.027142078109027353 0.05133184571983293 19.71491765975952\n",
      "Bigstep:  7\n",
      "Avarage Reward:  tensor(1.6602, device='cuda:0')\n",
      "Action Variance:  tensor(0.1098, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7086, device='cuda:0')\n",
      "0.027181248074630276 0.05193523714784533 19.17828941345215\n",
      "0.027181248066190166 0.051628058439819144 19.15553593635559\n",
      "0.027181248069391584 0.051420428194105625 19.172094345092773\n",
      "0.027181248039123602 0.05125243417685851 19.11389946937561\n",
      "0.027181248074630276 0.051117598344571885 19.013258934020996\n",
      "Bigstep:  8\n",
      "Avarage Reward:  tensor(1.6696, device='cuda:0')\n",
      "Action Variance:  tensor(0.1096, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7172, device='cuda:0')\n",
      "0.027133536320761778 0.05174292061827145 19.607067108154297\n",
      "0.027133536309411285 0.0514256817928981 19.174190044403076\n",
      "0.027133536303590517 0.051231772005558014 19.16084575653076\n",
      "0.027133536311157513 0.0510677320510149 19.105499505996704\n",
      "0.02713353629835183 0.050944038588786496 19.44466996192932\n",
      "Bigstep:  9\n",
      "Avarage Reward:  tensor(1.6697, device='cuda:0')\n",
      "Action Variance:  tensor(0.1098, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7141, device='cuda:0')\n",
      "0.027310628920968156 0.05151041794801131 19.28553056716919\n",
      "0.027310628943669143 0.05119170548161492 19.495393991470337\n",
      "0.027310628906125203 0.05098985252669081 19.33109211921692\n",
      "0.027310628932900725 0.050835495420033114 19.33651876449585\n",
      "0.027310628923296464 0.05070674527203664 19.248891830444336\n",
      "Bigstep:  10\n",
      "Avarage Reward:  tensor(1.6618, device='cuda:0')\n",
      "Action Variance:  tensor(0.1098, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7106, device='cuda:0')\n",
      "0.027153552600648254 0.05137374282698147 19.31169295310974\n",
      "0.027153552638192194 0.051044505827594545 19.27025842666626\n",
      "0.027153552640229464 0.05083927278406918 19.27907633781433\n",
      "0.027153552633535583 0.05070376013987698 19.381970405578613\n",
      "0.027153552596864756 0.05057509705657139 19.48109269142151\n",
      "Bigstep:  11\n",
      "Avarage Reward:  tensor(1.6607, device='cuda:0')\n",
      "Action Variance:  tensor(0.1100, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7091, device='cuda:0')\n",
      "0.02715240420802729 0.05140957092633471 19.57497000694275\n",
      "0.027152404204243794 0.051103379940614105 19.421489477157593\n",
      "0.027152404192020186 0.050898449331289154 19.35420823097229\n",
      "0.027152404185617342 0.05075672780163586 19.36040735244751\n",
      "0.027152404238877353 0.05063316141022369 19.45110511779785\n",
      "Bigstep:  12\n",
      "Avarage Reward:  tensor(1.6621, device='cuda:0')\n",
      "Action Variance:  tensor(0.1100, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7153, device='cuda:0')\n",
      "0.02673071765253553 0.051201551123522225 19.373494863510132\n",
      "0.026730717688915318 0.050872749108821155 19.435955286026\n",
      "0.02673071766650537 0.05067919187014922 19.328578233718872\n",
      "0.026730717644968535 0.05052911888924427 19.367594242095947\n",
      "0.02673071767523652 0.05039683633483946 19.627444744110107\n",
      "Bigstep:  13\n",
      "Avarage Reward:  tensor(1.6639, device='cuda:0')\n",
      "Action Variance:  tensor(0.1099, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7124, device='cuda:0')\n",
      "0.02702113665291108 0.05104165685130283 19.412523984909058\n",
      "0.027021136667754034 0.05073696067556739 19.414510011672974\n",
      "0.027021136649127583 0.050551024255109954 19.312415838241577\n",
      "0.02702113664679928 0.05039638874819502 19.391450881958008\n",
      "0.027021136704424863 0.05027445141109638 19.318986177444458\n",
      "Bigstep:  14\n",
      "Avarage Reward:  tensor(1.6679, device='cuda:0')\n",
      "Action Variance:  tensor(0.1099, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7164, device='cuda:0')\n",
      "0.027070587276830337 0.05079246600857004 19.460020065307617\n",
      "0.027070587290509138 0.05050473117502406 19.492929697036743\n",
      "0.027070587273919956 0.05031509596970864 19.041964292526245\n",
      "0.02707058729341952 0.05016625818912871 18.91630458831787\n",
      "0.0270705873076804 0.05004399008117616 19.078591346740723\n",
      "Bigstep:  15\n",
      "Avarage Reward:  tensor(1.6619, device='cuda:0')\n",
      "Action Variance:  tensor(0.1100, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7124, device='cuda:0')\n",
      "0.027138997759320774 0.05084967996925115 19.091090202331543\n",
      "0.027138997771253344 0.05053260462125763 19.01226282119751\n",
      "0.02713899777387269 0.05035450526047498 19.089294910430908\n",
      "0.027138997763977386 0.050210771677084265 19.187821626663208\n",
      "0.027138997743313666 0.050076865387381986 19.13040328025818\n",
      "Bigstep:  16\n",
      "Avarage Reward:  tensor(1.6698, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7156, device='cuda:0')\n",
      "0.027268249760381877 0.050691264831693846 19.15015697479248\n",
      "0.02726824973215116 0.05039103741175495 18.968785762786865\n",
      "0.027268249802291394 0.050201236216817054 19.12878727912903\n",
      "0.02726824973564362 0.05005229541682638 19.420608520507812\n",
      "0.027268249766202644 0.04993458077311516 19.974366188049316\n",
      "Bigstep:  17\n",
      "Avarage Reward:  tensor(1.6706, device='cuda:0')\n",
      "Action Variance:  tensor(0.1099, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7156, device='cuda:0')\n",
      "0.027149718649161513 0.05057359005906619 19.956932306289673\n",
      "0.027149718621512876 0.05026634680456482 20.010908126831055\n",
      "0.027149718632281293 0.05006468691397458 19.442211151123047\n",
      "0.027149718643631787 0.04992663072538562 19.780681371688843\n",
      "0.02714971861045342 0.049795782298315314 19.78949999809265\n",
      "Bigstep:  18\n",
      "Avarage Reward:  tensor(1.6685, device='cuda:0')\n",
      "Action Variance:  tensor(0.1100, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7181, device='cuda:0')\n",
      "0.027158347593795044 0.05045831125695258 20.17718195915222\n",
      "0.027158347660151776 0.05016839468968101 19.946629285812378\n",
      "0.027158347610675263 0.04995706465560943 19.61974263191223\n",
      "0.02715834763686871 0.04980923446128145 19.89015769958496\n",
      "0.027158347652293743 0.04968485714052804 19.76217484474182\n",
      "Bigstep:  19\n",
      "Avarage Reward:  tensor(1.6656, device='cuda:0')\n",
      "Action Variance:  tensor(0.1099, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7130, device='cuda:0')\n",
      "0.02724635915714316 0.0502912672131788 19.847514867782593\n",
      "0.027246359127457254 0.04998804447357543 19.651692390441895\n",
      "0.027246359142591246 0.04980203114217147 19.152474880218506\n",
      "0.027246359137352558 0.04966047349153087 19.156970024108887\n",
      "0.02724635915597901 0.049549102465389294 19.19666337966919\n",
      "Bigstep:  20\n",
      "Avarage Reward:  tensor(1.6659, device='cuda:0')\n",
      "Action Variance:  tensor(0.1100, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7134, device='cuda:0')\n",
      "0.027263450160098728 0.05010668853647075 19.61368465423584\n",
      "0.0272634501408902 0.049814492769073695 19.812770128250122\n",
      "0.027263450107711834 0.04962202921160497 19.319692611694336\n",
      "0.02726345013885293 0.0494853155000601 19.4047532081604\n",
      "0.027263450137979817 0.04936869485769421 19.424129009246826\n",
      "Bigstep:  21\n",
      "Avarage Reward:  tensor(1.6672, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7172, device='cuda:0')\n",
      "0.027203125705418642 0.050144055146956815 19.680403470993042\n",
      "0.02720312569726957 0.04985018157283776 19.825987100601196\n",
      "0.027203125732485206 0.049657365414313974 19.252731323242188\n",
      "0.027203125689993612 0.04953352912911214 19.246550798416138\n",
      "0.02720312571240356 0.04939020451158285 19.357951402664185\n",
      "Bigstep:  22\n",
      "Avarage Reward:  tensor(1.6695, device='cuda:0')\n",
      "Action Variance:  tensor(0.1100, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7174, device='cuda:0')\n",
      "0.027166218283819033 0.0500738727604039 18.823962688446045\n",
      "0.027166218264319468 0.049765024066437034 18.788138151168823\n",
      "0.0271662182349246 0.049580318339867514 19.11672306060791\n",
      "0.027166218222118912 0.049433285587001594 19.26512098312378\n",
      "0.02716621825558832 0.04931619233335369 19.141339778900146\n",
      "Bigstep:  23\n",
      "Avarage Reward:  tensor(1.6609, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7127, device='cuda:0')\n",
      "0.026884076094138436 0.050053088803542777 19.03129291534424\n",
      "0.026884076049609577 0.049739702286897225 19.11612892150879\n",
      "0.02688407608307898 0.049553905786015096 19.247397422790527\n",
      "0.026884076104615816 0.0494130487809889 19.1426682472229\n",
      "0.026884076068236028 0.049301044563762844 19.138737678527832\n",
      "Bigstep:  24\n",
      "Avarage Reward:  tensor(1.6628, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7142, device='cuda:0')\n",
      "0.02681100850139046 0.05005573768983595 19.188018560409546\n",
      "0.026811008529912216 0.04975023453938775 19.0334632396698\n",
      "0.026811008486838546 0.04956026372150518 19.143713235855103\n",
      "0.026811008535150904 0.0494240925332997 19.044182062149048\n",
      "0.026811008526419756 0.04929690527031198 19.21632671356201\n",
      "Bigstep:  25\n",
      "Avarage Reward:  tensor(1.6675, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7146, device='cuda:0')\n",
      "0.027114163531805388 0.05005327382241376 19.084961891174316\n",
      "0.027114163540245498 0.04975446632364765 19.113091468811035\n",
      "0.027114163588848897 0.04955482740188018 19.23743724822998\n",
      "0.027114163536462 0.04942724838736467 19.22314977645874\n",
      "0.027114163553342223 0.04930278353043832 19.131540060043335\n",
      "Bigstep:  26\n",
      "Avarage Reward:  tensor(1.6611, device='cuda:0')\n",
      "Action Variance:  tensor(0.1102, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7124, device='cuda:0')\n",
      "0.02698084373230813 0.04967733251862228 19.074471950531006\n",
      "0.026980843770434148 0.049382333752000705 19.13288903236389\n",
      "0.026980843804485632 0.049185950080864134 19.156917810440063\n",
      "0.026980843741621355 0.04905570649774745 19.10578441619873\n",
      "0.026980843769561034 0.048935828920220956 19.081446647644043\n",
      "Bigstep:  27\n",
      "Avarage Reward:  tensor(1.6660, device='cuda:0')\n",
      "Action Variance:  tensor(0.1102, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7150, device='cuda:0')\n",
      "0.027257833325711543 0.04978203248349018 19.05970859527588\n",
      "0.027257833352778107 0.049467142405919734 19.049145936965942\n",
      "0.027257833330659197 0.04927883148659021 19.128299236297607\n",
      "0.027257833336771 0.04913579205516726 19.122304439544678\n",
      "0.027257833319017665 0.04902416272554547 19.135974884033203\n",
      "Bigstep:  28\n",
      "Avarage Reward:  tensor(1.6605, device='cuda:0')\n",
      "Action Variance:  tensor(0.1102, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7133, device='cuda:0')\n",
      "0.02711410005314974 0.04974936279468238 19.10260558128357\n",
      "0.027114100057515314 0.04945453600143082 19.095400094985962\n",
      "0.027114100011240226 0.04925350693287328 19.088183403015137\n",
      "0.02711410003103083 0.04911709395004436 19.1597843170166\n",
      "0.027114100031612905 0.048996362027246505 19.32544493675232\n",
      "Bigstep:  29\n",
      "Avarage Reward:  tensor(1.6632, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7131, device='cuda:0')\n",
      "0.027091749812243507 0.04970583460526541 19.298691034317017\n",
      "0.027091749840474223 0.04940843771328218 19.589741230010986\n",
      "0.02709174980846001 0.049218395869247616 19.811782121658325\n",
      "0.02709174982854165 0.0490761481015943 19.9180748462677\n",
      "0.027091749825922308 0.048959930614801125 19.71408724784851\n",
      "Bigstep:  30\n",
      "Avarage Reward:  tensor(1.6654, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7130, device='cuda:0')\n",
      "0.027213453880394808 0.04959284347714856 20.0394287109375\n",
      "0.027213453815784307 0.04928923246799968 19.877811193466187\n",
      "0.0272134538445971 0.04910821776487864 19.991416931152344\n",
      "0.0272134538544924 0.04898212207015604 19.88757061958313\n",
      "0.027213453846925403 0.04884868514258414 19.814316034317017\n",
      "Bigstep:  31\n",
      "Avarage Reward:  tensor(1.6653, device='cuda:0')\n",
      "Action Variance:  tensor(0.1101, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7144, device='cuda:0')\n",
      "0.027174747135140932 0.04971473327255808 18.757717609405518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-39-ab3e40b49a70>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Bigstep: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0;31m#env.render()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-38-4088218dab20>\u001B[0m in \u001B[0;36mtrain_dataset\u001B[0;34m(num_batches, num_epochs, gamma, rand_prob)\u001B[0m\n\u001B[1;32m     21\u001B[0m             \u001B[0msum_v\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mlossv\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m             \u001B[0msum_p\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mlossp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/adam.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m                 \u001B[0;31m# Decay the first and second moment running average coefficient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 95\u001B[0;31m                 \u001B[0mexp_avg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbeta1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mbeta1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     96\u001B[0m                 \u001B[0mexp_avg_sq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbeta2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddcmul_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mbeta2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('Bigstep: ', i)\n",
    "    train_dataset(200, 5, 1, 0.05)\n",
    "    #env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avarage Reward:  tensor(1.6697, device='cuda:0')\n",
      "Action Variance:  tensor(0.0963, device='cuda:0')\n",
      "Avarage Start Value:  tensor(1.7141, device='cuda:0')\n",
      "Ratio of Actions taken from policy:  tensor(0.4216, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "nacs = 8\n",
    "obs, mem, value, actions, targets, avg_reward, p_ratio = make_dataset(100, BATCH_SIZE, MAX_ITER, s_model, q_model, p_model, model_memory, device, env, 1, nacs, DIMS, 0)\n",
    "print('Avarage Reward: ', avg_reward)\n",
    "print('Action Variance: ', actions.var(0).mean())\n",
    "print('Avarage Start Value: ', value[0].mean())\n",
    "print('Ratio of Actions taken from policy: ', p_ratio)\n",
    "obs, mem, value, targets = None, None, None, None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actions.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "actions=None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testenv = MultiEnv(BATCH_SIZE, MAX_ITER, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cobs = testenv.reset()\n",
    "cmem, h = model_memory(cobs, None)\n",
    "v_old = 0\n",
    "r_sum = 0\n",
    "for k in range(MAX_ITER):\n",
    "    action = torch.tensor([k / (MAX_ITER-1)], dtype=torch.float, device=device).expand(BATCH_SIZE, 1)\n",
    "    cobs, reward, _ = testenv.step(action)\n",
    "    r_sum += reward.mean()\n",
    "print(r_sum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testenv.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "max_iter = 16\n",
    "testenv = MultiEnv(BATCH_SIZE, max_iter, device, dims=DIMS)\n",
    "\n",
    "r_sum = 0\n",
    "for i in range(num_batches):\n",
    "    cobs = testenv.reset()\n",
    "    for k in range(max_iter):\n",
    "        action = torch.rand([BATCH_SIZE, 2], dtype=torch.float, device=device)\n",
    "        cobs, reward, _ = testenv.step(action)\n",
    "        r_sum += reward.mean()\n",
    "print(r_sum/num_batches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_batches = 1000\n",
    "max_iter = 16\n",
    "testenv = MultiEnv(BATCH_SIZE, max_iter, device, dims=DIMS)\n",
    "\n",
    "r_sum = 0\n",
    "for i in range(num_batches):\n",
    "    cobs = testenv.reset()\n",
    "    for p in range(4):\n",
    "        for k in range(4):\n",
    "            action = torch.tensor([k/4+1/8, p/4+1/8], dtype=torch.float, device=device).expand(BATCH_SIZE, 2)\n",
    "            cobs, reward, _ = testenv.step(action)\n",
    "            r_sum += reward.mean()\n",
    "print(r_sum/num_batches)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'wip.Code.train_q' from '../../wip/Code/train_q.py'>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wip.Code.train_q as train_q\n",
    "import importlib\n",
    "importlib.reload(train_q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}