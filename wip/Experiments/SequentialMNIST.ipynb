{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from wip.Code.distortion import distortion_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "USE_JIT = False\n",
    "\n",
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mnist = MNIST('../../', transform=transforms.ToTensor()) #distortion_transform([0,15], 3)\n",
    "test = MNIST('../../', transform=transforms.ToTensor(), train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_loader = DataLoader(mnist, batch_size=BATCH_SIZE, drop_last=True, num_workers=0, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=1024, drop_last=False, num_workers=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from Code.Networks import Selector, DynNetwork, OuterWrapper, LSTMWrapper, ReLuWrapper, DummyNeuron, make_SequenceWrapper, ParallelNetwork\n",
    "from Code.NewNeurons import SeqOnlySpike, CooldownNeuron, OutputNeuron, LIFNeuron, NoResetNeuron, AdaptiveNeuron\n",
    "\n",
    "base_config = {\n",
    "    'ALPHA': 0,\n",
    "    'BETA': 0.9,\n",
    "    'OFFSET': 2,\n",
    "    'RESET_ZERO': False,\n",
    "    'DECODING': 'potential',\n",
    "    'SPIKE_FN': 'ss'\n",
    "}\n",
    "\n",
    "heavyside = {\n",
    "    **base_config,\n",
    "    'BETA': 1,\n",
    "    'OFFSET': 7\n",
    "}\n",
    "\n",
    "mem_lif = {\n",
    "    **base_config,\n",
    "    'BETA': 0.5\n",
    "}\n",
    "\n",
    "'''mem_loop = OrderedDict([\n",
    "    ('input', 1),\n",
    "    ('pre_mem', [['input', 'output'], LIFNeuron(128, mem_lif), nn.Linear]),\n",
    "    ('output', [['pre_mem'], CooldownNeuron(128, heavyside), nn.Linear]),\n",
    "])'''\n",
    "\n",
    "mem_loop = OrderedDict([\n",
    "    ('input', 1),\n",
    "    ('pre_mem', [['input', 'output', 'shortterm'], NoResetNeuron(128, base_config), nn.Linear]),\n",
    "    ('shortterm', [['pre_mem'], CooldownNeuron(64, base_config), nn.Linear]),\n",
    "    ('output', [['pre_mem'], CooldownNeuron(128, heavyside), nn.Linear]),\n",
    "])\n",
    "\n",
    "post_mem = OrderedDict([\n",
    "    ('input', 128),\n",
    "    ('pre_mem', [['input'], make_SequenceWrapper(LIFNeuron(128, base_config), USE_JIT), nn.Linear]),\n",
    "    ('output', [['pre_mem'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "architecture = OrderedDict([\n",
    "    ('input', 1),\n",
    "    ('mem_loop', [['input'], make_SequenceWrapper(DynNetwork(mem_loop), USE_JIT), None]),\n",
    "    ('post_mem', [['mem_loop'], SeqOnlySpike(128, base_config), nn.Linear]),\n",
    "    ('output', [['post_mem'], OutputNeuron(10, heavyside), nn.Linear]),\n",
    "])\n",
    "\n",
    "architecturelstm = OrderedDict([\n",
    "    ('input', 1),\n",
    "    ('lstm', [['input'], LSTMWrapper(1, 128), None]),\n",
    "    ('post_mem', [['lstm'], ReLuWrapper(128), nn.Linear]),\n",
    "    ('output', [['post_mem'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "\n",
    "adap_config = {\n",
    "    'ALPHA': 0,\n",
    "    'BETA': 0.95,\n",
    "    'OFFSET': 2,\n",
    "    'ADAPDECAY': 0.998,\n",
    "    'RESET_ZERO': False,\n",
    "    'DECODING': 'potential',\n",
    "    'SPIKE_FN': 'bellec'\n",
    "}\n",
    "\n",
    "adap_arch = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('bundled', [['input', 'adaptive', 'regular'], DummyNeuron(81+100+120), None]),\n",
    "    ('adaptive', [['bundled'], AdaptiveNeuron(100, adap_config), nn.Linear]),\n",
    "    ('regular', [['bundled'], LIFNeuron(120, adap_config), nn.Linear]),\n",
    "    ('output', [['bundled'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "adap_arch2 = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('adaptive', [['input', 'adaptive', 'regular'], AdaptiveNeuron(100, adap_config), nn.Linear]),\n",
    "    ('regular', [['input', 'adaptive', 'regular'], LIFNeuron(120, adap_config), nn.Linear]),\n",
    "    ('output', [['adaptive', 'regular'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "adap_arch3 = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('regular', [['input', 'regular'], LIFNeuron(120, adap_config), nn.Linear]),\n",
    "    ('output', [['regular'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "adap_arch4 = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('adap', [['input', 'adap'], NoResetNeuron(120, base_config), nn.Linear]),\n",
    "    ('output', [['adap'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "mem_loop2 = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('pre_mem', [['input', 'output'], NoResetNeuron(128, base_config), nn.Linear]),\n",
    "    ('output', [['pre_mem'], CooldownNeuron(128, heavyside), nn.Linear]),\n",
    "])\n",
    "\n",
    "mem_loop3 = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('pre_mem', [['input', 'output'], LIFNeuron(128, adap_config), nn.Linear]),\n",
    "    ('output', [['pre_mem'], AdaptiveNeuron(128, adap_config), nn.Linear]),\n",
    "])\n",
    "\n",
    "cd_full = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('mem_loop', [['input'], make_SequenceWrapper(DynNetwork(mem_loop2), USE_JIT), None]),\n",
    "    ('post_mem', [['mem_loop'], make_SequenceWrapper(LIFNeuron(128, base_config), USE_JIT), nn.Linear]),\n",
    "    ('output', [['post_mem'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "ada_full = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('mem_loop', [['input'], make_SequenceWrapper(DynNetwork(mem_loop3), USE_JIT), None]),\n",
    "    ('post_mem', [['mem_loop'], make_SequenceWrapper(LIFNeuron(128, adap_config), USE_JIT), nn.Linear]),\n",
    "    ('output', [['post_mem'], DummyNeuron(10), nn.Linear]),\n",
    "])\n",
    "\n",
    "#TODO: fix output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#mem_model = OuterWrapper(torch.load('../../models/mem_big10_5'), device, USE_JIT)\n",
    "#post_model = OuterWrapper(torch.load('../../models/post_big10_5'), device, USE_JIT)\n",
    "#mem_model = OuterWrapper(n_mem, device, USE_JIT)\n",
    "#mem_model = OuterWrapper(make_SequenceWrapper(DynNetwork(mem_loop), USE_JIT), device, USE_JIT)\n",
    "#post_model = OuterWrapper(DynNetwork(post_mem), device, USE_JIT)\n",
    "\n",
    "#144, 150, 137, 150\n",
    "#model = OuterWrapper(torch.load('../../models/snn4_3'), device, USE_JIT)\n",
    "\n",
    "#model = OuterWrapper(DynNetwork(architecture), device, USE_JIT)\n",
    "\n",
    "#model = OuterWrapper(DynNetwork(architecturelstm), device, USE_JIT)\n",
    "\n",
    "#model = OuterWrapper(DynNetwork(cd_full), device, USE_JIT)\n",
    "#model = OuterWrapper(make_SequenceWrapper(ParallelNetwork(adap_arch2), USE_JIT), device, USE_JIT)\n",
    "#model = OuterWrapper(make_SequenceWrapper(DynNetwork(adap_arch), USE_JIT), device, USE_JIT)\n",
    "#model = OuterWrapper(DynNetwork(ada_full), device, USE_JIT)\n",
    "model = OuterWrapper(make_SequenceWrapper(DynNetwork(adap_arch4), USE_JIT), device, USE_JIT)\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    model.model.layers.lstm.lstm.bias_hh_l0[:256] += 3\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nwith torch.no_grad():\\n    for i in range(100):\\n        model.pretrace.model.layers.adaptive_synapse.weight[i, i+81] = 0\\n\\n    for i in range(120):\\n        model.pretrace.model.layers.regular_synapse.weight[i, i+181] = 0\\n\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        model.pretrace.model.layers.adaptive_synapse.weight[i, i+81] = 0\n",
    "\n",
    "    for i in range(120):\n",
    "        model.pretrace.model.layers.regular_synapse.weight[i, i+181] = 0\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "trigger_signal = torch.ones([783+56, 1, 1], device=device)\n",
    "trigger_signal[:783] = 0\n",
    "def encode_input(curr, last):\n",
    "    out = torch.zeros([783+56, curr.shape[1], 2,40], device=curr.device)\n",
    "    out[:783, :, 0, :] = ((torch.arange(40, device=curr.device) < 40 * last) & (torch.arange(40, device=curr.device) > 40 * curr)).float()\n",
    "    out[:783, :, 1, :] = ((torch.arange(40, device=curr.device) > 40 * last) & (torch.arange(40, device=curr.device) < 40 * curr)).float()\n",
    "    out = torch.cat((out.view([783+56, curr.shape[1], 80]), trigger_signal.expand([783+56, curr.shape[1], 1])), dim=-1)\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#params = list(mem_model.parameters())+list(post_model.parameters())\n",
    "params = model.parameters()\n",
    "ce = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params, lr=1e-3)#0.000011e-6\n",
    "#optimizer = optim.SGD(params, lr=1e-5)\n",
    "#optimizer = optim.Adam(mem_model.parameters(), lr=1e-3)#0.000011e-6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "gradient_history = {}\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    gradient_history[name] = {'iter': [], 'value':[], 'avg':1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def manage_gradients(iter):\n",
    "    do_gradient = True\n",
    "    for name, p in model.named_parameters():\n",
    "        v = p.grad.norm().item()\n",
    "        if v > 100 * gradient_history[name]['avg']:\n",
    "            #print(name, v)\n",
    "            do_gradient = False\n",
    "        else:\n",
    "            gradient_history[name]['avg'] = 0.9 * gradient_history[name]['avg'] + 0.1 * v\n",
    "    return do_gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def monitor_gradients(iter):\n",
    "    for name, p in model.named_parameters():\n",
    "        v = p.grad.norm().item()\n",
    "        if iter == 0 or v > 2* gradient_history[name]['value'][-1] or v < 0.5 * gradient_history[name]['value'][-1]:\n",
    "            print(name, v)\n",
    "            gradient_history[name]['value'].append(v)\n",
    "            gradient_history[name]['iter'].append(iter)\n",
    "        if iter - gradient_history[name]['iter'][-1] >= 20:\n",
    "            gradient_history[name]['value'].append(v)\n",
    "            gradient_history[name]['iter'].append(iter)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      ".2.3569815158843994 0.11784907579421997 0.004296875 0.8987224102020264 0.02374180033802986\n",
      "....................2.2997829914093018 2.311808669567108 0.095703125 1.0932927131652832 0.005496765952557325\n",
      "....................2.2981131076812744 2.303507947921753 0.119921875 0.9066684246063232 0.0015271632000803947\n",
      "....................2.2867183685302734 2.2997230529785155 0.123828125 0.9010732173919678 0.0006056968704797328\n",
      "....................2.303955554962158 2.3040708661079408 0.104296875 0.904564380645752 0.0038315022829920053\n",
      "....................2.283956289291382 2.282125735282898 0.12890625 0.8752143383026123 0.0007966957637108862\n",
      "....................2.2900655269622803 2.2937514901161196 0.13046875 0.937474250793457 0.0008277239976450801\n",
      "....................2.297236204147339 2.3033241271972655 0.116015625 0.861459493637085 0.002906516194343567\n",
      "....................2.2945237159729004 2.3237852454185486 0.113671875 0.9113798141479492 0.0005186368362046778\n",
      "....................2.2821826934814453 2.3002930283546448 0.11328125 0.8967797756195068 0.0012328560696914792\n",
      "....................2.2248001098632812 2.240154194831848 0.16796875 0.899167537689209 0.02457592822611332\n",
      "....................2.3096396923065186 2.2741521120071413 0.140625 0.8702206611633301 0.0014058395754545927\n",
      "....................2.3689723014831543 2.274305355548859 0.120703125 0.8793191909790039 0.004044354893267155\n",
      ".....||.............2.265277624130249 2.299202394485474 0.11953125 0.8773751258850098 0.007887140847742558\n",
      "....................2.4816715717315674 2.2791961312294005 0.15390625 0.8684232234954834 0.00012707356654573232\n",
      "....................2.3061606884002686 2.3310003876686096 0.1125 0.8927807807922363 9.235285688191652e-06\n",
      "....................2.3084497451782227 2.312217092514038 0.094140625 0.8717143535614014 0.00020892191969323903\n",
      "....................2.2924020290374756 2.306549882888794 0.091796875 0.882347583770752 1.7987805449592997e-06\n",
      "....................2.3014919757843018 2.301842141151428 0.117578125 0.85874342918396 5.284652161208214e-06\n",
      "....................2.289367437362671 2.3055254578590394 0.101953125 0.8877964019775391 6.951604973437497e-06\n",
      "....................2.3078536987304688 2.3042084693908693 0.096484375 0.8705019950866699 7.155415460147196e-07\n",
      "....................2.2975237369537354 2.304271936416626 0.098046875 0.864234447479248 1.0938041441477253e-06\n",
      "....................2.308119058609009 2.30765780210495 0.09765625 0.8771758079528809 8.637988685222808e-06\n",
      "....................2.3030002117156982 2.3017675042152406 0.11875 1.0432748794555664 9.448229320696555e-06\n",
      ".......Epoch:  1\n",
      ".............2.302208662033081 2.3047686576843263 0.109765625 0.8617799282073975 3.6259396438254043e-05\n",
      "........||....||...|2.3065028190612793 2.3014593720436096 0.101171875 0.9413313865661621 1.9813534891000018e-05\n",
      "|||.................2.3005611896514893 2.299193334579468 0.112890625 0.8775253295898438 1.015425254990987e-06\n",
      "....................2.304893970489502 2.303615593910217 0.102734375 0.8897495269775391 1.4344583178171888e-05\n",
      "....................2.301189422607422 2.303614354133606 0.105859375 0.8745310306549072 7.674012886127457e-05\n",
      ".........|.|.......|2.2874510288238525 2.299571132659912 0.12734375 0.8804929256439209 0.0004965331172570586\n",
      "....|........|.|....2.296518564224243 2.30185604095459 0.10546875 0.8644669055938721 0.000565304362680763\n",
      "..|||....|.....|.|.|2.290236234664917 2.2876705646514894 0.14765625 0.881016731262207 0.0014220023294910789\n",
      ".||.|||......||..|..2.2509140968322754 2.288504457473755 0.1328125 0.874739408493042 0.007327703293412924\n",
      "|||...|........|||||2.330376625061035 2.2783191561698914 0.141015625 0.8698503971099854 0.00016165712440852076\n",
      "||||................2.308847427368164 2.307708942890167 0.111328125 0.8821132183074951 0.002155858790501952\n",
      "....................2.2916419506073 2.2833141326904296 0.146484375 0.9005670547485352 0.005163662601262331\n",
      "............|.......2.20177960395813 2.1687958002090455 0.183203125 0.8920760154724121 0.0999089851975441\n",
      "....|......|........1.8795160055160522 2.1068074643611907 0.245703125 0.8541102409362793 0.41250643134117126\n",
      "..........|.........2.22725772857666 1.9058685839176177 0.287890625 0.9182171821594238 0.7100248336791992\n",
      "....||....|..||.....1.7591743469238281 1.867826271057129 0.280078125 0.9204928874969482 0.8937922716140747\n",
      "...||...|....|.....|1.8752361536026 1.779137170314789 0.317578125 0.952333927154541 0.9561329483985901\n",
      "........|....|..||.|1.8096201419830322 1.782604604959488 0.3125 0.8573448657989502 1.1405190229415894\n",
      ".....|.....|.|...||.1.6649996042251587 1.7899656414985656 0.30703125 0.9410550594329834 1.533011555671692\n",
      "||.......|..........1.7797563076019287 1.783578908443451 0.32265625 0.872093677520752 1.4906795024871826\n",
      "....................1.7039512395858765 1.7368013679981231 0.35390625 0.9013073444366455 1.713862419128418\n",
      "....................1.6972399950027466 1.731689703464508 0.36015625 0.8577039241790771 1.8402904272079468\n",
      "....................1.581074833869934 1.7171524941921235 0.345703125 0.8671448230743408 2.1026318073272705\n",
      "...............Epoch:  2\n",
      ".....1.7452795505523682 1.7310356557369233 0.335546875 0.8702421188354492 1.8792400360107422\n",
      "....................1.6724165678024292 1.6850274920463562 0.365234375 0.8699638843536377 1.9484246969223022\n",
      "....................1.7809432744979858 1.7203625380992889 0.33828125 0.8630666732788086 2.0567820072174072\n",
      "....................1.7545830011367798 1.751738053560257 0.329296875 0.8699839115142822 2.0474395751953125\n",
      "....................1.8767876625061035 1.7308065950870515 0.325390625 0.879910945892334 1.735565185546875\n",
      "....................1.8038675785064697 1.7287262558937073 0.3453125 0.9027295112609863 2.215104103088379\n",
      "...................|1.7574368715286255 1.7033174872398376 0.3578125 0.8536534309387207 2.4568896293640137\n",
      "....................1.786879062652588 1.7078523635864258 0.342578125 0.8818166255950928 1.9499825239181519\n",
      "....................1.6547704935073853 1.7140152156352997 0.351953125 0.8655614852905273 2.3702824115753174\n",
      "....................1.7537459135055542 1.6848197162151337 0.355859375 0.8727207183837891 2.1597273349761963\n",
      "....................1.6629729270935059 1.698553168773651 0.35 0.8720836639404297 2.422577381134033\n",
      "....................1.6143149137496948 1.7044069647789002 0.353125 0.8546867370605469 2.2646541595458984\n",
      "....................1.7045443058013916 1.7021243035793305 0.36484375 0.8959650993347168 2.46574068069458\n",
      "....................1.7077701091766357 1.688254815340042 0.35390625 0.8658709526062012 2.5875844955444336\n",
      "....................1.6762139797210693 1.6964419960975647 0.34609375 0.8761706352233887 2.55767560005188\n",
      "....................1.7812106609344482 1.6848976790905 0.359765625 0.8638498783111572 2.3313241004943848\n",
      "....................1.628377914428711 1.7071126997470856 0.35078125 0.8731353282928467 2.8495824337005615\n",
      "....................1.6113325357437134 1.6961462795734406 0.366015625 0.8926708698272705 2.545233964920044\n",
      "....................1.6838304996490479 1.7154450297355652 0.35703125 0.8683173656463623 2.7852208614349365\n",
      "....................1.794437050819397 1.6999282896518708 0.33515625 0.848597526550293 2.1068029403686523\n",
      ".......|.....|......1.7597237825393677 1.6858146965503693 0.3578125 0.8598623275756836 2.6491856575012207\n",
      ".................|..1.712182879447937 1.6953259825706481 0.366015625 0.8757307529449463 2.2386152744293213\n",
      "....................1.5845692157745361 1.681930023431778 0.370703125 0.8499431610107422 2.403434991836548\n",
      ".............|......1.593667984008789 1.694849967956543 0.350390625 0.86556077003479 2.4718704223632812\n",
      "...Epoch:  3\n",
      ".................1.5750250816345215 1.6812642216682434 0.365234375 0.8635365962982178 2.7365474700927734\n",
      "....................1.5456345081329346 1.7117423951625823 0.35546875 0.8547489643096924 2.622724771499634\n",
      "...........|........1.6412752866744995 1.6957408249378205 0.362109375 0.8560807704925537 2.3477537631988525\n",
      "..|.................1.7820414304733276 1.7142607867717743 0.328515625 0.8987970352172852 2.608647108078003\n",
      "|..|............|...1.6910291910171509 1.7283255279064178 0.3484375 0.908001184463501 2.497478485107422\n",
      "....................1.6698403358459473 1.67782581448555 0.351953125 0.8592245578765869 2.679959535598755\n",
      "....................1.5180509090423584 1.6799921631813048 0.360546875 0.8619921207427979 2.735933542251587\n",
      "....................1.6887233257293701 1.6833015978336334 0.358203125 0.8667712211608887 2.6288654804229736\n",
      "....................1.7521816492080688 1.6970054745674132 0.354296875 0.8683254718780518 2.6257336139678955\n",
      "....................1.7666959762573242 1.690349394083023 0.35625 0.8553082942962646 2.5562379360198975\n",
      "....................1.6240205764770508 1.6852698802947998 0.364453125 0.8624699115753174 2.7477591037750244\n",
      "....................1.6844667196273804 1.6774725914001465 0.345703125 0.8669934272766113 2.6879498958587646\n",
      "....................1.8439600467681885 1.7227622509002685 0.335546875 0.8497424125671387 2.538914680480957\n",
      "...........|........1.8038572072982788 1.6951586067676545 0.352734375 0.8700146675109863 2.7226345539093018\n",
      "....................1.5993585586547852 1.6943215250968933 0.3546875 0.8825750350952148 2.5941781997680664\n",
      "................|...1.6043422222137451 1.6906264424324036 0.35234375 0.8782968521118164 2.8832597732543945\n",
      "....................1.7093256711959839 1.667968463897705 0.38125 0.8725664615631104 2.9774160385131836\n",
      "....................1.7250473499298096 1.720850372314453 0.33671875 0.8543968200683594 2.660280227661133\n",
      "..................|.1.6512702703475952 1.6946110427379608 0.34296875 0.8464696407318115 2.8249621391296387\n",
      ".......|............1.628831386566162 1.6868437826633453 0.35 0.8655083179473877 3.074514865875244\n",
      "....................1.7734278440475464 1.7248491108417512 0.334765625 0.8888821601867676 2.8930845260620117\n",
      "....................1.603679895401001 1.6856556594371797 0.365234375 0.8686974048614502 2.9026472568511963\n",
      "....................1.8025856018066406 1.7030303955078125 0.363671875 0.8656907081604004 2.8354177474975586\n",
      "....|......Epoch:  4\n",
      ".........1.7479307651519775 1.678502094745636 0.358984375 0.8857696056365967 2.682229518890381\n",
      "......|.............1.6853610277175903 1.7148864269256592 0.34296875 0.8466882705688477 2.930607557296753\n",
      "....................1.5764610767364502 1.6814860939979552 0.36640625 0.8621482849121094 2.8489763736724854\n",
      "|...................1.6598459482192993 1.690512728691101 0.361328125 0.8554468154907227 3.086075782775879\n",
      "...........|........1.7238404750823975 1.708879578113556 0.35703125 0.8734738826751709 2.8771157264709473\n",
      "....................1.5904998779296875 1.675043100118637 0.36484375 0.8641691207885742 3.0914900302886963\n",
      "..............|.....1.8132010698318481 1.7075691044330596 0.3453125 0.8595314025878906 2.9050216674804688\n",
      "....................1.620794415473938 1.7194474935531616 0.343359375 0.8515043258666992 2.9831905364990234\n",
      "....................1.7103993892669678 1.7149314165115357 0.35703125 0.8556575775146484 3.0087778568267822\n",
      "...................|1.6591864824295044 1.6811596512794496 0.34453125 0.8910346031188965 2.8163795471191406\n",
      "....................1.7730388641357422 1.6667444229125976 0.3671875 0.8854045867919922 2.361910581588745\n",
      "...............|....1.4524692296981812 1.6577775537967683 0.373046875 0.8598029613494873 3.316782236099243\n",
      "...........|........1.667870044708252 1.696310156583786 0.342578125 0.869356632232666 3.119720458984375\n",
      "....................1.7130038738250732 1.6623144567012786 0.369921875 0.8450419902801514 3.2144598960876465\n",
      "...................|1.7030408382415771 1.6919006049633025 0.35859375 0.8482561111450195 2.7163796424865723\n",
      "....................1.8026751279830933 1.7078479290008546 0.361328125 0.8614873886108398 2.6752665042877197\n",
      "....................1.6139811277389526 1.6691095113754273 0.365625 0.8764443397521973 2.858865737915039\n",
      "...........|........1.6492252349853516 1.6919049620628357 0.367578125 0.8515419960021973 2.7553892135620117\n",
      ".....|..............1.5739970207214355 1.6664386868476868 0.368359375 0.8549041748046875 3.2477869987487793\n",
      "....................1.8155404329299927 1.6875750064849853 0.350390625 0.866870641708374 2.889373779296875\n",
      "....................1.6601488590240479 1.6901430368423462 0.354296875 0.8612456321716309 2.9065961837768555\n",
      "....................1.7282536029815674 1.6915583074092866 0.35078125 0.8529098033905029 3.0375964641571045\n",
      "....................1.6467901468276978 1.7146093010902406 0.330859375 0.85536789894104 2.589611053466797\n",
      "...................Epoch:  5\n",
      ".1.5914738178253174 1.7007987380027771 0.34453125 0.8731522560119629 2.5267958641052246\n",
      "....................1.7322200536727905 1.7088800251483918 0.34609375 0.8575079441070557 2.9417965412139893\n",
      "....................1.7216699123382568 1.6851953148841858 0.356640625 0.8542959690093994 2.812826633453369\n",
      "................|...1.7456693649291992 1.7185617446899415 0.335546875 0.8636164665222168 2.975637197494507\n",
      "....................1.7344802618026733 1.688955819606781 0.347265625 0.8525269031524658 2.871079683303833\n",
      "....................1.6134686470031738 1.6952864289283753 0.354296875 0.8601744174957275 3.1496291160583496\n",
      "....................1.748451590538025 1.6939927458763122 0.359375 0.8608887195587158 2.7816553115844727\n",
      "....................1.701013207435608 1.7027405679225922 0.356640625 0.8597724437713623 2.9462733268737793\n",
      "....................1.6931524276733398 1.68626925945282 0.35859375 0.8572537899017334 2.7409067153930664\n",
      "....................1.7014994621276855 1.672998046875 0.367578125 0.8530411720275879 2.8909316062927246\n",
      "....|...............1.7607836723327637 1.7060961365699767 0.3578125 0.8429281711578369 2.9180569648742676\n",
      "....................1.6494420766830444 1.6992946028709413 0.346875 0.8439762592315674 2.776665449142456\n",
      "....................1.7077661752700806 1.6944951295852662 0.365234375 0.8709940910339355 3.100658416748047\n",
      "..................|.1.7141937017440796 1.6792007088661194 0.3640625 0.8634929656982422 2.966752052307129\n",
      "...................|1.6967110633850098 1.6804379642009735 0.36796875 0.8479185104370117 2.9511044025421143\n",
      ".||.................1.8102809190750122 1.7015702247619628 0.353515625 0.8639111518859863 2.8766841888427734\n",
      "....|...............1.6872044801712036 1.6491731882095337 0.372265625 0.8472385406494141 3.019880771636963\n",
      "....|...............1.7385433912277222 1.6695390045642853 0.366796875 0.8787822723388672 2.8404109477996826\n",
      "....................1.613086223602295 1.6961877048015594 0.36484375 0.8407435417175293 3.567479133605957\n",
      "....................1.738525629043579 1.6841291427612304 0.35703125 0.8365817070007324 2.4831390380859375\n",
      "....................1.839367389678955 1.7081078112125396 0.346875 0.8580396175384521 2.910156726837158\n",
      "....................1.6615331172943115 1.6693204283714294 0.35859375 0.8540935516357422 2.8478147983551025\n",
      ".............|......1.5696276426315308 1.6536635518074037 0.366796875 0.8699400424957275 3.3225865364074707\n",
      "....................1.641147494316101 1.6625471591949463 0.3828125 0.8754334449768066 2.8935835361480713\n",
      ".......Epoch:  6\n",
      ".............1.7409358024597168 1.7100029230117797 0.349609375 0.8877372741699219 3.1506433486938477\n",
      "...............|....1.6154148578643799 1.6497532367706298 0.390234375 0.8598537445068359 3.1850228309631348\n",
      ".|.|............|...1.6151478290557861 1.6753753006458283 0.37109375 0.8353440761566162 3.1525728702545166\n",
      "........|...........1.791908621788025 1.6737221598625183 0.38125 0.8452842235565186 3.0339412689208984\n",
      "..................|.1.7114512920379639 1.6597054183483124 0.384765625 0.8337006568908691 3.2408571243286133\n",
      "....................1.7079932689666748 1.6638393640518188 0.364453125 0.867074728012085 3.101888656616211\n",
      "....................1.6847472190856934 1.6943391859531403 0.35859375 0.8582608699798584 3.232120990753174\n",
      "...........|||......1.857728362083435 1.6818427383899688 0.35234375 0.8779778480529785 3.1626949310302734\n",
      "....................1.723732829093933 1.6979582846164702 0.336328125 0.8646137714385986 3.1511127948760986\n",
      ".|.||...|||.........1.687191367149353 1.6575215876102447 0.355078125 0.8545246124267578 3.041998863220215\n",
      "....................1.7348123788833618 1.6663197517395019 0.35703125 0.899153470993042 3.324632406234741\n",
      "||..................1.6818221807479858 1.6864205658435822 0.36640625 0.8825633525848389 3.078152656555176\n",
      "....................1.6463595628738403 1.7176107823848725 0.340234375 0.8677000999450684 3.4096474647521973\n",
      ".......|............1.7673500776290894 1.67148819565773 0.3546875 0.8455672264099121 3.155432939529419\n",
      "...|................1.587801218032837 1.6573813438415528 0.36484375 0.8595566749572754 3.540416717529297\n",
      "....................1.6583631038665771 1.6555547773838044 0.35859375 0.8819894790649414 3.157989978790283\n",
      "............|.|...|.1.6766703128814697 1.6496640622615815 0.362109375 0.87351393699646 3.8962464332580566\n",
      "....................1.6287530660629272 1.6223562240600586 0.38984375 0.8908052444458008 3.656498908996582\n",
      "....................1.6144993305206299 1.6067361772060393 0.38515625 0.8447685241699219 3.3462817668914795\n",
      "....................1.5897036790847778 1.593060666322708 0.3953125 0.8706212043762207 4.0193352699279785\n",
      "|.|||......|........1.5891940593719482 1.684296178817749 0.3921875 0.8659968376159668 4.3267717361450195\n",
      "|.................||1.8943984508514404 1.6483193993568421 0.398828125 0.8487391471862793 3.153362512588501\n",
      "||||||||||||||||||||1.7207006216049194 1.688178414106369 0.381640625 0.8844835758209229 3.357325315475464\n",
      "|||||||||||||||Epoch:  7\n",
      "|||||1.641752004623413 1.684642457962036 0.383984375 0.8615109920501709 3.604370594024658\n",
      "||||||||||||||||||||1.6007500886917114 1.7439930856227874 0.3859375 0.8651390075683594 3.082185983657837\n",
      "||||||||||||||||||||1.7619986534118652 1.7165198504924775 0.376171875 0.8864865303039551 3.613783597946167\n",
      "||||||||||||||||||||1.699756383895874 1.7129178583621978 0.39765625 0.8867614269256592 3.504067897796631\n",
      "||||||||||||||||||||1.7416223287582397 1.693429958820343 0.375 0.858914852142334 3.3915631771087646\n",
      "||||||||||||||||||||1.7799166440963745 1.71280118227005 0.384765625 0.863703727722168 3.6476423740386963\n",
      "||||||||||||||||||||1.7411421537399292 1.7410321056842804 0.3796875 0.8468389511108398 3.4007248878479004\n",
      "||||||||||||||||||||1.6105623245239258 1.708321315050125 0.394921875 0.8681328296661377 3.240737199783325\n",
      "||||||||||||||||||||1.720797061920166 1.7158799052238465 0.38125 0.8792674541473389 3.4376041889190674\n",
      "||||||||||||||||||||1.73654043674469 1.6838849127292632 0.4078125 0.886284589767456 3.563786029815674\n",
      "||||||||||||||||||||1.6959445476531982 1.72576105594635 0.375390625 0.860694408416748 3.4090113639831543\n",
      "||||||||||||||||||||1.6985344886779785 1.7123874247074127 0.383203125 0.8519420623779297 3.486905813217163\n",
      "||||||||||||||||||||1.6438993215560913 1.7194979965686799 0.383984375 0.8568837642669678 3.565518379211426\n",
      "||||||||||||||||||||1.6506472826004028 1.7129101157188416 0.382421875 0.8733441829681396 3.438676118850708\n",
      "||||||||||||||||||||1.5486507415771484 1.7367458283901214 0.37890625 0.8453719615936279 3.999077081680298\n",
      "||||||||||||||||||||1.7472410202026367 1.7294049918651582 0.375390625 0.8727779388427734 3.224421977996826\n",
      "||||||||||||||||||||1.5995280742645264 1.716739684343338 0.3765625 0.8721134662628174 3.7028286457061768\n",
      "||||||||||||||||||||1.5434788465499878 1.6703797638416291 0.393359375 0.846015214920044 3.297036647796631\n",
      "||||||||||||||||||||1.8640691041946411 1.71129070520401 0.38671875 0.8568172454833984 3.5289602279663086\n",
      "||||||||||||||||||||1.6906508207321167 1.702374565601349 0.37578125 0.8556480407714844 3.355592727661133\n",
      "||||||||||||||||||||1.6055891513824463 1.6825956106185913 0.385546875 0.8743157386779785 3.66552734375\n",
      "||||||||||||||||||||1.6220141649246216 1.6940368294715882 0.390234375 0.8678555488586426 3.742483615875244\n",
      "||||||||||||||||||||1.7693756818771362 1.7350189924240111 0.363671875 0.8500094413757324 3.3598952293395996\n",
      "||||||||||||||||||||1.6230026483535767 1.7029716551303864 0.386328125 0.84885573387146 3.2034378051757812\n",
      "|||Epoch:  8\n",
      "|||||||||||||||||1.8715579509735107 1.7080161333084107 0.385546875 0.8574049472808838 3.0895140171051025\n",
      "||||||||||||||||||||1.792914867401123 1.6921823978424073 0.399609375 0.845414400100708 3.0390260219573975\n",
      "||||||||||||||||||||1.6220703125 1.720138281583786 0.3765625 0.8515453338623047 3.2837233543395996\n",
      "||||||||||||||||||||1.721717119216919 1.719878751039505 0.384375 0.8883616924285889 3.3516488075256348\n",
      "||||||||||||||||||||1.6913371086120605 1.719021725654602 0.39140625 0.8647365570068359 3.2796030044555664\n",
      "||||||||||||||||||||1.6257154941558838 1.7295672595500946 0.37578125 0.8559305667877197 3.1632704734802246\n",
      "||||||||||||||||||||1.7961705923080444 1.702408504486084 0.380859375 0.8622641563415527 3.3963160514831543\n",
      "||||||||||||||||||||1.8182379007339478 1.7202147245407104 0.38671875 0.8438882827758789 3.2359557151794434\n",
      "||||||||||||||||||||1.924974799156189 1.738166970014572 0.373046875 0.8572518825531006 2.9983279705047607\n",
      "||||||||||||||||||||1.7259162664413452 1.6947866320610045 0.376953125 0.8603842258453369 3.4157207012176514\n",
      "||||||||||||||||||||1.8225362300872803 1.6971660733222962 0.38515625 0.8723442554473877 3.4480459690093994\n",
      "||||||||||||||||||||1.6885290145874023 1.7455542445182801 0.3765625 0.8480894565582275 4.015302658081055\n",
      "||||||||||||||||||||1.6785999536514282 1.726314014196396 0.37109375 0.8544366359710693 3.4136741161346436\n",
      "||||||||||||||||||||1.6729238033294678 1.716894257068634 0.390234375 0.849006175994873 3.6686463356018066\n",
      "||||||||||||||||||||1.7590125799179077 1.7218467891216278 0.383984375 0.8482885360717773 3.4689278602600098\n",
      "||||||||||||||||||||1.7208728790283203 1.725645327568054 0.382421875 0.884284257888794 3.399364948272705\n",
      "||||||||||||||||||||1.7979618310928345 1.7011837720870973 0.388671875 0.8471689224243164 3.729858875274658\n",
      "||||||||||||||||||||1.7695642709732056 1.6954390406608582 0.3875 0.85750412940979 3.146475315093994\n",
      "||||||||||||||||||||1.6557270288467407 1.6962082743644715 0.389453125 0.8627090454101562 3.7620632648468018\n",
      "||||||||||||||||||||1.7783387899398804 1.704569548368454 0.394921875 0.8583230972290039 3.622237205505371\n",
      "||||||||||||||||||||1.6571437120437622 1.7039824664592742 0.376171875 0.8528354167938232 3.5582358837127686\n",
      "||||||||||||||||||||1.7017346620559692 1.7163266003131867 0.38203125 0.8669943809509277 3.4206511974334717\n",
      "||||||||||||||||||||1.7291313409805298 1.7014247179031372 0.386328125 0.862598180770874 3.2534964084625244\n",
      "|||||||||||Epoch:  9\n",
      "|||||||||1.6674368381500244 1.702817451953888 0.384765625 0.8666806221008301 3.4618475437164307\n",
      "||||||||||||||||||||1.7013301849365234 1.7157898366451263 0.3828125 0.8483145236968994 3.6756129264831543\n",
      "||||||||||||||||||||1.6983146667480469 1.7134669601917267 0.390234375 0.8677806854248047 3.0201454162597656\n",
      "||||||||||||||||||||1.7217074632644653 1.720085471868515 0.38515625 0.8600754737854004 3.4768588542938232\n",
      "||||||||||||||||||||1.7474110126495361 1.7089178919792176 0.384765625 0.872281551361084 3.445504665374756\n",
      "||||||||||||||||||||1.6024785041809082 1.7041673600673675 0.383984375 0.8610520362854004 3.6358578205108643\n",
      "||||||||||||||||||||1.7415027618408203 1.7062726080417634 0.390625 0.8566203117370605 3.656641960144043\n",
      "||||||||||||||||||||1.6604942083358765 1.7142483353614808 0.387890625 0.8629469871520996 3.303098678588867\n",
      "||||||||||||||||||||1.8016560077667236 1.7320131957530975 0.384765625 0.8684148788452148 3.3800156116485596\n",
      "||||||||||||||||||||1.7506206035614014 1.717849999666214 0.371875 0.8615152835845947 3.322131395339966\n",
      "||||||||||||||||||||1.6364333629608154 1.7202729284763336 0.39296875 0.8860259056091309 4.071230888366699\n",
      "||||||||||||||||||||1.695284128189087 1.7121949195861816 0.371875 0.8715410232543945 3.3051979541778564\n",
      "||||||||||||||||||||1.8102890253067017 1.7419598579406739 0.36484375 0.863060474395752 3.4516100883483887\n",
      "||||||||||||||||||||1.6582386493682861 1.6894063830375672 0.387109375 0.8644845485687256 3.5749752521514893\n",
      "||||||||||||||||||||1.6174182891845703 1.7318090736865996 0.373046875 0.8471968173980713 4.0185160636901855\n",
      "||||||||||||||||||||1.7194268703460693 1.749372160434723 0.384375 0.8447978496551514 3.1148908138275146\n",
      "||||||||||||||||||||1.7647762298583984 1.7116295039653777 0.381640625 0.8585293292999268 3.689343214035034\n",
      "||||||||||||||||||||1.682304859161377 1.7109462201595307 0.376953125 0.8487691879272461 3.293012857437134\n",
      "||||||||||||||||||||1.7720166444778442 1.6994637608528138 0.38671875 0.8636314868927002 3.267690658569336\n",
      "||||||||||||||||||||1.669023871421814 1.6879855632781982 0.398046875 0.8619604110717773 3.737750291824341\n",
      "||||||||||||||||||||1.7167019844055176 1.6912994861602784 0.3875 0.8618028163909912 2.9726626873016357\n",
      "||||||||||||||||||||1.5614545345306396 1.7063422799110413 0.385546875 0.8692224025726318 3.759756088256836\n",
      "||||||||||||||||||||1.8982295989990234 1.6972798466682435 0.3828125 0.8703262805938721 3.1962006092071533\n",
      "|||||||||||||||||||Epoch:  10\n",
      "|1.86294686794281 1.7170445144176483 0.384375 0.8984599113464355 3.0566515922546387\n",
      "||||||||||||||||||||1.656819462776184 1.6972751557826995 0.386328125 0.849931001663208 3.2303109169006348\n",
      "||||||||||||||||||||1.6999883651733398 1.707843041419983 0.3703125 0.851266622543335 3.631314277648926\n",
      "||||||||||||||||||||1.7290784120559692 1.741332119703293 0.363671875 0.8658628463745117 3.1168243885040283\n",
      "||||||||||||||||||||1.6959367990493774 1.6905146837234497 0.387890625 0.8750286102294922 3.3855950832366943\n",
      "||||||||||||||||||||1.6394950151443481 1.6987093687057495 0.3921875 0.8577408790588379 3.614718198776245\n",
      "||||||||||||||||||||1.84512460231781 1.72287215590477 0.384375 0.8732621669769287 2.9649009704589844\n",
      "||||||||||||||||||||1.7553818225860596 1.7190617978572846 0.382421875 0.8613348007202148 3.1780827045440674\n",
      "||||||||||||||||||||1.6657392978668213 1.699593448638916 0.391015625 0.8795485496520996 3.6438028812408447\n",
      "||||||||||||||||||||1.641142725944519 1.7216648578643798 0.384765625 0.8710095882415771 3.957486391067505\n",
      "||||||||||||||||||||1.7824615240097046 1.7209929645061492 0.3796875 0.8594701290130615 3.377018451690674\n",
      "||||||||||||||||||||1.646454095840454 1.7106612205505372 0.378125 0.8800523281097412 3.730611801147461\n",
      "||||||||||||||||||||1.73214590549469 1.7469532787799835 0.371484375 0.8787457942962646 3.341564893722534\n",
      "||||||||||||||||||||1.608941674232483 1.7121161341667175 0.38984375 0.8811192512512207 3.3993747234344482\n",
      "||||||||||||||||||||1.7431402206420898 1.7255006670951842 0.38203125 0.8520162105560303 3.632727861404419\n",
      "||||||||||||||||||||1.6527090072631836 1.7099496364593505 0.389453125 0.862743616104126 3.8093087673187256\n",
      "||||||||||||||||||||1.783862590789795 1.721460771560669 0.38515625 0.8615851402282715 3.471059799194336\n",
      "||||||||||||||||||||1.6325567960739136 1.6532924711704253 0.408203125 0.8504879474639893 3.672393798828125\n",
      "||||||||||||||||||||1.9110088348388672 1.7259358286857605 0.388671875 0.8676652908325195 3.278428554534912\n",
      "||||||||||||||||||||1.6694574356079102 1.718246042728424 0.378125 0.848818302154541 3.2370920181274414\n",
      "||||||||||||||||||||1.620711088180542 1.6824933111667633 0.392578125 0.8517839908599854 4.0407538414001465\n",
      "||||||||||||||||||||1.7129504680633545 1.7020513415336609 0.376953125 0.8527846336364746 3.1127493381500244\n",
      "||||||||||||||||||||1.656582236289978 1.7075679779052735 0.396875 0.8567461967468262 3.736246109008789\n",
      "||||||||||||||||||||1.7103183269500732 1.7167492270469666 0.37109375 0.8495569229125977 3.2651660442352295\n",
      "|||||||Epoch:  11\n",
      "|||||||||||||1.7400002479553223 1.743133044242859 0.38203125 0.8685870170593262 3.6059741973876953\n",
      "||||||||||||||||||||1.6802356243133545 1.702529776096344 0.391015625 0.85030198097229 3.4515206813812256\n",
      "||||||||||||||||||||1.6879987716674805 1.6949993669986725 0.390625 0.850355863571167 3.455095052719116\n",
      "||||||||||||||||||||1.7283085584640503 1.6985666275024414 0.39453125 0.8576667308807373 3.248676061630249\n",
      "||||||||||||||||||||1.7221027612686157 1.7308194041252136 0.3828125 0.8730764389038086 2.9062483310699463\n",
      "||||||||||||||||||||1.6228134632110596 1.7086077630519867 0.384375 0.8649864196777344 3.0526058673858643\n",
      "||||||||||||||||||||1.7060760259628296 1.711298531293869 0.391796875 0.8473889827728271 3.498603105545044\n",
      "||||||||||||||||||||1.7442549467086792 1.7034146189689636 0.380078125 0.8597958087921143 3.2322630882263184\n",
      "||||||||||||||||||||1.6444456577301025 1.713049066066742 0.386328125 0.8740284442901611 3.676164150238037\n",
      "||||||||||||||||||||1.6690711975097656 1.708439201116562 0.383203125 0.8678181171417236 3.6120622158050537\n",
      "||||||||||||||||||||1.6986982822418213 1.714218443632126 0.38125 0.8608663082122803 3.4671339988708496\n",
      "||||||||||||||||||||1.7096214294433594 1.6973743259906768 0.386328125 0.8529019355773926 3.2816808223724365\n",
      "||||||||||||||||||||1.7402217388153076 1.711261934041977 0.38359375 0.8719818592071533 3.3838000297546387\n",
      "||||||||||||||||||||1.726188063621521 1.714650523662567 0.382421875 0.8633296489715576 3.5076096057891846\n",
      "||||||||||||||||||||1.7010221481323242 1.7120532810688018 0.375 0.8556745052337646 3.738363742828369\n",
      "||||||||||||||||||||1.8672988414764404 1.6808512210845947 0.392578125 0.85514235496521 3.858811378479004\n",
      "||||||||||||||||||||1.7510099411010742 1.7235839068889618 0.380078125 0.8627767562866211 3.521942138671875\n",
      "||||||||||||||||||||1.8548557758331299 1.723681902885437 0.377734375 0.8642528057098389 3.694240093231201\n",
      "||||||||||||||||||||1.7391464710235596 1.7066309809684754 0.38984375 0.8508806228637695 3.325171947479248\n",
      "||||||||||||||||||||1.905678153038025 1.727199798822403 0.378125 0.8829131126403809 3.4388511180877686\n",
      "||||||||||||||||||||1.6702271699905396 1.7309158146381378 0.378515625 0.8435647487640381 3.323138952255249\n",
      "||||||||||||||||||||2.006216049194336 1.7298585295677185 0.3765625 0.8783278465270996 3.171314239501953\n",
      "||||||||||||||||||||1.6946362257003784 1.7094676852226258 0.378125 0.8532886505126953 3.5058932304382324\n",
      "|||||||||||||||Epoch:  12\n",
      "|||||1.7406235933303833 1.726341587305069 0.377734375 0.8651463985443115 3.5176963806152344\n",
      "||||||||||||||||||||1.6438441276550293 1.739400362968445 0.362890625 0.8431661128997803 3.8213632106781006\n",
      "||||||||||||||||||||1.7220593690872192 1.701420146226883 0.375 0.8829264640808105 3.814330577850342\n",
      "||||||||||||||||||||1.6103565692901611 1.7046499729156495 0.393359375 0.859459638595581 3.3059544563293457\n",
      "||||||||||||||||||||1.6520109176635742 1.7043759524822235 0.373046875 0.8892803192138672 3.292020082473755\n",
      "||||||||||||||||||||1.750613808631897 1.710223925113678 0.401171875 0.8483905792236328 3.537858486175537\n",
      "||||||||||||||||||||1.710274338722229 1.7399897158145905 0.37578125 0.861398458480835 3.1272928714752197\n",
      "||||||||||||||||||||1.7726385593414307 1.6998814582824706 0.3828125 0.8568785190582275 3.729762554168701\n",
      "||||||||||||||||||||1.6043963432312012 1.713739138841629 0.38515625 0.868304967880249 3.528244733810425\n",
      "||||||||||||||||||||1.7399219274520874 1.694712269306183 0.378515625 0.8469517230987549 2.926823139190674\n",
      "||||||||||||||||||||1.5925337076187134 1.6973321080207824 0.3890625 0.8587887287139893 3.426532506942749\n",
      "||||||||||||||||||||1.6726791858673096 1.7000685155391693 0.3890625 0.8607654571533203 3.603600025177002\n",
      "||||||||||||||||||||1.719022274017334 1.7310124278068542 0.38046875 0.8653841018676758 3.768986701965332\n",
      "||||||||||||||||||||1.562471866607666 1.7056082844734193 0.38125 0.8760683536529541 3.8888492584228516\n",
      "||||||||||||||||||||1.7978698015213013 1.699109321832657 0.376171875 0.8417222499847412 3.673698663711548\n",
      "||||||||||||||||||||1.6997613906860352 1.7138030350208282 0.387890625 0.8557858467102051 3.4877429008483887\n",
      "||||||||||||||||||||1.5661627054214478 1.6988170385360717 0.38671875 0.8615806102752686 3.3185060024261475\n",
      "||||||||||||||||||||1.7300536632537842 1.7025622427463531 0.382421875 0.8472704887390137 3.407245397567749\n",
      "||||||||||||||||||||1.7273272275924683 1.7099954307079315 0.39140625 0.862058162689209 3.176513671875\n",
      "||||||||||||||||||||1.6532257795333862 1.713732773065567 0.388671875 0.8679213523864746 3.306426763534546\n",
      "||||||||||||||||||||1.7663766145706177 1.756665086746216 0.375 0.8716034889221191 3.5567467212677\n",
      "||||||||||||||||||||1.748104214668274 1.731350189447403 0.38203125 0.8415837287902832 3.29189133644104\n",
      "||||||||||||||||||||1.7692980766296387 1.723259150981903 0.37734375 0.8705499172210693 3.584300994873047\n",
      "||||||||||||||||||||1.6263391971588135 1.6944384336471559 0.403125 0.8592855930328369 3.3673157691955566\n",
      "|||Epoch:  13\n",
      "|||||||||||||||||1.8223766088485718 1.7152301847934723 0.368359375 0.8755855560302734 3.2138266563415527\n",
      "||||||||||||||||||||1.6866858005523682 1.714627856016159 0.392578125 0.8429505825042725 3.52417254447937\n",
      "||||||||||||||||||||1.6527931690216064 1.6976712346076965 0.3984375 0.8612406253814697 3.3825762271881104\n",
      "||||||||||||||||||||1.6911325454711914 1.7167875170707703 0.375 0.865070104598999 3.8361153602600098\n",
      "||||||||||||||||||||1.5441184043884277 1.7220918595790864 0.3765625 0.854790210723877 3.49942684173584\n",
      "||||||||||||||||||||1.7069848775863647 1.711313945055008 0.380859375 0.8678610324859619 3.5141875743865967\n",
      "||||||||||||||||||||1.6384868621826172 1.7117044091224671 0.382421875 0.8386540412902832 3.5345284938812256\n",
      "||||||||||||||||||||1.7524802684783936 1.7329090297222138 0.381640625 0.8495938777923584 2.9522557258605957\n",
      "||||||||||||||||||||1.708990454673767 1.696155881881714 0.392578125 0.880589485168457 3.437565803527832\n",
      "||||||||||||||||||||1.7021604776382446 1.6819823801517486 0.399609375 0.8556196689605713 3.0457568168640137\n",
      "||||||||||||||||||||1.5532467365264893 1.7248493075370788 0.375390625 0.8558011054992676 3.4981601238250732\n",
      "||||||||||||||||||||1.7596862316131592 1.7274358868598938 0.383984375 0.8601336479187012 3.489455461502075\n",
      "||||||||||||||||||||1.6252388954162598 1.7253470301628113 0.378515625 0.8613710403442383 4.001928329467773\n",
      "||||||||||||||||||||1.603489637374878 1.7115433931350708 0.392578125 0.856605052947998 3.477311134338379\n",
      "||||||||||||||||||||1.869463562965393 1.694076007604599 0.3890625 0.8662822246551514 3.3493845462799072\n",
      "||||||||||||||||||||1.6376398801803589 1.728363960981369 0.373828125 0.8549902439117432 3.398327589035034\n",
      "||||||||||||||||||||1.7810825109481812 1.6872251391410829 0.40234375 0.8723287582397461 3.203965187072754\n",
      "||||||||||||||||||||1.6693753004074097 1.7206743001937865 0.379296875 0.8530068397521973 3.394784688949585\n",
      "||||||||||||||||||||1.805637240409851 1.7058101654052735 0.378125 0.8798532485961914 3.3097269535064697\n",
      "||||||||||||||||||||1.7297694683074951 1.6706479370594025 0.399609375 0.8730454444885254 3.6217544078826904\n",
      "||||||||||||||||||||1.7177976369857788 1.7309507727622986 0.36640625 0.8761775493621826 3.7111799716949463\n",
      "||||||||||||||||||||1.6772311925888062 1.7292177021503448 0.387109375 0.8448572158813477 3.672180652618408\n",
      "||||||||||||||||||||1.6863430738449097 1.7127257227897643 0.383984375 0.858466386795044 3.7687265872955322\n",
      "|||||||||||Epoch:  14\n",
      "|||||||||1.693604826927185 1.7152016282081604 0.37265625 0.8714337348937988 3.8408987522125244\n",
      "||||||||||||||||||||1.5640579462051392 1.7094308972358703 0.384375 0.851963996887207 3.4524011611938477\n",
      "||||||||||||||||||||1.7032208442687988 1.6958110809326172 0.38046875 0.8488638401031494 3.4027109146118164\n",
      "||||||||||||||||||||1.7199288606643677 1.7162383019924163 0.38046875 0.8561704158782959 3.5115981101989746\n",
      "||||||||||||||||||||1.6463360786437988 1.703539389371872 0.37734375 0.899207353591919 3.3247900009155273\n",
      "||||||||||||||||||||1.8174378871917725 1.7069576621055602 0.38203125 0.8590888977050781 3.3823904991149902\n",
      "||||||||||||||||||||1.681260347366333 1.7011139035224914 0.3890625 0.8536622524261475 3.6806156635284424\n",
      "||||||||||||||||||||1.7269530296325684 1.7301547169685363 0.381640625 0.8439188003540039 3.3809008598327637\n",
      "||||||||||||||||||||1.7074922323226929 1.7172554910182953 0.38203125 0.8666760921478271 3.6820085048675537\n",
      "||||||||||||||||||||1.8106580972671509 1.7138689458370209 0.39453125 0.8814487457275391 3.1353628635406494\n",
      "||||||||||||||||||||1.7071117162704468 1.7302068173885345 0.376953125 0.853114128112793 3.3160691261291504\n",
      "||||||||||||||||||||1.822180986404419 1.7458890616893767 0.371875 0.8498115539550781 3.487244129180908\n",
      "||||||||||||||||||||1.7028077840805054 1.7222002029418946 0.376953125 0.85298752784729 3.675231456756592\n",
      "||||||||||||||||||||1.7138137817382812 1.7164498090744018 0.3859375 0.8570811748504639 3.671604633331299\n",
      "||||||||||||||||||||1.6504426002502441 1.7098706543445588 0.366796875 0.8630988597869873 3.638211250305176\n",
      "||||||||||||||||||||1.695797324180603 1.683549177646637 0.38828125 0.8600897789001465 3.0601048469543457\n",
      "||||||||||||||||||||1.7049247026443481 1.7168654680252076 0.376171875 0.856834888458252 3.355376958847046\n",
      "||||||||||||||||||||1.688811182975769 1.6922283828258515 0.398046875 0.8980679512023926 3.112065315246582\n",
      "||||||||||||||||||||1.7082035541534424 1.6838228523731231 0.397265625 0.876039981842041 3.299098491668701\n",
      "||||||||||||||||||||1.692663550376892 1.703143984079361 0.38828125 0.8565442562103271 3.16681170463562\n",
      "||||||||||||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-14fc9d553195>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m         \u001B[0;31m#monitor_gradients(i)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmanage_gradients\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    193\u001B[0m                 \u001B[0mproducts\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    194\u001B[0m         \"\"\"\n\u001B[0;32m--> 195\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m     97\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m     98\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 0\n",
    "sumloss = 0\n",
    "sumacc = 0\n",
    "for k in range(150):\n",
    "    print('Epoch: ', k)\n",
    "    for inp, target in data_loader:\n",
    "        batchstart = time.time()\n",
    "        x = inp.view(BATCH_SIZE, -1, 1).transpose(0,1).to(device)\n",
    "        x = encode_input(x[1:], x[:-1])\n",
    "        #print(x.shape)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(x)\n",
    "        #mem, _ = mem_model(x)\n",
    "        #outputs, _ = post_model(mem[-1].expand(56, BATCH_SIZE, 256))\n",
    "        outputs = outputs[-56:]\n",
    "        loss = ce(outputs.mean(dim=0), target)\n",
    "\n",
    "        loss.backward()\n",
    "        #monitor_gradients(i)\n",
    "        if manage_gradients(i):\n",
    "            optimizer.step()\n",
    "            print('.', end='')\n",
    "        else:\n",
    "            print('|', end='')\n",
    "        sumloss += loss.item()\n",
    "        sumacc += (torch.argmax(outputs.mean(dim=0), 1) == target).float().mean().item()\n",
    "        if i%20 == 0:\n",
    "            print(loss.item(), sumloss/20, sumacc/20, time.time()-batchstart, outputs.var(1).mean().item()) #torch.argmax(outputs[-1], 1).float().var()\n",
    "            sumloss = 0\n",
    "            sumacc = 0\n",
    "        i += 1\n",
    "    model.save('../../models/adap_clip5_'+str(k))\n",
    "    #post_model.save('../../models/post_big11_'+str(k))\n",
    "\n",
    "\n",
    "print('Total time: ', time.time()-start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model.save('../../models/seq_mnist_rsnn1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion = torch.zeros([10,10])\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    acc = 0\n",
    "    for inp, target in test_loader:\n",
    "        x = inp.view(inp.shape[0], -1, 1).transpose(0,1).to(device)\n",
    "        target = target.to(device)\n",
    "        mem, _ = mem_model(x)\n",
    "        outputs, _ = post_model(mem[-1].expand(56, x.shape[1], 256))\n",
    "        choice = torch.argmax(outputs.mean(dim=0), 1)\n",
    "        acc += (choice == target).float().mean()\n",
    "        i += 1\n",
    "        for k in range(len(target)):\n",
    "            confusion[choice[k], target[k]] += 1\n",
    "    print(acc/i)\n",
    "print(confusion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max = confusion.max().item()\n",
    "from PIL import Image\n",
    "img = Image.new('L',(10,10),color=128)\n",
    "for i in range(10):\n",
    "    for k in range(10):\n",
    "        img.putpixel((i, k), int(confusion[i,k]/max*255))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img.resize((500, 500))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testi = MNIST('../../', train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show = []\n",
    "schoice = []\n",
    "starget = []\n",
    "for img, target in testi:\n",
    "    x = transforms.ToTensor()(img).view(-1, 1, 1).to(device)\n",
    "    mem, _ = mem_model(x)\n",
    "    outputs, _ = post_model(mem[-1].expand(56, 1, 256))\n",
    "    choice = torch.argmax(outputs.mean(dim=0), 1).item()\n",
    "    if choice != target:\n",
    "        show.append(img)\n",
    "        schoice.append(choice)\n",
    "        starget.append(target)\n",
    "        if len(show) == 10:\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show[7].resize((500,500))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(schoice)\n",
    "print(starget)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mem_model.model.model.layers.shortterm_synapse.named_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, p in n_mem.named_parameters():\n",
    "    print(name, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, p in mem_model2.named_parameters():\n",
    "    print(name, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mem_model2 = torch.load('../../models/mem_nores3_76')\n",
    "n_mem = make_SequenceWrapper(DynNetwork(mem_loop), USE_JIT)\n",
    "with torch.no_grad():\n",
    "    n_mem.model.layers.output_synapse.weight = mem_model2.model.layers.output_synapse.weight\n",
    "    n_mem.model.layers.output_synapse.bias = mem_model2.model.layers.output_synapse.bias\n",
    "    n_mem.model.layers.output.initial_mem = mem_model2.model.layers.output.initial_mem\n",
    "    n_mem.model.layers.pre_mem_synapse.bias = mem_model2.model.layers.pre_mem_synapse.bias\n",
    "    n_mem.model.layers.pre_mem.initial_mem = mem_model2.model.layers.pre_mem.initial_mem\n",
    "    n_mem.model.layers.pre_mem_synapse.weight[:, :129] = mem_model2.model.layers.pre_mem_synapse.weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader.__iter__().__next__()[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "28*28"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "inp, target = data_loader.__iter__().__next__()\n",
    "x = inp.view(BATCH_SIZE, -1, 1).transpose(0,1).to(device)\n",
    "x = encode_input(x[1:], x[:-1]).cpu()\n",
    "inpimg = transforms.ToPILImage()(x[:,4,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inpimg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target[4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name, p.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "trans, img = pickle.load(open('../some_img', 'rb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "timg = torch.tensor(img).view(-1, 1, 1).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mytrans = torch.cat((torch.zeros((1, 81)), encode_input(timg[1:], timg[:-1]).squeeze().cpu()), dim=0)\n",
    "their_trans = torch.cat((torch.zeros((840, 1)), torch.tensor(trans).squeeze()), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mytrans.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "their_trans.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "pimg = Image.new('RGB',(840,81),color=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(81):\n",
    "    for k in range(840):\n",
    "        pimg.putpixel((k, i), (int(mytrans[k, i])*255, int(their_trans[k, i*2%81])*255, 0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pimg.save('input_comparison', 'png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pimg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}