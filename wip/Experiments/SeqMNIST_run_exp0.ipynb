{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "#TODO: path\n",
    "sys.path.append('../../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "#TODO: change to 256\n",
    "BATCH_SIZE = 128#256\n",
    "\n",
    "USE_JIT = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "mnist = MNIST('../../', transform=transforms.ToTensor(), download=True) #distortion_transform([0,15], 3)\n",
    "test = MNIST('../../', transform=transforms.ToTensor(), train=False)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(mnist, batch_size=BATCH_SIZE, drop_last=True, num_workers=0, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=1024, drop_last=False, num_workers=0)\n",
    "\n",
    "like_bellec = {\n",
    "    'spkfn' : 'bellec',\n",
    "    'spkconfig' : 0,\n",
    "    'architecture': '1L',\n",
    "    'beta': 0.95,\n",
    "    'control_neuron': 'LIF',\n",
    "    'mem_neuron' : 'Adaptive',\n",
    "    'lr' : 1e-2,\n",
    "    '1-beta': True,\n",
    "    'decay_out': True\n",
    "}\n",
    "\n",
    "spec = like_bellec\n",
    "#spec['decay_out'] = False\n",
    "#TODO: remove\n",
    "#spec['1-beta'] = False\n",
    "\n",
    "from Code.Networks import Selector, DynNetwork, OuterWrapper, LSTMWrapper, ReLuWrapper, DummyNeuron, make_SequenceWrapper, ParallelNetwork\n",
    "from Code.NewNeurons2 import SeqOnlySpike, CooldownNeuron, OutputNeuron, LIFNeuron, NoResetNeuron, AdaptiveNeuron\n",
    "\n",
    "built_config = {\n",
    "    'BETA': spec['beta'],\n",
    "    'OFFSET': 2, # TODO: this?\n",
    "    'SPIKE_FN': spec['spkfn'],\n",
    "    '1-beta': spec['1-beta'],\n",
    "    'ADAPDECAY': 0.9985,\n",
    "    'ADAPSCALE': 180\n",
    "}\n",
    "\n",
    "n_control = 120\n",
    "n_mem = 100\n",
    "\n",
    "control_lookup = {\n",
    "    'LIF': LIFNeuron,\n",
    "    'Disc': SeqOnlySpike,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "mem_lookup = {\n",
    "    'Adaptive': AdaptiveNeuron,\n",
    "    'Cooldown': CooldownNeuron,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "control_neuron = control_lookup[spec['control_neuron']](n_control, built_config)\n",
    "mem_neuron = mem_lookup[spec['mem_neuron']](n_mem, built_config)\n",
    "out_neuron = OutputNeuron(n_control+n_mem, built_config) if spec['decay_out'] else DummyNeuron(n_control+n_mem, built_config)\n",
    "\n",
    "\n",
    "loop_2L = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('control', [['input', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['control'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop_1L = OrderedDict([\n",
    "    ('input', 81),\n",
    "    ('control', [['input', 'control', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['input', 'control', 'mem'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop = loop_1L if spec['architecture'] == '1L' else loop_2L\n",
    "\n",
    "loop_model = OuterWrapper(make_SequenceWrapper(ParallelNetwork(loop), USE_JIT), device, USE_JIT)\n",
    "\n",
    "final_linear = nn.Linear(n_control+n_mem, 10).to(device)\n",
    "\n",
    "\n",
    "\n",
    "params = list(loop_model.parameters())+list(final_linear.parameters())\n",
    "lr = spec['lr']\n",
    "optimizer = optim.Adam(params, lr=lr)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "\n",
    "#TODO: check correctness here\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        loop_model.pretrace.model.layers.mem_synapse.weight[i, i+201] = 0\n",
    "\n",
    "    for i in range(120):\n",
    "        loop_model.pretrace.model.layers.control_synapse.weight[i, i+81] = 0\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "trigger_signal = torch.ones([783+56, 1, 1], device=device)\n",
    "trigger_signal[:783] = 0\n",
    "def encode_input(curr, last):\n",
    "    out = torch.zeros([783+56, curr.shape[1], 2,40], device=curr.device)\n",
    "    out[:783, :, 0, :] = ((torch.arange(40, device=curr.device) < 40 * last) & (torch.arange(40, device=curr.device) > 40 * curr)).float()\n",
    "    out[:783, :, 1, :] = ((torch.arange(40, device=curr.device) > 40 * last) & (torch.arange(40, device=curr.device) < 40 * curr)).float()\n",
    "    out = torch.cat((out.view([783+56, curr.shape[1], 80]), trigger_signal.expand([783+56, curr.shape[1], 1])), dim=-1)\n",
    "    return out\n",
    "\n",
    "stats = {\n",
    "    'grad_norm': [],\n",
    "    'loss': [],\n",
    "    'acc': [],\n",
    "    'batch_var': []\n",
    "}\n",
    "\n",
    "grad_norm_history = []\n",
    "def record_norm():\n",
    "    norms = []\n",
    "    for p in params:\n",
    "        norms.append(p.grad.norm().item())\n",
    "    stats['grad_norm'].append(torch.tensor(norms).norm().item())\n",
    "\n",
    "\n",
    "ITERATIONS = 36000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "2.3025848865509033 0.125 0.0 0.006161743076518178 5 0.0\n",
      "2.3026351928710938 0.0625 0.0 0.006130308611318469 2 0.0\n",
      "2.3023581504821777 0.125 0.0 0.005754795856773853 9 0.0\n",
      "2.3022964000701904 0.109375 0.0 0.006037699640728533 6 0.0\n",
      "2.305745840072632 0.0859375 0.0 0.005270394613035023 2 0.0\n",
      "2.3032639026641846 0.09375 0.0 0.004994799382984638 7 0.0\n",
      "2.3048617839813232 0.09375 0.0 0.0045761046931147575 6 0.0\n",
      "2.3012917041778564 0.0703125 0.0 0.007414498832076788 6 0.0\n",
      "2.301555633544922 0.1171875 6.604082831604193e-19 0.004308718489482999 5 0.6755813360214233\n",
      "2.2982327938079834 0.125 3.1570808051867457e-18 0.005354820750653744 5 1.7809325456619263\n",
      "2.3051164150238037 0.125 3.581160009784616e-15 0.005532428622245789 6 4.745429515838623\n",
      "2.300459623336792 0.1171875 1.9023432494254342e-15 0.006046640337444842 0 6.503476619720459\n",
      "2.304121971130371 0.0859375 8.262989359582307e-14 0.004799041780643165 1 8.712943077087402\n",
      "2.3019790649414062 0.09375 2.434931770101123e-11 0.008298408356495202 0 14.145621299743652\n",
      "2.300734281539917 0.140625 1.1328430598103734e-12 0.006282951799221337 2 20.544626235961914\n",
      "2.304960250854492 0.09375 1.1410917783588603e-12 0.008993771625682712 0 27.10210609436035\n",
      "2.3015296459198 0.125 4.984047462935903e-12 0.007132275495678186 4 31.796092987060547\n",
      "2.3001513481140137 0.0703125 2.3113891443188317e-12 0.00825935392640531 0 36.07468795776367\n",
      "2.3053371906280518 0.0859375 9.629291808366247e-10 0.008878947119228542 2 35.67633819580078\n",
      "2.305976629257202 0.09375 4.672928710647284e-11 0.006805231678299606 0 42.39555358886719\n",
      "2.305976629257202 2.3027596354484556 0.101953125 2.2302298545837402 4.672928710647284e-11\n",
      "2.3027024269104004 0.125 1.1227733187624267e-09 0.009454214596189559 3 46.58879089355469\n",
      "2.298177480697632 0.140625 5.3976130232058495e-09 0.00790157646406442 6 51.129085540771484\n",
      "2.310134172439575 0.0703125 2.5358506405837034e-08 0.010189360473304987 5 55.654937744140625\n",
      "2.2974612712860107 0.1328125 9.699168401766656e-08 0.009097290458157659 5 59.6932373046875\n",
      "2.3040852546691895 0.1015625 2.2897906148955371e-07 0.007046505343168974 2 67.08692932128906\n",
      "2.296637773513794 0.1484375 1.6059848917393538e-07 0.00890218245331198 2 70.44844818115234\n",
      "2.301541805267334 0.125 1.5547996667919506e-07 0.008571596117690206 3 77.79620361328125\n",
      "2.2904717922210693 0.1953125 7.997063988796071e-08 0.012811545748263597 8 81.73639678955078\n",
      "2.2988295555114746 0.1015625 1.9309392484956334e-07 0.011767743853852153 2 87.79732513427734\n",
      "2.2976884841918945 0.1171875 1.2828724038627115e-07 0.007680879207327962 7 93.0762939453125\n",
      "2.2957820892333984 0.109375 7.455732884409372e-07 0.013711459469050169 7 94.75208282470703\n",
      "2.3039391040802 0.078125 4.704730258708878e-07 0.010626692092046142 1 102.44123077392578\n",
      "2.3015453815460205 0.109375 2.131073415512219e-07 0.016953330487012863 9 107.46226501464844\n",
      "2.302259683609009 0.1328125 1.1043214698247539e-07 0.013492964208126068 1 117.23870849609375\n",
      "2.2958483695983887 0.1328125 2.920424151398038e-07 0.019606458954513073 7 124.73815155029297\n",
      "2.2975378036499023 0.109375 2.9248997179820435e-07 0.013140746159479022 4 137.27548217773438\n",
      "2.2951223850250244 0.15625 3.8860747508806526e-07 0.014104495057836175 7 145.62734985351562\n",
      "2.302541494369507 0.125 8.044095807235863e-07 0.013133699540048838 5 158.2041473388672\n",
      "2.2883176803588867 0.1796875 1.1913818980247015e-06 0.015482810558751225 4 167.33535766601562\n",
      "2.2925987243652344 0.1484375 2.9612517664645566e-06 0.03645665477961302 5 177.95738220214844\n",
      "2.2925987243652344 2.298661136627197 0.126953125 2.224574089050293 2.9612517664645566e-06\n",
      "2.285031318664551 0.203125 3.1405190838995622e-06 0.020663852337747812 8 181.406982421875\n",
      "2.288869857788086 0.15625 3.2696748348826077e-06 0.025494347792118788 8 188.603271484375\n",
      "2.2970123291015625 0.15625 9.912197356243269e-07 0.026217482518404722 5 209.8198699951172\n",
      "2.2824349403381348 0.1953125 2.458998324073036e-06 0.025316488463431597 7 222.43312072753906\n",
      "2.2845265865325928 0.1875 9.14165468657302e-07 0.027784290723502636 3 249.83045959472656\n",
      "2.269698143005371 0.203125 7.861823178245686e-06 0.03991108853369951 7 277.3356018066406\n",
      "2.286090612411499 0.1328125 5.23562357557239e-06 0.032047322019934654 4 294.31695556640625\n",
      "2.2859301567077637 0.1796875 1.5911938362478395e-06 0.04180045332759619 5 328.6261901855469\n",
      "2.267049551010132 0.1640625 8.549920494260732e-06 0.049192262813448906 2 360.1322937011719\n",
      "2.2802176475524902 0.171875 2.7717228476831224e-06 0.04574291408061981 7 401.11065673828125\n",
      "2.2230727672576904 0.265625 1.986448114621453e-05 0.15346561558544636 7 454.8792724609375\n",
      "2.259368896484375 0.25 3.6455335248319898e-06 0.07296029943972826 9 513.4150390625\n",
      "2.2242116928100586 0.28125 8.17767340777209e-06 0.19197581335902214 8 569.4248657226562\n",
      "2.1882474422454834 0.2265625 6.588092946913093e-05 0.20983289927244186 9 683.2832641601562\n",
      "2.2059237957000732 0.1484375 4.8708672693464905e-05 0.24130279198288918 3 828.9893188476562\n",
      "2.175318956375122 0.234375 6.363246211549267e-05 0.3045385517179966 0 955.8694458007812\n",
      "2.0809340476989746 0.21875 0.0002051170013146475 0.5271612852811813 7 1183.0953369140625\n",
      "2.0725486278533936 0.21875 0.0006095406715758145 0.22321103140711784 0 1426.912841796875\n",
      "2.1657602787017822 0.109375 0.0013037507887929678 0.517110750079155 5 1686.3232421875\n",
      "2.2993862628936768 0.0859375 0.002344026928767562 2.590189278125763 2 1789.3114013671875\n",
      "2.2993862628936768 2.2360816955566407 0.189453125 2.235365152359009 0.002344026928767562\n",
      "2.127614974975586 0.1328125 0.0029903107788413763 0.9156042337417603 3 1725.01904296875\n",
      "2.0870447158813477 0.1796875 0.0003958473389502615 0.21982412785291672 6 1628.49951171875\n",
      "2.117849826812744 0.171875 0.0005056136287748814 0.28666768223047256 7 1542.3282470703125\n",
      "2.105006694793701 0.2109375 0.0007755482220090926 0.2663259394466877 4 1472.4248046875\n",
      "2.149397373199463 0.1015625 7.97676548245363e-05 0.3834134340286255 5 1447.616943359375\n",
      "2.1065924167633057 0.171875 0.0005947822937741876 0.2487616427242756 6 1410.5018310546875\n",
      "2.1420793533325195 0.1796875 9.474320540903136e-05 0.285514909774065 0 1387.1573486328125\n",
      "2.0989437103271484 0.1953125 0.00013031107664573938 0.35447459667921066 6 1412.8973388671875\n",
      "2.067915916442871 0.1875 0.00026292912662029266 0.36852747201919556 3 1422.276123046875\n",
      "2.0730154514312744 0.2265625 0.0004669461923185736 0.33423878252506256 2 1420.8304443359375\n",
      "2.0938291549682617 0.1796875 0.00019851065007969737 0.2437307871878147 3 1504.85302734375\n",
      "2.1101150512695312 0.1484375 0.0004967478453181684 0.14332099817693233 0 1559.122314453125\n",
      "2.0873913764953613 0.2578125 0.0006217476911842823 0.141792930662632 9 1585.3431396484375\n",
      "2.0472452640533447 0.21875 0.0010141106322407722 0.3585943207144737 1 1635.90087890625\n",
      "2.067909002304077 0.1640625 0.0013897629687562585 0.07623263634741306 2 1693.09619140625\n",
      "2.103928327560425 0.1796875 0.000780156347900629 0.6641868501901627 9 1741.950927734375\n",
      "2.041135311126709 0.2109375 0.0027030082419514656 0.703602209687233 1 1750.123046875\n",
      "1.9226982593536377 0.28125 0.0027406008448451757 0.079814363270998 2 1748.015625\n",
      "2.001110076904297 0.203125 0.0018757237121462822 0.03359813475981355 0 1738.5306396484375\n",
      "2.0243234634399414 0.21875 0.0010533870663493872 0.06794405169785023 1 1722.622314453125\n",
      "2.0243234634399414 2.0787572860717773 0.191015625 2.219501256942749 0.0010533870663493872\n",
      "2.013134241104126 0.1796875 0.0016937664477154613 0.24119248613715172 3 1727.8013916015625\n",
      "1.980181097984314 0.1875 0.000990586238913238 0.062149674631655216 0 1707.9102783203125\n",
      "2.0864338874816895 0.1953125 0.0015012561343610287 0.22434955462813377 0 1689.679443359375\n",
      "2.026411294937134 0.1953125 0.0007480672793462873 0.43200448155403137 3 1687.0166015625\n",
      "2.068916082382202 0.1796875 0.0006191923748701811 0.14631499536335468 8 1712.437744140625\n",
      "2.0594868659973145 0.171875 0.0005079155671410263 0.1509891264140606 5 1708.4107666015625\n",
      "2.0759057998657227 0.203125 0.001201746053993702 0.1178312860429287 4 1720.0013427734375\n",
      "1.9921998977661133 0.1953125 0.0021558122243732214 0.0338176591321826 3 1711.3447265625\n",
      "1.9633147716522217 0.21875 0.0009432838414795697 0.46090345829725266 5 1725.9818115234375\n",
      "1.9584840536117554 0.21875 0.0020285933278501034 0.05526931490749121 3 1740.344482421875\n",
      "2.043334484100342 0.171875 0.002522175433114171 0.5878586694598198 1 1771.7529296875\n",
      "2.0085480213165283 0.171875 0.0008608701755292714 0.11229640804231167 6 1760.8787841796875\n",
      "1.9765619039535522 0.1953125 0.0010432798881083727 0.12222962453961372 5 1752.05126953125\n",
      "1.9578063488006592 0.1953125 0.0015736573841422796 0.033609652891755104 9 1752.81005859375\n",
      "1.9547747373580933 0.2109375 0.0009865817846730351 0.2563886158168316 8 1746.7398681640625\n",
      "1.9995125532150269 0.171875 0.0016303706215694547 0.07502328138798475 3 1736.79833984375\n",
      "2.0190584659576416 0.2421875 0.0012243874371051788 0.05851892754435539 6 1740.244140625\n",
      "1.97391676902771 0.171875 0.0015022051520645618 0.08146307431161404 7 1765.7115478515625\n",
      "1.9285428524017334 0.21875 0.0019647390581667423 0.1611306145787239 6 1752.088134765625\n",
      "1.968585729598999 0.25 0.0021946385968476534 0.08654782548546791 1 1772.8380126953125\n",
      "1.968585729598999 2.002755492925644 0.197265625 2.243213176727295 0.0021946385968476534\n",
      "1.9674224853515625 0.2890625 0.002343353582546115 0.15003195963799953 3 1766.81982421875\n",
      "2.046415090560913 0.1484375 0.0024786563590168953 0.9185651689767838 9 1792.845458984375\n",
      "1.975703477859497 0.21875 0.0017814699094742537 0.03111230442300439 9 1772.099365234375\n",
      "1.8755112886428833 0.2734375 0.0014390574069693685 0.3820508345961571 7 1759.0684814453125\n",
      "1.9283820390701294 0.2421875 0.002322073793038726 0.05449370946735144 3 1753.98046875\n",
      "1.9545339345932007 0.234375 0.002098625060170889 0.17556468024849892 1 1757.8192138671875\n",
      "2.1265604496002197 0.1796875 0.001373426290228963 0.5520303919911385 1 1756.76904296875\n",
      "2.090864419937134 0.234375 0.0003650079888757318 0.1819019950926304 9 1739.838623046875\n",
      "2.019514560699463 0.265625 0.001722774002701044 0.1660108007490635 9 1740.56591796875\n",
      "1.888156533241272 0.265625 0.0012199415359646082 0.5257556214928627 2 1739.3184814453125\n",
      "1.9176609516143799 0.25 0.001395747414790094 0.4616566002368927 2 1755.03515625\n",
      "1.9373127222061157 0.265625 0.0011052251793444157 0.2792620472609997 3 1774.8616943359375\n",
      "1.9880672693252563 0.234375 0.0028901309706270695 0.3864232823252678 4 1799.9901123046875\n",
      "2.0018677711486816 0.2578125 0.0021384740248322487 0.13359684497117996 9 1826.865966796875\n",
      "2.078547954559326 0.1796875 0.002086753724142909 1.0546933859586716 3 1826.437255859375\n",
      "2.0288634300231934 0.1796875 0.002989265602082014 0.4736632853746414 6 1823.6048583984375\n",
      "1.946516990661621 0.21875 0.0032186144962906837 0.050941151566803455 9 1800.5479736328125\n",
      "1.9136378765106201 0.25 0.0017527471063658595 0.7678640633821487 7 1771.0015869140625\n",
      "1.9366936683654785 0.328125 0.0014475842472165823 0.37995439022779465 5 1760.3558349609375\n",
      "1.9015135765075684 0.3203125 0.0012256648624315858 0.6244908273220062 2 1778.119384765625\n",
      "1.9015135765075684 1.9761873245239259 0.241796875 2.2640719413757324 0.0012256648624315858\n",
      "1.9750016927719116 0.2265625 0.0013525394024327397 0.0666725542396307 9 1791.37744140625\n",
      "1.8548239469528198 0.265625 0.0021360788960009813 0.1940200850367546 8 1811.1324462890625\n",
      "2.0035953521728516 0.296875 0.002391765359789133 0.5684288963675499 6 1824.565185546875\n",
      "1.8681107759475708 0.3515625 0.0029071110766381025 0.19315408542752266 5 1834.6614990234375\n",
      "2.0160794258117676 0.2578125 0.00370846432633698 0.7317289710044861 4 1834.9454345703125\n",
      "1.8961243629455566 0.3828125 0.0020183527376502752 0.15033180825412273 4 1822.21923828125\n",
      "1.9628705978393555 0.2578125 0.0011325947707518935 0.18822817131876945 1 1817.470947265625\n",
      "1.8729549646377563 0.28125 0.001614079112187028 0.25585100054740906 0 1807.27978515625\n",
      "1.7905638217926025 0.2890625 0.004309291951358318 0.6817615777254105 2 1809.9490966796875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-fcf48b1b4700>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloop_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0mmeaned\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m56\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#TODO: what is this value really in bellec?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mout_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfinal_linear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmeaned\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/Networks.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    198\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m             \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpretrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_initial_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtwo_dim\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maddr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/Networks.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    178\u001B[0m         \u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mout1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 180\u001B[0;31m             \u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    181\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/Networks.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m                 \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m'_synapse'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m             \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midxState\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m             \u001B[0mnew_state\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0midxState\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/NewNeurons2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, h)\u001B[0m\n\u001B[1;32m    204\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    205\u001B[0m             \u001B[0mmem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbeta\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mold_mem\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mold_spike\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 206\u001B[0;31m         \u001B[0mspikes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspike_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mold_mem\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    207\u001B[0m         \u001B[0;31m#spikes = torch.where(old_spike > 0, torch.zeros_like(spikes), spikes)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    208\u001B[0m         \u001B[0mnew_h\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 1\n",
    "sumloss = 0\n",
    "sumacc = 0\n",
    "k = 0\n",
    "while i < ITERATIONS:\n",
    "    print('Epoch: ', k)\n",
    "    k = k + 1\n",
    "    for inp, target in data_loader:\n",
    "        batchstart = time.time()\n",
    "        x = inp.view(BATCH_SIZE, -1, 1).transpose(0,1).to(device)\n",
    "        x = encode_input(x[1:], x[:-1])\n",
    "        #print(x.shape)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = loop_model(x)\n",
    "        meaned = outputs[-56:].mean(dim=0) #TODO: what is this value really in bellec?\n",
    "        out_final = final_linear(meaned)\n",
    "        loss = ce(out_final, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            record_norm()\n",
    "            stats['loss'].append(loss.item())\n",
    "            acc = (torch.argmax(out_final, 1) == target).float().mean().item()\n",
    "            stats['acc'].append(acc)\n",
    "            batch_var = meaned.var(0).mean().item()\n",
    "            stats['batch_var'].append(batch_var)\n",
    "        print(loss.item(), acc, batch_var, loop_model.pretrace.model.layers.control_synapse.weight.grad.norm().item()*20, target[0].item(), outputs.norm().item())\n",
    "\n",
    "        sumloss += loss.item()\n",
    "        sumacc += acc\n",
    "        if i%20 == 0:\n",
    "            print(loss.item(), sumloss/20, sumacc/20, time.time()-batchstart, batch_var) #torch.argmax(outputs[-1], 1).float().var()\n",
    "            sumloss = 0\n",
    "            sumacc = 0\n",
    "        if i%2500 == 0:\n",
    "            lr = lr * 0.8\n",
    "            optimizer = optim.Adam(params, lr=lr)\n",
    "            print('Learning Rate: ', lr)\n",
    "        i += 1\n",
    "    pickle.dump(stats, open('loc_stats', 'wb'))\n",
    "    #model.save('../../models/adap_clip5_'+str(k))\n",
    "    #post_model.save('../../models/post_big11_'+str(k))\n",
    "\n",
    "\n",
    "print('Total time: ', time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: what about data augmentation?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrace.model.layers.control_synapse.weight torch.Size([120, 301])\n",
      "pretrace.model.layers.control_synapse.bias torch.Size([120])\n",
      "pretrace.model.layers.control.initial_mem torch.Size([120])\n",
      "pretrace.model.layers.mem_synapse.weight torch.Size([100, 301])\n",
      "pretrace.model.layers.mem_synapse.bias torch.Size([100])\n",
      "pretrace.model.layers.mem.initial_mem torch.Size([100])\n",
      "pretrace.model.layers.output.initial_mem torch.Size([220])\n",
      "weight torch.Size([10, 220])\n",
      "bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, p in loop_model.named_parameters():\n",
    "    print(name, p.shape)\n",
    "for name, p in final_linear.named_parameters():\n",
    "    print(name, p.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pickle\n",
    "o_weights = pickle.load(open('../../weight_transplant', 'rb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "o1 = torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).t()\n",
    "o2 = torch.tensor(o_weights['InputWeights/InputWeight:0']).t()\n",
    "o3 = torch.cat((o1,o2), dim=1)\n",
    "with torch.no_grad():\n",
    "    loop_model.pretrace.model.layers.control_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.mem_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.control_synapse.weight[:, :300] = o3[:120]\n",
    "    loop_model.pretrace.model.layers.mem_synapse.weight[:, :300] = o3[120:]\n",
    "    final_linear.bias *= 0\n",
    "    final_linear.weight.data = torch.tensor(o_weights['out_weight:0']).t()\n",
    "loop_model.to(device)\n",
    "final_linear.to(device)\n",
    "\n",
    "params = [ loop_model.pretrace.model.layers.control_synapse.weight, loop_model.pretrace.model.layers.mem_synapse.weight, final_linear.weight, final_linear.bias]\n",
    "optimizer = optim.Adam(params, lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loop_model.pretrace.model.layers.control_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.mem_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.control_synapse.weight *= 20\n",
    "    loop_model.pretrace.model.layers.mem_synapse.weight *= 20\n",
    "    final_linear.bias *= 0\n",
    "loop_model.to(device)\n",
    "final_linear.to(device)\n",
    "\n",
    "params = [ loop_model.pretrace.model.layers.control_synapse.weight, loop_model.pretrace.model.layers.mem_synapse.weight, final_linear.weight, final_linear.bias]\n",
    "optimizer = optim.Adam(params, lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n, p in o_weights.items():\n",
    "    print(n, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.3832)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(o_weights['out_weight:0']).norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.3832, grad_fn=<NormBackward0>)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_linear.weight.norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 220])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(o_weights['out_weight:0']).t().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0619, -0.0065,  0.0658,  ...,  0.0344,  0.0014, -0.0114],\n        [-0.0217, -0.0400,  0.0581,  ...,  0.0532,  0.0516, -0.0228],\n        [-0.0063,  0.0566, -0.0608,  ..., -0.0267,  0.0659,  0.0009],\n        ...,\n        [-0.0648, -0.0411,  0.0619,  ...,  0.0228, -0.0491, -0.0315],\n        [-0.0311, -0.0110, -0.0269,  ...,  0.0393, -0.0209, -0.0272],\n        [-0.0143,  0.0655, -0.0398,  ...,  0.0148,  0.0179,  0.0602]],\n       device='cuda:0')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_linear.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(14.8264)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).norm()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(8.5887, grad_fn=<NormBackward0>)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(220,220).weight.norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([220, 300])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}