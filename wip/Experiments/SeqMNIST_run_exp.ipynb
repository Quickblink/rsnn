{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "#TODO: path\n",
    "sys.path.append('../../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "#TODO: change to 256\n",
    "BATCH_SIZE = 128#256\n",
    "\n",
    "USE_JIT = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "n_in = 81\n",
    "\n",
    "mnist = MNIST('../../', transform=transforms.ToTensor(), download=True) #distortion_transform([0,15], 3)\n",
    "test = MNIST('../../', transform=transforms.ToTensor(), train=False)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(mnist, batch_size=BATCH_SIZE, drop_last=True, num_workers=0, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=1024, drop_last=False, num_workers=0)\n",
    "\n",
    "like_bellec = {\n",
    "    'spkfn' : 'bellec',\n",
    "    'spkconfig' : 0,\n",
    "    'architecture': '1L',\n",
    "    'beta': 0.95,\n",
    "    'control_neuron': 'LIF',\n",
    "    'mem_neuron' : 'Adaptive',\n",
    "    'lr' : 1e-2,\n",
    "    '1-beta': True,\n",
    "    'decay_out': True\n",
    "}\n",
    "\n",
    "spec = like_bellec\n",
    "#spec['decay_out'] = False\n",
    "#TODO: remove\n",
    "#spec['1-beta'] = False\n",
    "\n",
    "from Code.Networks import Selector, DynNetwork, OuterWrapper, LSTMWrapper, ReLuWrapper, DummyNeuron, make_SequenceWrapper, ParallelNetwork\n",
    "from Code.NewNeurons2 import SeqOnlySpike, CooldownNeuron, OutputNeuron, LIFNeuron, NoResetNeuron, AdaptiveNeuron\n",
    "\n",
    "built_config = {\n",
    "    'BETA': spec['beta'],\n",
    "    'OFFSET': 2, # TODO: this?\n",
    "    'SPIKE_FN': spec['spkfn'],\n",
    "    '1-beta': spec['1-beta'],\n",
    "    'ADAPDECAY': 0.9985,\n",
    "    'ADAPSCALE': 180\n",
    "}\n",
    "\n",
    "n_control = 120\n",
    "n_mem = 100\n",
    "\n",
    "control_lookup = {\n",
    "    'LIF': LIFNeuron,\n",
    "    'Disc': SeqOnlySpike,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "mem_lookup = {\n",
    "    'Adaptive': AdaptiveNeuron,\n",
    "    'Cooldown': CooldownNeuron,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "control_neuron = control_lookup[spec['control_neuron']](n_control, built_config)\n",
    "mem_neuron = mem_lookup[spec['mem_neuron']](n_mem, built_config)\n",
    "out_neuron = OutputNeuron(n_control+n_mem, built_config) if spec['decay_out'] else DummyNeuron(n_control+n_mem, built_config)\n",
    "\n",
    "\n",
    "loop_2L = OrderedDict([\n",
    "    ('input', n_in),\n",
    "    ('control', [['input', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['control'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop_1L = OrderedDict([\n",
    "    ('input', n_in),\n",
    "    ('control', [['input', 'control', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['input', 'control', 'mem'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop = loop_1L if spec['architecture'] == '1L' else loop_2L\n",
    "\n",
    "loop_model = OuterWrapper(make_SequenceWrapper(ParallelNetwork(loop), USE_JIT), device, USE_JIT)\n",
    "\n",
    "final_linear = nn.Linear(n_control+n_mem, 10).to(device)\n",
    "\n",
    "\n",
    "\n",
    "params = list(loop_model.parameters())+list(final_linear.parameters())\n",
    "lr = spec['lr']\n",
    "optimizer = optim.Adam(params, lr=lr)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "\n",
    "#TODO: check correctness here\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        loop_model.pretrace.model.layers.mem_synapse.weight[i, i+201] = 0\n",
    "\n",
    "    for i in range(120):\n",
    "        loop_model.pretrace.model.layers.control_synapse.weight[i, i+81] = 0\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "trigger_signal = torch.ones([783+56, 1, 1], device=device)\n",
    "trigger_signal[:783] = 0\n",
    "def encode_input(curr, last):\n",
    "    out = torch.zeros([783+56, curr.shape[1], 2,40], device=curr.device)\n",
    "    out[:783, :, 0, :] = ((torch.arange(40, device=curr.device) < 40 * last) & (torch.arange(40, device=curr.device) > 40 * curr)).float()\n",
    "    out[:783, :, 1, :] = ((torch.arange(40, device=curr.device) > 40 * last) & (torch.arange(40, device=curr.device) < 40 * curr)).float()\n",
    "    out = torch.cat((out.view([783+56, curr.shape[1], 80]), trigger_signal.expand([783+56, curr.shape[1], 1])), dim=-1)\n",
    "    return out\n",
    "\n",
    "stats = {\n",
    "    'grad_norm': [],\n",
    "    'loss': [],\n",
    "    'acc': [],\n",
    "    'batch_var': []\n",
    "}\n",
    "\n",
    "grad_norm_history = []\n",
    "def record_norm():\n",
    "    norms = []\n",
    "    for p in params:\n",
    "        norms.append(p.grad.norm().item())\n",
    "    stats['grad_norm'].append(torch.tensor(norms).norm().item())\n",
    "\n",
    "\n",
    "ITERATIONS = 36000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "2.3014233112335205 2.302599775791168 0.10078125 2.278226852416992 6.317734113636675e-10\n",
      "2.2966341972351074 2.3008622407913206 0.128515625 2.2598235607147217 1.4453613630394102e-06\n",
      "2.1057357788085938 2.2633551955223083 0.182421875 2.242460250854492 0.0001427228271495551\n",
      "1.9980257749557495 2.101233237981796 0.176953125 2.267404556274414 0.0007529367576353252\n",
      "1.9813475608825684 2.0273540318012238 0.201953125 2.2706615924835205 0.0017053470946848392\n",
      "1.9449255466461182 1.9629006505012512 0.248046875 2.2592811584472656 0.00238977768458426\n",
      "1.9568041563034058 1.9248777449131012 0.291796875 2.263963222503662 0.0016061929054558277\n",
      "1.8096979856491089 1.8500484466552733 0.291796875 2.2819485664367676 0.004246073309332132\n",
      "1.8092256784439087 1.8441750228404998 0.30234375 2.3208301067352295 0.005605306010693312\n",
      "1.7024308443069458 1.7333423137664794 0.3609375 2.262558937072754 0.004948707763105631\n",
      "1.409973382949829 1.6296229720115663 0.408203125 2.2701761722564697 0.007296879310160875\n",
      "1.3958616256713867 1.4827085793018342 0.470703125 2.250434160232544 0.004776963032782078\n",
      "1.5245336294174194 1.4693640291690826 0.48046875 2.283296823501587 0.005773632787168026\n",
      "1.4117661714553833 1.457819265127182 0.4703125 2.2469370365142822 0.007265658117830753\n",
      "1.328688383102417 1.380036360025406 0.50703125 2.2687621116638184 0.0050317090936005116\n",
      "1.3595330715179443 1.4068255841732025 0.500390625 2.2307164669036865 0.0052595739252865314\n",
      "1.2787954807281494 1.3596544802188872 0.495703125 2.209322690963745 0.006608971394598484\n",
      "1.4680535793304443 1.350189596414566 0.516796875 2.2324295043945312 0.007573168724775314\n",
      "1.351860761642456 1.3603715777397156 0.5078125 2.2553348541259766 0.004373433068394661\n",
      "1.2930183410644531 1.3415864706039429 0.5078125 2.22670578956604 0.00539542268961668\n",
      "1.332473635673523 1.3255592823028564 0.503515625 2.231468439102173 0.005543483421206474\n",
      "1.3033027648925781 1.3465859591960907 0.49921875 2.2290680408477783 0.004281262401491404\n",
      "1.248103141784668 1.3237977266311645 0.528515625 2.2324163913726807 0.006926980800926685\n",
      "Epoch:  1\n",
      "1.3496378660202026 1.2840080916881562 0.528515625 2.235116720199585 0.005905349273234606\n",
      "1.312595248222351 1.326277732849121 0.518359375 2.2443060874938965 0.0049559385515749454\n",
      "1.216459035873413 1.3094884872436523 0.509375 2.2757930755615234 0.004596334882080555\n",
      "1.2644696235656738 1.2606246948242188 0.538671875 2.2425200939178467 0.006294499151408672\n",
      "1.2471320629119873 1.253703272342682 0.55 2.256676197052002 0.006009788252413273\n",
      "1.2568979263305664 1.2356877088546754 0.53828125 2.2318685054779053 0.007053855340927839\n",
      "1.198078989982605 1.2637498080730438 0.52421875 2.2175142765045166 0.006070416420698166\n",
      "1.2148749828338623 1.3245668172836305 0.505078125 2.254240036010742 0.005032376851886511\n",
      "1.3067996501922607 1.338086372613907 0.503125 2.2492010593414307 0.003337436355650425\n",
      "1.2593952417373657 1.3147173941135406 0.5234375 2.2213401794433594 0.005453296471387148\n",
      "1.2591845989227295 1.2511152505874634 0.544140625 2.2413432598114014 0.004652294330298901\n",
      "1.0527722835540771 1.2277968585491181 0.53984375 2.243901014328003 0.0069537595845758915\n",
      "1.3495036363601685 1.2579344630241394 0.540625 2.25201416015625 0.006004332099109888\n",
      "1.3258095979690552 1.2588641226291657 0.53984375 2.2334561347961426 0.0052542672492563725\n",
      "1.1846920251846313 1.2068358480930328 0.567578125 2.255157709121704 0.0054305302910506725\n",
      "1.1908092498779297 1.2496338486671448 0.548828125 2.286928415298462 0.007225747220218182\n",
      "1.1257978677749634 1.2333169758319855 0.546875 2.267385244369507 0.005140132270753384\n",
      "1.2622697353363037 1.2376518249511719 0.52734375 2.2758126258850098 0.00529668852686882\n",
      "1.1628674268722534 1.2156095027923584 0.54296875 2.2699337005615234 0.004204236902296543\n",
      "1.1093285083770752 1.2228283643722535 0.55625 2.2592010498046875 0.005280789453536272\n",
      "1.1886987686157227 1.1861369550228118 0.556640625 2.2591300010681152 0.005284867715090513\n",
      "1.2117362022399902 1.2098534762859345 0.53359375 2.2654285430908203 0.0048380447551608086\n",
      "1.149811029434204 1.2047647774219512 0.554296875 2.259350299835205 0.00514414394274354\n",
      "Epoch:  2\n",
      "1.324493408203125 1.2246133744716645 0.553125 2.2730255126953125 0.005434361286461353\n",
      "1.2516064643859863 1.1668179154396057 0.554296875 2.268521308898926 0.004007613752037287\n",
      "1.311661720275879 1.1874006330966949 0.56796875 2.268495559692383 0.00458530941978097\n",
      "1.1182326078414917 1.1788750052452088 0.557421875 2.2625691890716553 0.0040516662411391735\n",
      "1.2418944835662842 1.2318594753742218 0.5515625 2.244446277618408 0.004192809108644724\n",
      "1.3526448011398315 1.1917636811733245 0.55625 2.235245704650879 0.005631486885249615\n",
      "1.2879340648651123 1.2472577393054962 0.542578125 2.2182466983795166 0.004647735506296158\n",
      "1.2139593362808228 1.228930813074112 0.54921875 2.24173903465271 0.004955214448273182\n",
      "0.9901606440544128 1.1992653220891953 0.55234375 2.2677972316741943 0.0062167649157345295\n",
      "1.0910100936889648 1.1753823637962342 0.5640625 2.246168613433838 0.004610661882907152\n",
      "1.2171670198440552 1.2397811889648438 0.534765625 2.2333388328552246 0.004534976091235876\n",
      "1.393943190574646 1.1772385001182557 0.558203125 2.2247610092163086 0.005956615786999464\n",
      "1.3168206214904785 1.2016730666160584 0.56015625 2.2805726528167725 0.004727519582957029\n",
      "0.9852479100227356 1.2022007316350938 0.5515625 2.252011775970459 0.004054661374539137\n",
      "1.2317092418670654 1.1627761989831924 0.556640625 2.232551336288452 0.004851271398365498\n",
      "1.1887733936309814 1.1622008442878724 0.565625 2.2319746017456055 0.0046721817925572395\n",
      "1.2373543977737427 1.158764162659645 0.57421875 2.237565040588379 0.004869840107858181\n",
      "1.086687445640564 1.1184097468852996 0.578125 2.2397499084472656 0.00429154047742486\n",
      "1.087437629699707 1.1745277851819993 0.580078125 2.2588272094726562 0.0058527193032205105\n",
      "1.1625093221664429 1.1293533831834792 0.585546875 2.231050729751587 0.005214673466980457\n",
      "1.3901817798614502 1.2011960983276366 0.5578125 2.230633020401001 0.0033800587989389896\n",
      "1.1524100303649902 1.1646647334098816 0.566796875 2.2456579208374023 0.0049587432295084\n",
      "1.132856011390686 1.1482365161180497 0.590625 2.2436723709106445 0.004716098308563232\n",
      "1.3170480728149414 1.120779001712799 0.595703125 2.225127696990967 0.004514376632869244\n",
      "Epoch:  3\n",
      "1.2431806325912476 1.1153237164020537 0.5921875 2.2435238361358643 0.0048325806856155396\n",
      "1.0466291904449463 1.091961419582367 0.611328125 2.2864155769348145 0.004235710017383099\n",
      "1.1458874940872192 1.1093837916851044 0.609375 2.2361652851104736 0.004072129260748625\n",
      "1.0290837287902832 1.1261032581329347 0.605078125 2.2258048057556152 0.006521542090922594\n",
      "1.127270221710205 1.1262760043144227 0.59609375 2.238523483276367 0.005760315340012312\n",
      "1.1588973999023438 1.1105923891067504 0.588671875 2.2623507976531982 0.004853532183915377\n",
      "0.8703020215034485 1.0763878166675567 0.607421875 2.2496206760406494 0.0033833172637969255\n",
      "1.0919872522354126 1.0462991774082184 0.609375 2.223644733428955 0.003712306497618556\n",
      "1.2619550228118896 1.099443656206131 0.598828125 2.2482635974884033 0.005297802854329348\n",
      "1.0276416540145874 1.0513733863830566 0.604296875 2.264679431915283 0.004024126101285219\n",
      "1.0472623109817505 1.0650208950042725 0.603515625 2.2603073120117188 0.0049324845895171165\n",
      "1.1360667943954468 1.04984230697155 0.6234375 2.2502224445343018 0.003755104262381792\n",
      "0.9548274874687195 1.051726958155632 0.61015625 2.270944118499756 0.00452071288600564\n",
      "1.0388821363449097 1.0259741425514222 0.61484375 2.238874912261963 0.003421737113967538\n",
      "1.03700852394104 1.0484744906425476 0.605859375 2.2323660850524902 0.0033983313478529453\n",
      "0.8067009449005127 0.9619533836841583 0.63515625 2.247190475463867 0.004233424551784992\n",
      "0.8688859939575195 1.002508345246315 0.6328125 2.257011651992798 0.004005420487374067\n",
      "1.1837538480758667 1.0139143407344817 0.619921875 2.2481918334960938 0.0038966129068285227\n",
      "1.06034255027771 1.0254725009202956 0.625390625 2.258305311203003 0.0032348723616451025\n",
      "0.9984939694404602 0.9859450429677963 0.638671875 2.2537801265716553 0.003208311740309\n",
      "1.0977646112442017 1.0086328893899918 0.62578125 2.2433035373687744 0.004839163739234209\n",
      "1.133535385131836 1.0180648922920228 0.6 2.2455766201019287 0.003984466195106506\n",
      "1.0888525247573853 0.9906364768743515 0.62421875 2.2382006645202637 0.004972145427018404\n",
      "Epoch:  4\n",
      "0.8180112838745117 0.954780113697052 0.641796875 2.2546048164367676 0.004064119886606932\n",
      "1.2425419092178345 0.9807729542255401 0.63515625 2.2504141330718994 0.004613191355019808\n",
      "1.0221692323684692 0.971869170665741 0.638671875 2.2397074699401855 0.003637530142441392\n",
      "0.9015934467315674 0.9410128831863404 0.653515625 2.253722906112671 0.005001983139663935\n",
      "0.791364848613739 0.9515382170677185 0.641015625 2.2453019618988037 0.004691621754318476\n",
      "1.0582764148712158 0.9527399152517319 0.641796875 2.2551686763763428 0.004346717149019241\n",
      "0.9428413510322571 0.9883081912994385 0.63046875 2.2656750679016113 0.004280181601643562\n",
      "1.0267199277877808 1.0105373710393906 0.61796875 2.2489519119262695 0.0040858471766114235\n",
      "1.025661587715149 0.9823365062475204 0.634375 2.243363380432129 0.004242410883307457\n",
      "0.9138609766960144 0.933676752448082 0.64375 2.2318227291107178 0.004165845923125744\n",
      "1.0045005083084106 0.9690570414066315 0.65546875 2.281601667404175 0.0033268684055656195\n",
      "0.947711706161499 0.9636000365018844 0.638671875 2.259328842163086 0.003714483231306076\n",
      "0.952778160572052 0.9476456135511399 0.63671875 2.2171530723571777 0.003831427311524749\n",
      "0.7819046974182129 0.9279686272144317 0.656640625 2.244053602218628 0.00443224236369133\n",
      "0.9872871041297913 0.9552336454391479 0.6296875 2.234389305114746 0.004009605851024389\n",
      "0.9388643503189087 0.9733090251684189 0.636328125 2.240936279296875 0.004321240819990635\n",
      "0.9339423179626465 0.9082601994276047 0.665234375 2.242784261703491 0.004196144174784422\n",
      "0.9339311718940735 0.9471622258424759 0.63359375 2.2404305934906006 0.0031947141978889704\n",
      "0.8404707908630371 0.9447022765874863 0.64609375 2.2378721237182617 0.0039145005866885185\n",
      "1.0661665201187134 0.9640420526266098 0.645703125 2.251875877380371 0.003491039155051112\n",
      "0.8358129858970642 0.9677208423614502 0.641796875 2.2341370582580566 0.0032061797101050615\n",
      "0.9239155054092407 0.932396411895752 0.63515625 2.246328353881836 0.003465232439339161\n",
      "0.8673691749572754 0.9145471930503846 0.658984375 2.2454216480255127 0.0032812063582241535\n",
      "1.0002275705337524 0.9578385889530182 0.6421875 2.2690999507904053 0.003954948391765356\n",
      "Epoch:  5\n",
      "0.9459153413772583 0.9369987696409225 0.655078125 2.238853693008423 0.003872003871947527\n",
      "0.8142052292823792 0.9058237433433532 0.652734375 2.2578558921813965 0.00354985473677516\n",
      "0.901901364326477 0.9425666511058808 0.629296875 2.2406225204467773 0.004314291290938854\n",
      "0.797263503074646 0.9035660177469254 0.670703125 2.245299816131592 0.003775761928409338\n",
      "0.9309362173080444 0.9080544859170914 0.659765625 2.268427848815918 0.003882195334881544\n",
      "0.9552658796310425 0.9239846974611282 0.65234375 2.2482898235321045 0.004156622104346752\n",
      "0.8505284786224365 0.9090602099895477 0.655859375 2.2353715896606445 0.004036345984786749\n",
      "1.013364553451538 0.938133928179741 0.6546875 2.270017623901367 0.0038038860075175762\n",
      "Learning Rate:  0.008\n",
      "1.0146300792694092 0.9598198503255844 0.637109375 2.2517175674438477 0.0029315773863345385\n",
      "1.0848127603530884 0.8898772180080414 0.659765625 2.2226033210754395 0.00390209280885756\n",
      "0.8667951822280884 0.8768946170806885 0.67734375 2.2543413639068604 0.004161799792200327\n",
      "0.8295767903327942 0.927844125032425 0.65234375 2.2757136821746826 0.0032921049278229475\n",
      "0.8538904786109924 0.9312559127807617 0.65078125 2.237511157989502 0.004299290478229523\n",
      "0.7894806861877441 0.8809136897325516 0.66953125 2.2591288089752197 0.0033908612094819546\n",
      "0.8267573118209839 0.9065419197082519 0.65859375 2.2456157207489014 0.003809284884482622\n",
      "0.9198735356330872 0.9113047808408737 0.662890625 2.2357609272003174 0.003783650230616331\n",
      "0.8538426756858826 0.8738304436206817 0.664453125 2.230536937713623 0.004014464560896158\n",
      "0.9469602108001709 0.8952210247516632 0.659375 2.2567381858825684 0.003505483502522111\n",
      "0.8683063387870789 0.8633825510740281 0.684765625 2.270402669906616 0.004119206219911575\n",
      "0.8202385306358337 0.8999726980924606 0.66171875 2.258169174194336 0.0034794313833117485\n",
      "0.7094883322715759 0.8474415123462677 0.681640625 2.2212421894073486 0.0043336451053619385\n",
      "0.74574214220047 0.8935385763645172 0.665625 2.2250537872314453 0.0035313423722982407\n",
      "0.9026610851287842 0.8756392866373062 0.680859375 2.241837501525879 0.003241118974983692\n",
      "Epoch:  6\n",
      "0.9426523447036743 0.874108049273491 0.661328125 2.2477664947509766 0.004050594288855791\n",
      "0.9088325500488281 0.8998198986053467 0.652734375 2.229221820831299 0.003332795575261116\n",
      "0.8281189799308777 0.8589014858007431 0.65859375 2.244187831878662 0.003922082483768463\n",
      "0.8870776295661926 0.8693654596805572 0.683984375 2.2376351356506348 0.0029727662913501263\n",
      "1.0705103874206543 0.8657050013542176 0.688671875 2.227933883666992 0.004179829265922308\n",
      "0.8522445559501648 0.8809671491384506 0.6734375 2.250166893005371 0.003921984229236841\n",
      "0.8404793739318848 0.888843634724617 0.665625 2.2590935230255127 0.004672185983508825\n",
      "0.8505898714065552 0.8583542168140411 0.680078125 2.237421751022339 0.0036199730820953846\n",
      "0.780548095703125 0.8859393507242203 0.670703125 2.2320847511291504 0.004991215653717518\n",
      "0.7633325457572937 0.8211555033922195 0.69375 2.2355833053588867 0.0034201384987682104\n",
      "0.7008765339851379 0.8396544277667999 0.69140625 2.2440741062164307 0.0036305561661720276\n",
      "0.7094317078590393 0.8137811869382858 0.6796875 2.244460344314575 0.0044518266804516315\n",
      "0.8462880849838257 0.8737446129322052 0.668359375 2.2637383937835693 0.003419027430936694\n",
      "0.8107568025588989 0.8784647136926651 0.66640625 2.241128444671631 0.004349211696535349\n",
      "0.9820494651794434 0.8550321817398071 0.672265625 2.234210968017578 0.003501894185319543\n",
      "0.9127898216247559 0.8660555183887482 0.67421875 2.2577908039093018 0.004112833645194769\n",
      "1.0621747970581055 0.9045544445514679 0.6671875 2.2708256244659424 0.0038857352919876575\n",
      "0.953789234161377 0.857451754808426 0.66953125 2.282646656036377 0.0035303651820868254\n",
      "0.893693745136261 0.8195064812898636 0.68828125 2.229240655899048 0.00392663711681962\n",
      "0.8224815726280212 0.8322350978851318 0.68046875 2.2346205711364746 0.004067497327923775\n",
      "0.9217669367790222 0.8609176486730575 0.682421875 2.2369205951690674 0.002668729517608881\n",
      "0.8153280019760132 0.8683907479047775 0.680859375 2.2862467765808105 0.0036323987878859043\n",
      "0.9035817980766296 0.8687449604272842 0.6765625 2.2500545978546143 0.0036980051081627607\n",
      "Epoch:  7\n",
      "0.8922565579414368 0.866598379611969 0.680859375 2.233812093734741 0.0036371576134115458\n",
      "1.0149431228637695 0.8788355112075805 0.674609375 2.247797727584839 0.0040825181640684605\n",
      "0.8683103322982788 0.808100837469101 0.70625 2.257338523864746 0.003045064629986882\n",
      "0.9038508534431458 0.8271069794893264 0.683203125 2.271026849746704 0.003534789429977536\n",
      "0.7507515549659729 0.8173896342515945 0.694140625 2.228116750717163 0.004007524345070124\n",
      "0.8783965110778809 0.846053472161293 0.6875 2.255403757095337 0.003171742893755436\n",
      "0.8231161236763 0.8616937458515167 0.6703125 2.2511658668518066 0.004383551422506571\n",
      "0.7512549757957458 0.8416863977909088 0.681640625 2.251638650894165 0.003624638309702277\n",
      "0.9277276396751404 0.8055261045694351 0.694140625 2.2551140785217285 0.004067439120262861\n",
      "1.057233452796936 0.8379685431718826 0.691796875 2.2275874614715576 0.0027325416449457407\n",
      "0.9320207834243774 0.8302242994308472 0.69453125 2.239478826522827 0.003618913237005472\n",
      "0.7009890675544739 0.7783079087734223 0.706640625 2.2285635471343994 0.003938006237149239\n",
      "0.7186896204948425 0.7802125245332718 0.71484375 2.2314679622650146 0.004497069865465164\n",
      "0.748095691204071 0.7681913882493973 0.721484375 2.234563112258911 0.004230593331158161\n",
      "0.819230854511261 0.761253759264946 0.709375 2.267449378967285 0.004341872408986092\n",
      "0.7511951923370361 0.7829778164625167 0.71015625 2.222965717315674 0.0033611252438277006\n",
      "0.6901749968528748 0.7895516455173492 0.704296875 2.2438151836395264 0.003677347442135215\n",
      "0.6810324192047119 0.804956567287445 0.70859375 2.2586605548858643 0.004343459848314524\n",
      "0.8295437693595886 0.7836618334054947 0.716796875 2.2632968425750732 0.0035591390915215015\n",
      "0.8353917598724365 0.774436405301094 0.71484375 2.290087938308716 0.004544912371784449\n",
      "0.9791585206985474 0.7910424530506134 0.71328125 2.2676312923431396 0.0037310223560780287\n",
      "0.7069249749183655 0.7414684057235718 0.725390625 2.2730846405029297 0.003769575385376811\n",
      "0.7159661054611206 0.7498163968324661 0.7328125 2.2653517723083496 0.0033857948146760464\n",
      "0.7114409804344177 0.7504274785518646 0.719921875 2.2496337890625 0.004277079366147518\n",
      "Epoch:  8\n",
      "0.6829477548599243 0.7634670168161393 0.707421875 2.2785282135009766 0.004080814775079489\n",
      "0.8964853882789612 0.7627650618553161 0.7140625 2.268969774246216 0.003467913717031479\n",
      "0.8833628296852112 0.760007792711258 0.721875 2.2654528617858887 0.003872125642374158\n",
      "0.6982226371765137 0.7721644014120101 0.7203125 2.275137424468994 0.0037787235341966152\n",
      "0.7987992763519287 0.8170939385890961 0.691796875 2.2627367973327637 0.0030800492968410254\n",
      "0.8335544466972351 0.757453665137291 0.730078125 2.2521896362304688 0.004696889314800501\n",
      "0.7160087823867798 0.7593299865722656 0.72578125 2.27439546585083 0.004161195829510689\n",
      "0.7250872850418091 0.7531365066766739 0.71640625 2.264401435852051 0.0043608094565570354\n",
      "0.7593106627464294 0.7522244811058044 0.73203125 2.2535593509674072 0.003912273328751326\n",
      "0.8544026017189026 0.7661652147769928 0.72578125 2.248469591140747 0.0042891367338597775\n",
      "0.6607048511505127 0.7047384470701218 0.7515625 2.239461898803711 0.003777955425903201\n",
      "0.7876322269439697 0.7281501770019532 0.71953125 2.2321176528930664 0.002987766172736883\n",
      "0.7543652653694153 0.7595618933439254 0.7203125 2.2510621547698975 0.004281602334231138\n",
      "0.79096919298172 0.7626854658126831 0.732421875 2.230288028717041 0.003907315898686647\n",
      "0.8209624886512756 0.7554043292999267 0.735546875 2.258941650390625 0.0039780717343091965\n",
      "0.8407895565032959 0.7475588589906692 0.730859375 2.22847580909729 0.0034681351389735937\n",
      "0.8434177041053772 0.7450336009263993 0.712890625 2.2592520713806152 0.0033942812588065863\n",
      "0.7142592072486877 0.8031832993030548 0.71875 2.246635675430298 0.0037643967662006617\n",
      "0.7031559944152832 0.7761916786432266 0.71796875 2.2366418838500977 0.004164975602179766\n",
      "0.6033574938774109 0.7182406187057495 0.73515625 2.2336273193359375 0.004225476179271936\n",
      "0.7213322520256042 0.7373938769102096 0.737109375 2.230903148651123 0.003962848335504532\n",
      "0.757888913154602 0.7279229670763016 0.734375 2.2498490810394287 0.004225138109177351\n",
      "0.7841023206710815 0.6938060849905014 0.741796875 2.245053291320801 0.0041918097995221615\n",
      "Epoch:  9\n",
      "0.7785374522209167 0.7369515389204025 0.73125 2.236705780029297 0.003488700371235609\n",
      "0.7331618070602417 0.6989095449447632 0.741015625 2.242631196975708 0.003512095659971237\n",
      "0.8235204219818115 0.7336204022169113 0.7375 2.2281601428985596 0.0038751314859837294\n",
      "0.7613052129745483 0.7308384507894516 0.737109375 2.233224868774414 0.003905537771061063\n",
      "0.6216076612472534 0.6987254947423935 0.751953125 2.2251486778259277 0.003946340177208185\n",
      "0.701553463935852 0.7280257821083069 0.737890625 2.2467689514160156 0.004481571726500988\n",
      "0.8472663760185242 0.743491142988205 0.7234375 2.265730857849121 0.004315359517931938\n",
      "0.6573424339294434 0.7221811503171921 0.743359375 2.2353646755218506 0.003959547728300095\n",
      "0.785819947719574 0.69904323220253 0.755078125 2.2348339557647705 0.004566134884953499\n",
      "0.8558818101882935 0.7235551178455353 0.753125 2.2398014068603516 0.0037449344526976347\n",
      "0.8606064915657043 0.7744236588478088 0.726953125 2.2548606395721436 0.0036030265036970377\n",
      "0.6864831447601318 0.7283770233392716 0.73125 2.2426633834838867 0.0037536697927862406\n",
      "0.5962012410163879 0.7104687124490738 0.737109375 2.246952772140503 0.00491574639454484\n",
      "0.7438339591026306 0.7151926070451736 0.73671875 2.2457706928253174 0.0033500727731734514\n",
      "0.7049381732940674 0.7678814142942428 0.7234375 2.2410788536071777 0.0041785757057368755\n",
      "0.7949618697166443 0.700302517414093 0.74765625 2.2449324131011963 0.003775630611926317\n",
      "0.8792908191680908 0.7200607746839524 0.753515625 2.2464637756347656 0.00399341806769371\n",
      "0.8473752737045288 0.6987647473812103 0.74140625 2.242438554763794 0.004226006101816893\n",
      "0.7218533754348755 0.7742319017648697 0.7234375 2.252786636352539 0.0040021538734436035\n",
      "0.8183289170265198 0.7051699161529541 0.735546875 2.24397873878479 0.004348658956587315\n",
      "0.7458692193031311 0.710933843255043 0.740234375 2.2241291999816895 0.004096668679267168\n",
      "0.6776931285858154 0.6732595786452293 0.757421875 2.2482235431671143 0.004029660485684872\n",
      "0.6753590703010559 0.7103900134563446 0.743359375 2.2435100078582764 0.0044846306554973125\n",
      "0.5024798512458801 0.670186385512352 0.75546875 2.2407615184783936 0.005925856530666351\n",
      "Epoch:  10\n",
      "0.7568718791007996 0.7379857033491135 0.740625 2.2524337768554688 0.004281253553926945\n",
      "0.6495777368545532 0.7254098415374756 0.72109375 2.2457354068756104 0.003797824727371335\n",
      "0.8422979712486267 0.6812509387731552 0.75078125 2.249143362045288 0.003729982068762183\n",
      "0.8647394180297852 0.7594994992017746 0.723828125 2.2340400218963623 0.0036196825094521046\n",
      "0.6771429777145386 0.7369403123855591 0.744921875 2.2612357139587402 0.005174214951694012\n",
      "0.5053462386131287 0.7041933298110962 0.75078125 2.2436208724975586 0.004109087400138378\n",
      "0.6076282262802124 0.6967807650566101 0.744921875 2.2854325771331787 0.004441595636308193\n",
      "0.5992329716682434 0.6964454054832458 0.749609375 2.246821165084839 0.004168571438640356\n",
      "0.7027367949485779 0.6965057402849197 0.7453125 2.232560634613037 0.003717476036399603\n",
      "0.745447039604187 0.699795800447464 0.748046875 2.2531285285949707 0.0044725555926561356\n",
      "0.8624473810195923 0.6782082438468933 0.75546875 2.2582056522369385 0.004164477344602346\n",
      "0.764444887638092 0.7032099217176437 0.753125 2.2564780712127686 0.003500923980027437\n",
      "0.6274707913398743 0.6941465139389038 0.73671875 2.237586259841919 0.004233156330883503\n",
      "0.8140602707862854 0.6843996733427048 0.745703125 2.2663745880126953 0.00499501870945096\n",
      "0.6128445267677307 0.6896962553262711 0.7609375 2.2356998920440674 0.004025583155453205\n",
      "0.6996534466743469 0.7228717297315598 0.734375 2.2623047828674316 0.004299468826502562\n",
      "Learning Rate:  0.0064\n",
      "0.5926169753074646 0.7079573929309845 0.747265625 2.2520077228546143 0.003995734732598066\n",
      "0.5189398527145386 0.6980746328830719 0.740625 2.223590850830078 0.004720080643892288\n",
      "0.6442223787307739 0.6727629363536834 0.759375 2.2482173442840576 0.003969985991716385\n",
      "0.7949023842811584 0.6643560364842415 0.751171875 2.2403197288513184 0.004159143194556236\n",
      "0.8166735172271729 0.6921684950590133 0.75390625 2.236050605773926 0.0042361123487353325\n",
      "0.6854414343833923 0.6842181593179703 0.754296875 2.2269866466522217 0.004662166349589825\n",
      "0.5786841511726379 0.6895387411117554 0.754296875 2.29483699798584 0.004746873863041401\n",
      "Epoch:  11\n",
      "0.5382400751113892 0.6870555996894836 0.75 2.2451391220092773 0.004251781385391951\n",
      "0.8963482975959778 0.7037840634584427 0.748046875 2.2487237453460693 0.004744615405797958\n",
      "0.7557104825973511 0.6808214992284775 0.75078125 2.2343435287475586 0.0033807840663939714\n",
      "0.5667082667350769 0.6901039451360702 0.753125 2.310317039489746 0.003777681617066264\n",
      "0.6014370918273926 0.6548901587724686 0.769140625 2.2716057300567627 0.00411429675295949\n",
      "0.6375265717506409 0.7048695623874665 0.74296875 2.2649896144866943 0.004956705961376429\n",
      "0.6183156371116638 0.71475298255682 0.729296875 2.468083381652832 0.004101750440895557\n",
      "0.6605047583580017 0.7264611065387726 0.73046875 2.41227650642395 0.004425046965479851\n",
      "0.7204272150993347 0.689442801475525 0.75546875 2.302075147628784 0.003995039500296116\n",
      "0.7755356431007385 0.7222220331430436 0.734765625 2.634932518005371 0.0033437544479966164\n",
      "0.7467941045761108 0.6980603963136673 0.74375 2.303976535797119 0.003813991090282798\n",
      "0.6452955007553101 0.6846213713288307 0.75234375 2.33317232131958 0.004607120994478464\n",
      "0.6354476809501648 0.6532916277647018 0.76328125 2.6221249103546143 0.004141918383538723\n",
      "0.6790662407875061 0.6827788352966309 0.753515625 2.3257250785827637 0.004477056674659252\n",
      "0.6656014919281006 0.683671897649765 0.746875 2.6410772800445557 0.0036047818139195442\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 1\n",
    "sumloss = 0\n",
    "sumacc = 0\n",
    "k = 0\n",
    "while i < ITERATIONS:\n",
    "    print('Epoch: ', k)\n",
    "    k = k + 1\n",
    "    for inp, target in data_loader:\n",
    "        batchstart = time.time()\n",
    "        x = inp.view(BATCH_SIZE, -1, 1).transpose(0,1).to(device)\n",
    "        x = encode_input(x[1:], x[:-1])\n",
    "        #print(x.shape)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = loop_model(x)\n",
    "        meaned = outputs[-56:].mean(dim=0) #TODO: what is this value really in bellec?\n",
    "        out_final = final_linear(meaned)\n",
    "        loss = ce(out_final, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            record_norm()\n",
    "            stats['loss'].append(loss.item())\n",
    "            acc = (torch.argmax(out_final, 1) == target).float().mean().item()\n",
    "            stats['acc'].append(acc)\n",
    "            batch_var = meaned.var(0).mean().item()\n",
    "            stats['batch_var'].append(batch_var)\n",
    "        #print(loss.item(), acc, batch_var, loop_model.pretrace.model.layers.control_synapse.weight.grad.norm().item()*20, target[0].item(), outputs.norm().item(), x.mean().item())\n",
    "\n",
    "        sumloss += loss.item()\n",
    "        sumacc += acc\n",
    "        if i%20 == 0:\n",
    "            print(loss.item(), sumloss/20, sumacc/20, time.time()-batchstart, batch_var) #torch.argmax(outputs[-1], 1).float().var()\n",
    "            sumloss = 0\n",
    "            sumacc = 0\n",
    "        if i%2500 == 0:\n",
    "            lr = lr * 0.8\n",
    "            optimizer = optim.Adam(params, lr=lr)\n",
    "            print('Learning Rate: ', lr)\n",
    "        i += 1\n",
    "    pickle.dump(stats, open('loc_stats', 'wb'))\n",
    "    #model.save('../../models/adap_clip5_'+str(k))\n",
    "    #post_model.save('../../models/post_big11_'+str(k))\n",
    "\n",
    "\n",
    "print('Total time: ', time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: what about data augmentation?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, p in loop_model.named_parameters():\n",
    "    print(name, p.shape)\n",
    "for name, p in final_linear.named_parameters():\n",
    "    print(name, p.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pickle\n",
    "o_weights = pickle.load(open('../../weight_transplant_enc', 'rb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "o1 = torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).t()\n",
    "o2 = torch.tensor(o_weights['InputWeights/InputWeight:0']).t()\n",
    "o3 = torch.cat((o2,o1), dim=1)\n",
    "with torch.no_grad():\n",
    "    loop_model.pretrace.model.layers.control_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.mem_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.control_synapse.weight.data[:,:300] = o3[:120]\n",
    "    loop_model.pretrace.model.layers.mem_synapse.weight.data[:,:300] = o3[120:]\n",
    "    final_linear.bias *= 0\n",
    "    final_linear.weight.data = torch.tensor(o_weights['out_weight:0']).t()\n",
    "loop_model.to(device)\n",
    "final_linear.to(device)\n",
    "\n",
    "params = [ loop_model.pretrace.model.layers.control_synapse.weight, loop_model.pretrace.model.layers.mem_synapse.weight, final_linear.weight, final_linear.bias]\n",
    "optimizer = optim.Adam(params, lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loop_model.pretrace.model.layers.control_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.mem_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.control_synapse.weight *= 20\n",
    "    loop_model.pretrace.model.layers.mem_synapse.weight *= 20\n",
    "    final_linear.bias *= 0\n",
    "loop_model.to(device)\n",
    "final_linear.to(device)\n",
    "\n",
    "params = [ loop_model.pretrace.model.layers.control_synapse.weight, loop_model.pretrace.model.layers.mem_synapse.weight, final_linear.weight, final_linear.bias]\n",
    "optimizer = optim.Adam(params, lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(14.9409, device='cuda:0', grad_fn=<NormBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_inp = torch.zeros([221], device=device)\n",
    "samp_inp[0] = 1\n",
    "torch.cat((loop_model.pretrace.model.layers.control_synapse(samp_inp),\n",
    "           loop_model.pretrace.model.layers.mem_synapse(samp_inp)), dim=0).norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0882, device='cuda:0', grad_fn=<DivBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_model.pretrace.model.layers.control_synapse(samp_inp)[0]/20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0882, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.1720, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.2516, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.3272, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.3991, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.4673, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.5321, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.5937, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.6523, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.7078, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         ...,\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n        grad_fn=<CopySlices>),\n (((tensor([[ 0.7078,  0.1606,  0.3927,  0.8992,  0.7494, -0.3921,  0.3812, -0.0607,\n             -0.0414,  0.1648,  0.0578,  0.5835,  0.3054,  0.0488,  0.1781,  0.1339,\n              0.5995, -0.0823,  0.1256, -0.3427, -1.0244,  0.2623,  0.3469, -0.2978,\n              0.9108, -0.5836,  0.0184, -0.0751,  0.6150,  0.5896,  0.0622,  0.1517,\n             -0.3562, -0.7948, -0.1396,  0.0627,  0.4937,  0.4825, -0.1554, -0.1213,\n             -0.4207, -0.5698, -0.6847,  0.7828, -0.2045, -0.1758, -0.5027,  0.3120,\n             -0.6476, -0.0854, -0.3593,  0.1552, -0.2050, -0.4737, -0.0113,  0.1719,\n              0.0267,  0.1214, -0.2545, -0.1456, -0.2698, -0.1443, -0.3263, -0.6927,\n              0.0712, -0.1612, -0.6541,  0.1857, -0.3641,  0.0208,  0.2926,  0.0518,\n              0.4572, -0.4955,  0.1614, -0.2748, -0.3494, -0.2323, -0.1250,  0.0225,\n             -0.4675,  0.3615,  0.1869, -0.6164,  0.5972,  0.7608,  0.4730, -0.0722,\n             -0.4297,  0.4231, -0.1618,  0.4905,  0.0836,  0.3919,  0.1430,  0.2835,\n              0.0042,  0.7166,  0.0509,  0.1613,  0.7556, -0.5408, -0.5098,  0.3890,\n             -0.4707,  0.7799, -0.1660, -0.2999,  0.7716,  0.5941,  0.7494,  0.3636,\n             -0.3456,  0.7664, -0.1075,  0.3220,  0.3801, -0.0622,  0.2464,  0.3700]],\n           device='cuda:0', grad_fn=<SubBackward0>),\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n           device='cuda:0', grad_fn=<BellecSpikeBackward>)),\n   (tensor([[ 0.1510, -0.4411,  0.1197,  0.5322, -0.2787, -0.0600, -0.1746,  0.7420,\n              0.2698,  0.1635, -0.3089,  0.2164, -0.2706,  0.0128, -0.2551,  0.2714,\n              0.2314, -0.0836,  0.1589, -0.4386, -0.5984,  0.1763,  0.0669,  0.2548,\n              0.9563,  0.3790, -0.3663,  0.4482, -0.5280, -0.1852, -0.0274,  0.6875,\n             -0.2988, -0.3316, -0.0395, -0.2662,  0.4521, -0.4333, -0.4604, -0.1757,\n             -0.1998,  0.7742,  0.3810,  0.0351, -0.4917,  0.3388, -0.4013, -0.6199,\n              0.4767,  0.1272,  0.3695,  0.1279,  0.3438, -0.2612, -0.4150,  0.2735,\n             -0.3224, -0.2767, -0.1828,  0.0070, -0.1420, -0.5517, -0.2583, -0.8922,\n              0.2509, -0.6428, -0.4431,  0.0209, -0.2968,  0.6192, -0.5188,  0.1072,\n             -0.0158, -0.4687,  0.2100, -0.0688,  0.3097,  0.3304,  0.8680,  0.5363,\n             -0.1481, -0.0961,  0.4413,  0.2629,  0.2569, -0.6488, -0.0098, -0.2961,\n              0.1123, -0.0394,  0.3652,  0.1273,  0.3155, -0.1872, -0.3790, -0.1645,\n             -0.0068,  0.1521,  0.9066, -0.0170]], device='cuda:0',\n           grad_fn=<SubBackward0>),\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0.]], device='cuda:0', grad_fn=<AddBackward0>),\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0.]], device='cuda:0', grad_fn=<BellecSpikeBackward>)),\n   tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0.]], device='cuda:0', grad_fn=<AddBackward0>)),\n  (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n          device='cuda:0', grad_fn=<BellecSpikeBackward>),\n   tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0.]], device='cuda:0', grad_fn=<BellecSpikeBackward>))))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_model(torch.ones([10,1,1], device=device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n, p in o_weights.items():\n",
    "    print(n, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(o_weights['out_weight:0']).norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_linear.weight.norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(o_weights['out_weight:0']).t().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_linear.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).norm()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn.Linear(220,220).weight.norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}