{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "#TODO: path\n",
    "sys.path.append('../../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "#TODO: change to 256\n",
    "BATCH_SIZE = 128#256\n",
    "\n",
    "USE_JIT = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "n_in = 81\n",
    "\n",
    "mnist = MNIST('../../', transform=transforms.ToTensor(), download=True) #distortion_transform([0,15], 3)\n",
    "test = MNIST('../../', transform=transforms.ToTensor(), train=False)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(mnist, batch_size=BATCH_SIZE, drop_last=True, num_workers=0, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=1024, drop_last=False, num_workers=0)\n",
    "\n",
    "like_bellec = {\n",
    "    'spkfn' : 'bellec',\n",
    "    'spkconfig' : 0,\n",
    "    'architecture': '1L',\n",
    "    'beta': 0.95,\n",
    "    'control_neuron': 'LIF',\n",
    "    'mem_neuron' : 'Adaptive',\n",
    "    'lr' : 1e-2,\n",
    "    '1-beta': True,\n",
    "    'decay_out': True\n",
    "}\n",
    "\n",
    "spec = like_bellec\n",
    "#spec['decay_out'] = False\n",
    "#TODO: remove\n",
    "#spec['1-beta'] = False\n",
    "\n",
    "from Code.Networks import Selector, DynNetwork, OuterWrapper, LSTMWrapper, ReLuWrapper, DummyNeuron, make_SequenceWrapper, ParallelNetwork\n",
    "from Code.NewNeurons2 import SeqOnlySpike, CooldownNeuron, OutputNeuron, LIFNeuron, NoResetNeuron, AdaptiveNeuron\n",
    "\n",
    "built_config = {\n",
    "    'BETA': spec['beta'],\n",
    "    'OFFSET': 2, # TODO: this?\n",
    "    'SPIKE_FN': spec['spkfn'],\n",
    "    '1-beta': spec['1-beta'],\n",
    "    'ADAPDECAY': 0.9985,\n",
    "    'ADAPSCALE': 180\n",
    "}\n",
    "\n",
    "n_control = 120\n",
    "n_mem = 100\n",
    "\n",
    "control_lookup = {\n",
    "    'LIF': LIFNeuron,\n",
    "    'Disc': SeqOnlySpike,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "mem_lookup = {\n",
    "    'Adaptive': AdaptiveNeuron,\n",
    "    'Cooldown': CooldownNeuron,\n",
    "    'NoReset': NoResetNeuron\n",
    "}\n",
    "\n",
    "control_neuron = control_lookup[spec['control_neuron']](n_control, built_config)\n",
    "mem_neuron = mem_lookup[spec['mem_neuron']](n_mem, built_config)\n",
    "out_neuron = OutputNeuron(n_control+n_mem, built_config) if spec['decay_out'] else DummyNeuron(n_control+n_mem, built_config)\n",
    "\n",
    "\n",
    "loop_2L = OrderedDict([\n",
    "    ('input', n_in),\n",
    "    ('control', [['input', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['control'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop_1L = OrderedDict([\n",
    "    ('input', n_in),\n",
    "    ('control', [['input', 'control', 'mem'], control_neuron, nn.Linear]),\n",
    "    ('mem', [['input', 'control', 'mem'], mem_neuron, nn.Linear]),\n",
    "    ('output', [['control', 'mem'], out_neuron, None]),\n",
    "])\n",
    "\n",
    "loop = loop_1L if spec['architecture'] == '1L' else loop_2L\n",
    "\n",
    "loop_model = OuterWrapper(make_SequenceWrapper(ParallelNetwork(loop), USE_JIT), device, USE_JIT)\n",
    "\n",
    "final_linear = nn.Linear(n_control+n_mem, 10).to(device)\n",
    "\n",
    "\n",
    "\n",
    "params = list(loop_model.parameters())+list(final_linear.parameters())\n",
    "#TODO: end trial\n",
    "lr = spec['lr'] * 0.15 * 2\n",
    "#TODO: adam at 3 locs\n",
    "optimizer = optim.SGD(params, lr=lr)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "\n",
    "#TODO: check correctness here\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        loop_model.pretrace.model.layers.mem_synapse.weight[i, i+201] = 0\n",
    "\n",
    "    for i in range(120):\n",
    "        loop_model.pretrace.model.layers.control_synapse.weight[i, i+81] = 0\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "trigger_signal = torch.ones([783+56, 1, 1], device=device)\n",
    "trigger_signal[:783] = 0\n",
    "def encode_input(curr, last):\n",
    "    out = torch.zeros([783+56, curr.shape[1], 2,40], device=curr.device)\n",
    "    out[:783, :, 0, :] = ((torch.arange(40, device=curr.device) < 40 * last) & (torch.arange(40, device=curr.device) > 40 * curr)).float()\n",
    "    out[:783, :, 1, :] = ((torch.arange(40, device=curr.device) > 40 * last) & (torch.arange(40, device=curr.device) < 40 * curr)).float()\n",
    "    out = torch.cat((out.view([783+56, curr.shape[1], 80]), trigger_signal.expand([783+56, curr.shape[1], 1])), dim=-1)\n",
    "    return out\n",
    "\n",
    "stats = {\n",
    "    'grad_norm': [],\n",
    "    'loss': [],\n",
    "    'acc': [],\n",
    "    'batch_var': []\n",
    "}\n",
    "\n",
    "grad_norm_history = []\n",
    "def record_norm():\n",
    "    norms = []\n",
    "    for p in params:\n",
    "        norms.append(p.grad.norm().item())\n",
    "    stats['grad_norm'].append(torch.tensor(norms).norm().item())\n",
    "\n",
    "\n",
    "ITERATIONS = 36000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "2.305652141571045 2.3017991542816163 0.112890625 2.4868509769439697 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-4849bc2a7b0e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloop_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m         \u001B[0mmeaned\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m56\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m#TODO: what is this value really in bellec?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mout_final\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfinal_linear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmeaned\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/Networks.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    198\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m             \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpretrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_initial_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtwo_dim\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maddr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/Networks.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m    178\u001B[0m         \u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mout1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 180\u001B[0;31m             \u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    181\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/Networks.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inp, h)\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m                 \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m'_synapse'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m             \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlayer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midxState\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m             \u001B[0mnew_state\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0midxState\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 532\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    533\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Code/NewNeurons2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, h)\u001B[0m\n\u001B[1;32m    162\u001B[0m         \u001B[0mold_spike\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m         \u001B[0mrel_thresh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecay\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mrel_thresh\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecay\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mold_spike\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 164\u001B[0;31m         \u001B[0mthreshold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mrel_thresh\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    165\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0momb\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m             \u001B[0mmem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbeta\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mold_mem\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbeta\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mold_spike\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mthreshold\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 1\n",
    "sumloss = 0\n",
    "sumacc = 0\n",
    "k = 0\n",
    "while i < ITERATIONS:\n",
    "    print('Epoch: ', k)\n",
    "    k = k + 1\n",
    "    for inp, target in data_loader:\n",
    "        batchstart = time.time()\n",
    "        x = inp.view(BATCH_SIZE, -1, 1).transpose(0,1).to(device)\n",
    "        x = encode_input(x[1:], x[:-1])\n",
    "        #print(x.shape)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = loop_model(x)\n",
    "        meaned = outputs[-56:].mean(dim=0) #TODO: what is this value really in bellec?\n",
    "        out_final = final_linear(meaned)\n",
    "        loss = ce(out_final, target)\n",
    "\n",
    "        loss.backward()\n",
    "        #TODO: end trial, add * init_norm\n",
    "        #apply_grad_norm()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            record_norm()\n",
    "            stats['loss'].append(loss.item())\n",
    "            acc = (torch.argmax(out_final, 1) == target).float().mean().item()\n",
    "            stats['acc'].append(acc)\n",
    "            batch_var = meaned.var(0).mean().item()\n",
    "            stats['batch_var'].append(batch_var)\n",
    "        #print(loss.item(), acc, batch_var, loop_model.pretrace.model.layers.control_synapse.weight.grad.norm().item()*20, target[0].item(), outputs.norm().item(), x.mean().item())\n",
    "\n",
    "        sumloss += loss.item()\n",
    "        sumacc += acc\n",
    "        if i%20 == 0:\n",
    "            print(loss.item(), sumloss/20, sumacc/20, time.time()-batchstart, batch_var) #torch.argmax(outputs[-1], 1).float().var()\n",
    "            sumloss = 0\n",
    "            sumacc = 0\n",
    "        if i%2500 == 0:\n",
    "            #TODO: end trial\n",
    "            lr = lr * 0.8 * 0.95\n",
    "            optimizer = optim.SGD(params, lr=lr)\n",
    "            print('Learning Rate: ', lr)\n",
    "        i += 1\n",
    "    pickle.dump(stats, open('loc_stats', 'wb'))\n",
    "    #model.save('../../models/adap_clip5_'+str(k))\n",
    "    #post_model.save('../../models/post_big11_'+str(k))\n",
    "\n",
    "\n",
    "print('Total time: ', time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: what about data augmentation?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for name, p in loop_model.named_parameters():\n",
    "    print(name, p.shape)\n",
    "for name, p in final_linear.named_parameters():\n",
    "    print(name, p.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pickle\n",
    "o_weights = pickle.load(open('../../weight_transplant_enc', 'rb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "o1 = torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).t()\n",
    "o2 = torch.tensor(o_weights['InputWeights/InputWeight:0']).t()\n",
    "o3 = torch.cat((o2,o1), dim=1)\n",
    "with torch.no_grad():\n",
    "    loop_model.pretrace.model.layers.control_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.mem_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.control_synapse.weight.data[:,:300] = o3[:120]\n",
    "    loop_model.pretrace.model.layers.mem_synapse.weight.data[:,:300] = o3[120:]\n",
    "    final_linear.bias *= 0\n",
    "    final_linear.weight.data = torch.tensor(o_weights['out_weight:0']).t()\n",
    "loop_model.to(device)\n",
    "final_linear.to(device)\n",
    "\n",
    "params = [ loop_model.pretrace.model.layers.control_synapse.weight, loop_model.pretrace.model.layers.mem_synapse.weight, final_linear.weight, final_linear.bias]\n",
    "optimizer = optim.SGD(params, lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "grad_norms = torch.zeros([len(params)], device=device)\n",
    "def apply_grad_norm():\n",
    "    i = 0\n",
    "    for p in params:\n",
    "        grad_norms[i] = p.grad.norm()\n",
    "    f_norm = grad_norms.norm()\n",
    "    for p in params:\n",
    "        p.grad /= f_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loop_model.pretrace.model.layers.control_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.mem_synapse.bias *= 0\n",
    "    loop_model.pretrace.model.layers.control_synapse.weight *= 20\n",
    "    loop_model.pretrace.model.layers.mem_synapse.weight *= 20\n",
    "    final_linear.bias *= 0\n",
    "loop_model.to(device)\n",
    "final_linear.to(device)\n",
    "\n",
    "params = [ loop_model.pretrace.model.layers.control_synapse.weight, loop_model.pretrace.model.layers.mem_synapse.weight, final_linear.weight, final_linear.bias]\n",
    "optimizer = optim.Adam(params, lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(14.9409, device='cuda:0', grad_fn=<NormBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_inp = torch.zeros([221], device=device)\n",
    "samp_inp[0] = 1\n",
    "torch.cat((loop_model.pretrace.model.layers.control_synapse(samp_inp),\n",
    "           loop_model.pretrace.model.layers.mem_synapse(samp_inp)), dim=0).norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0882, device='cuda:0', grad_fn=<DivBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_model.pretrace.model.layers.control_synapse(samp_inp)[0]/20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0882, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.1720, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.2516, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.3272, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.3991, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.4673, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.5321, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.5937, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.6523, device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0.7078, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         ...,\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n        grad_fn=<CopySlices>),\n (((tensor([[ 0.7078,  0.1606,  0.3927,  0.8992,  0.7494, -0.3921,  0.3812, -0.0607,\n             -0.0414,  0.1648,  0.0578,  0.5835,  0.3054,  0.0488,  0.1781,  0.1339,\n              0.5995, -0.0823,  0.1256, -0.3427, -1.0244,  0.2623,  0.3469, -0.2978,\n              0.9108, -0.5836,  0.0184, -0.0751,  0.6150,  0.5896,  0.0622,  0.1517,\n             -0.3562, -0.7948, -0.1396,  0.0627,  0.4937,  0.4825, -0.1554, -0.1213,\n             -0.4207, -0.5698, -0.6847,  0.7828, -0.2045, -0.1758, -0.5027,  0.3120,\n             -0.6476, -0.0854, -0.3593,  0.1552, -0.2050, -0.4737, -0.0113,  0.1719,\n              0.0267,  0.1214, -0.2545, -0.1456, -0.2698, -0.1443, -0.3263, -0.6927,\n              0.0712, -0.1612, -0.6541,  0.1857, -0.3641,  0.0208,  0.2926,  0.0518,\n              0.4572, -0.4955,  0.1614, -0.2748, -0.3494, -0.2323, -0.1250,  0.0225,\n             -0.4675,  0.3615,  0.1869, -0.6164,  0.5972,  0.7608,  0.4730, -0.0722,\n             -0.4297,  0.4231, -0.1618,  0.4905,  0.0836,  0.3919,  0.1430,  0.2835,\n              0.0042,  0.7166,  0.0509,  0.1613,  0.7556, -0.5408, -0.5098,  0.3890,\n             -0.4707,  0.7799, -0.1660, -0.2999,  0.7716,  0.5941,  0.7494,  0.3636,\n             -0.3456,  0.7664, -0.1075,  0.3220,  0.3801, -0.0622,  0.2464,  0.3700]],\n           device='cuda:0', grad_fn=<SubBackward0>),\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n           device='cuda:0', grad_fn=<BellecSpikeBackward>)),\n   (tensor([[ 0.1510, -0.4411,  0.1197,  0.5322, -0.2787, -0.0600, -0.1746,  0.7420,\n              0.2698,  0.1635, -0.3089,  0.2164, -0.2706,  0.0128, -0.2551,  0.2714,\n              0.2314, -0.0836,  0.1589, -0.4386, -0.5984,  0.1763,  0.0669,  0.2548,\n              0.9563,  0.3790, -0.3663,  0.4482, -0.5280, -0.1852, -0.0274,  0.6875,\n             -0.2988, -0.3316, -0.0395, -0.2662,  0.4521, -0.4333, -0.4604, -0.1757,\n             -0.1998,  0.7742,  0.3810,  0.0351, -0.4917,  0.3388, -0.4013, -0.6199,\n              0.4767,  0.1272,  0.3695,  0.1279,  0.3438, -0.2612, -0.4150,  0.2735,\n             -0.3224, -0.2767, -0.1828,  0.0070, -0.1420, -0.5517, -0.2583, -0.8922,\n              0.2509, -0.6428, -0.4431,  0.0209, -0.2968,  0.6192, -0.5188,  0.1072,\n             -0.0158, -0.4687,  0.2100, -0.0688,  0.3097,  0.3304,  0.8680,  0.5363,\n             -0.1481, -0.0961,  0.4413,  0.2629,  0.2569, -0.6488, -0.0098, -0.2961,\n              0.1123, -0.0394,  0.3652,  0.1273,  0.3155, -0.1872, -0.3790, -0.1645,\n             -0.0068,  0.1521,  0.9066, -0.0170]], device='cuda:0',\n           grad_fn=<SubBackward0>),\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0.]], device='cuda:0', grad_fn=<AddBackward0>),\n    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0.]], device='cuda:0', grad_fn=<BellecSpikeBackward>)),\n   tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0.]], device='cuda:0', grad_fn=<AddBackward0>)),\n  (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n          device='cuda:0', grad_fn=<BellecSpikeBackward>),\n   tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n            0., 0., 0., 0.]], device='cuda:0', grad_fn=<BellecSpikeBackward>))))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_model(torch.ones([10,1,1], device=device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n, p in o_weights.items():\n",
    "    print(n, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(o_weights['out_weight:0']).norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "21.31194305419922"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = []\n",
    "for p in params:\n",
    "    norms.append(p.norm().item())\n",
    "torch.tensor(norms).norm().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(29.3991, device='cuda:0', grad_fn=<DivBackward0>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_linear.weight.norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(o_weights['out_weight:0']).t().shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_linear.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor(o_weights['RecWeights/RecurrentWeight:0']).norm()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn.Linear(220,220).weight.norm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optim.SGD()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "o3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "7679846928842235034"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hello'.__hash__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "-7651589359494145503"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('hello',).__hash__()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.system('date'))\n",
    "print('test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "307712"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "802.938368"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()/1000000\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pid = os.fork()\n",
    "print(pid)\n",
    "#pid = os.fork()\n",
    "#print(pid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['a', 'b', 'c']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a b c'.split(' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}